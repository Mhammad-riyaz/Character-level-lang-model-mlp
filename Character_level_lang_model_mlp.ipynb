{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMII9K+LRae5tu9TeGRTTnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhammad-riyaz/Character-level-lang-model-mlp/blob/main/Character_level_lang_model_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p6K2z6qrm241"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt','r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6eKVKiZnKKg",
        "outputId": "44e079e6-f11b-4308-fb8c-52cca748e284"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_q-TM45nhzi",
        "outputId": "2a2836b2-5c64-45e8-bda9-f4907dda09c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['<S>'] = 0\n",
        "stoi['<E>'] = 27\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgt_V2HQnh8v",
        "outputId": "778549a0-ce64-40ee-d0d7-6bb02ccad4f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '<S>', 27: '<E>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "\n",
        "block_size = 3 # context length : how many characters do we take to predict the next one ?\n",
        "X,Y = [],[]\n",
        "for w in words[:5]:\n",
        "  print(w)\n",
        "  context = [0] * block_size\n",
        "  for ch in list(w)+['<E>']:\n",
        "    ix = stoi[ch]\n",
        "    X.append(context)\n",
        "    Y.append(ix)\n",
        "    print(''.join(itos[i] for i in context), '--->',itos[ix])\n",
        "    context = context[1:] + [ix]\n",
        "    # print(context)\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVTJ7FgnKY8",
        "outputId": "ffe7432a-7644-45f6-de4d-dc5ef43ebdf5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emma\n",
            "<S><S><S> ---> e\n",
            "<S><S>e ---> m\n",
            "<S>em ---> m\n",
            "emm ---> a\n",
            "mma ---> <E>\n",
            "olivia\n",
            "<S><S><S> ---> o\n",
            "<S><S>o ---> l\n",
            "<S>ol ---> i\n",
            "oli ---> v\n",
            "liv ---> i\n",
            "ivi ---> a\n",
            "via ---> <E>\n",
            "ava\n",
            "<S><S><S> ---> a\n",
            "<S><S>a ---> v\n",
            "<S>av ---> a\n",
            "ava ---> <E>\n",
            "isabella\n",
            "<S><S><S> ---> i\n",
            "<S><S>i ---> s\n",
            "<S>is ---> a\n",
            "isa ---> b\n",
            "sab ---> e\n",
            "abe ---> l\n",
            "bel ---> l\n",
            "ell ---> a\n",
            "lla ---> <E>\n",
            "sophia\n",
            "<S><S><S> ---> s\n",
            "<S><S>s ---> o\n",
            "<S>so ---> p\n",
            "sop ---> h\n",
            "oph ---> i\n",
            "phi ---> a\n",
            "hia ---> <E>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNbutnpCqh1M",
        "outputId": "cfeef4a0-a7ef-47b9-8c2c-8585402db051"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K86pBzGlrsbp",
        "outputId": "d5c794da-6ae0-4dba-e586-2b15fc88572f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0],\n",
              "        [ 0,  0,  5],\n",
              "        [ 0,  5, 13],\n",
              "        [ 5, 13, 13],\n",
              "        [13, 13,  1],\n",
              "        [ 0,  0,  0],\n",
              "        [ 0,  0, 15],\n",
              "        [ 0, 15, 12],\n",
              "        [15, 12,  9],\n",
              "        [12,  9, 22],\n",
              "        [ 9, 22,  9],\n",
              "        [22,  9,  1],\n",
              "        [ 0,  0,  0],\n",
              "        [ 0,  0,  1],\n",
              "        [ 0,  1, 22],\n",
              "        [ 1, 22,  1],\n",
              "        [ 0,  0,  0],\n",
              "        [ 0,  0,  9],\n",
              "        [ 0,  9, 19],\n",
              "        [ 9, 19,  1],\n",
              "        [19,  1,  2],\n",
              "        [ 1,  2,  5],\n",
              "        [ 2,  5, 12],\n",
              "        [ 5, 12, 12],\n",
              "        [12, 12,  1],\n",
              "        [ 0,  0,  0],\n",
              "        [ 0,  0, 19],\n",
              "        [ 0, 19, 15],\n",
              "        [19, 15, 16],\n",
              "        [15, 16,  8],\n",
              "        [16,  8,  9],\n",
              "        [ 8,  9,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((28,2)) # look matrix, each of the 26 characters are embedded in 2 dimensional space"
      ],
      "metadata": {
        "id": "DqBAYtubrsLd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj465MjTr0-O",
        "outputId": "9e820313-b410-415e-dfa2-ee053ed894b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8214, -0.3580],\n",
              "        [-1.0191,  1.1457],\n",
              "        [ 0.6001,  0.0715],\n",
              "        [ 1.1801,  0.3633],\n",
              "        [ 1.4165, -2.8182],\n",
              "        [-1.3021, -0.6592],\n",
              "        [ 0.7364,  0.0975],\n",
              "        [ 1.4247,  0.7500],\n",
              "        [ 0.8237, -0.3435],\n",
              "        [-0.6809, -0.5768],\n",
              "        [ 0.7837,  0.3559],\n",
              "        [ 0.4296, -0.7301],\n",
              "        [-0.2894,  1.8371],\n",
              "        [-0.5835, -0.5349],\n",
              "        [ 0.4917,  0.8154],\n",
              "        [-1.3134, -0.8577],\n",
              "        [-0.7354, -0.0706],\n",
              "        [-0.6958, -0.3908],\n",
              "        [-0.4926,  0.3683],\n",
              "        [ 0.4349, -0.4292],\n",
              "        [ 0.6910, -0.3227],\n",
              "        [ 1.8025,  2.0263],\n",
              "        [-1.2043, -2.4364],\n",
              "        [-0.2549,  0.8470],\n",
              "        [-0.6146, -1.4437],\n",
              "        [-1.5758, -0.1354],\n",
              "        [ 0.4051,  0.9715],\n",
              "        [ 1.4221,  0.7390]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFOZhZVPr06o",
        "outputId": "ad0f9cf7-0576-475b-dd27-35f846d4b465"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3021, -0.6592])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(torch.tensor(5), num_classes=28).float() @  C\n",
        "# see the output is same as above, since only our 5th bit is on all the other will be masked out to zeroes and we only get the 5th row,\n",
        "# but we won't use this as the indexing is fast."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdnQ_k8Ar04P",
        "outputId": "c1a610b5-2395-4e18-b8cf-8c38ad3b9fa8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3021, -0.6592])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[X]\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWBlR2L9r01n",
        "outputId": "10205af7-5eff-4c4c-823f-a8b09b6d7abf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.randn((6,100)) # This is the hidden layer and there are 100 neurons with 6 weights, bc the input will be context size * 2 (dim space)\n",
        "b1 = torch.randn(100)"
      ],
      "metadata": {
        "id": "8BjJqbhOr0pH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now our step should be multiplication like below\n",
        "emb @ W1 + b1\n",
        "# but this won't work bc of the different shapes, we have to concatenate the 3 inputs (context) to feed into the net."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "yQijT0JixVW4",
        "outputId": "fc6ed114-03b8-4a29-b72e-fdc5d47eb40f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4222716973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now our step should be multiplication like below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# but this won't work bc of the different shapes, we have to concatenate the 3 inputs (context) to feed into the net.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4gsd9RxrywWk",
        "outputId": "a33026ef-090f-4a47-b60c-4553070b1f32"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-1.3021, -0.6592]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-1.3021, -0.6592],\n",
              "         [-0.5835, -0.5349]],\n",
              "\n",
              "        [[-1.3021, -0.6592],\n",
              "         [-0.5835, -0.5349],\n",
              "         [-0.5835, -0.5349]],\n",
              "\n",
              "        [[-0.5835, -0.5349],\n",
              "         [-0.5835, -0.5349],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-1.3134, -0.8577]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-1.3134, -0.8577],\n",
              "         [-0.2894,  1.8371]],\n",
              "\n",
              "        [[-1.3134, -0.8577],\n",
              "         [-0.2894,  1.8371],\n",
              "         [-0.6809, -0.5768]],\n",
              "\n",
              "        [[-0.2894,  1.8371],\n",
              "         [-0.6809, -0.5768],\n",
              "         [-1.2043, -2.4364]],\n",
              "\n",
              "        [[-0.6809, -0.5768],\n",
              "         [-1.2043, -2.4364],\n",
              "         [-0.6809, -0.5768]],\n",
              "\n",
              "        [[-1.2043, -2.4364],\n",
              "         [-0.6809, -0.5768],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-1.0191,  1.1457],\n",
              "         [-1.2043, -2.4364]],\n",
              "\n",
              "        [[-1.0191,  1.1457],\n",
              "         [-1.2043, -2.4364],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.6809, -0.5768]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.6809, -0.5768],\n",
              "         [ 0.4349, -0.4292]],\n",
              "\n",
              "        [[-0.6809, -0.5768],\n",
              "         [ 0.4349, -0.4292],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[ 0.4349, -0.4292],\n",
              "         [-1.0191,  1.1457],\n",
              "         [ 0.6001,  0.0715]],\n",
              "\n",
              "        [[-1.0191,  1.1457],\n",
              "         [ 0.6001,  0.0715],\n",
              "         [-1.3021, -0.6592]],\n",
              "\n",
              "        [[ 0.6001,  0.0715],\n",
              "         [-1.3021, -0.6592],\n",
              "         [-0.2894,  1.8371]],\n",
              "\n",
              "        [[-1.3021, -0.6592],\n",
              "         [-0.2894,  1.8371],\n",
              "         [-0.2894,  1.8371]],\n",
              "\n",
              "        [[-0.2894,  1.8371],\n",
              "         [-0.2894,  1.8371],\n",
              "         [-1.0191,  1.1457]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [-0.8214, -0.3580],\n",
              "         [ 0.4349, -0.4292]],\n",
              "\n",
              "        [[-0.8214, -0.3580],\n",
              "         [ 0.4349, -0.4292],\n",
              "         [-1.3134, -0.8577]],\n",
              "\n",
              "        [[ 0.4349, -0.4292],\n",
              "         [-1.3134, -0.8577],\n",
              "         [-0.7354, -0.0706]],\n",
              "\n",
              "        [[-1.3134, -0.8577],\n",
              "         [-0.7354, -0.0706],\n",
              "         [ 0.8237, -0.3435]],\n",
              "\n",
              "        [[-0.7354, -0.0706],\n",
              "         [ 0.8237, -0.3435],\n",
              "         [-0.6809, -0.5768]],\n",
              "\n",
              "        [[ 0.8237, -0.3435],\n",
              "         [-0.6809, -0.5768],\n",
              "         [-1.0191,  1.1457]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((emb[:, 0, :], emb[:, 1 ,:], emb[:, 2 ,:]), 1 ).shape\n",
        "# the problem with the above code is we have fixed the middle indices, so its not generalizable if we increae the context size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9Mz_UezOyH1Y",
        "outputId": "18a6fe51-983f-4a1e-8e42-38138ab3da96"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat(torch.unbind(emb,1),1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3u18am0X4M",
        "outputId": "1df0a366-af49-4213-e6e6-b5bba3c5078f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# acutally there is a more efficent method,\n",
        "a = torch.arange(18)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW8s0sJV0X7P",
        "outputId": "30e80271-d2fb-4ed9-bd20-5101e8936a76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use torch.view to adjust/change the dimmensions\n",
        "a.view(2,9) # this works as long as the dimmensions we are giving multiplies to the original length"
      ],
      "metadata": {
        "id": "h3gPiZWT0X9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# why this is eficient is in memory, it will be stored like linear number, but some attribuets will tell it to how to represent accoding\n",
        "# to the dimmensions, so we are not changing/creating the memory and the concatenation will create new tensor and new storage for it.\n",
        "a.storage()"
      ],
      "metadata": {
        "id": "KNXAFEMk0YA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb.view(32,6)"
      ],
      "metadata": {
        "id": "Ajupyvx-2Fwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # -1 because not hardcoding 32, keeping -1 will infer the size, because we fixed the 6\n",
        "h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRarJKEf2Z-g",
        "outputId": "6d36ccb4-610d-473a-844b-b09fe427af82"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7341, -0.7132,  0.3745,  ..., -0.1624, -0.9996, -0.9807],\n",
              "        [-0.8004, -0.8200,  0.1699,  ..., -0.2842, -1.0000, -0.9967],\n",
              "        [-0.9218, -0.4727,  0.0819,  ..., -0.1351, -0.9997, -0.9887],\n",
              "        ...,\n",
              "        [ 0.0878,  0.4188,  0.9123,  ...,  0.7680, -0.8699, -0.5589],\n",
              "        [ 0.8177, -0.9917,  0.8741,  ..., -0.0664, -0.9994,  0.7057],\n",
              "        [-0.9116, -0.9996, -0.9913,  ..., -0.9549, -0.9827, -0.9814]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrE0sLWN2Z7C",
        "outputId": "ee0fd3a6-28c0-467f-da7c-98a238530ce1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The plus b1 we are doing we have to be carefull there because of braodcasting, we have to make sure of elment-wise addition.\n",
        "we had\n",
        "32 , 100\n",
        " 1, 100\n",
        " In this case it is correct\n",
        "'''"
      ],
      "metadata": {
        "id": "qztKHjBC2Z4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = torch.randn((100,28)) # 100 bc we have 100 inputs and 27 because we have 27 possibilities\n",
        "b2 = torch.randn(28)"
      ],
      "metadata": {
        "id": "v6tKD2NR2Z16"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = h @ W2 + b2"
      ],
      "metadata": {
        "id": "OMhciejY2Zzd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IpvKdqm2Zwv",
        "outputId": "0c276a7d-918a-4c5e-b237-332de590a3c1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = logits.exp()\n",
        "prob = counts/counts.sum(1,keepdims=True)\n",
        "prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2a171pb2ZuT",
        "outputId": "21f81993-9907-4bfd-9143-4454b61a6e6a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr0lyjQ12Zrz",
        "outputId": "dd2dba1d-898c-49d4-eddd-8849a6d2d1d7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1UMiGqK2Zp0",
        "outputId": "c4a5312f-ed1b-42a6-d9a2-7b899e70878d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1, 27, 15, 12,  9, 22,  9,  1, 27,  1, 22,  1, 27,  9, 19,\n",
              "         1,  2,  5, 12, 12,  1, 27, 19, 15, 16,  8,  9,  1, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob[torch.arange(32),Y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciDsvbDk2Znf",
        "outputId": "45206d16-d67c-4180-f1f3-5e9a3d1a7710"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4204e-09, 8.8305e-06, 2.7765e-08, 6.3467e-04, 1.2167e-07, 1.2443e-04,\n",
              "        6.3855e-06, 8.8716e-11, 8.8270e-12, 1.0872e-01, 7.5065e-07, 1.1480e-09,\n",
              "        2.2192e-02, 4.6719e-13, 3.5619e-08, 2.6184e-13, 1.0325e-02, 2.9283e-12,\n",
              "        2.5516e-08, 2.6705e-03, 3.5668e-12, 4.2790e-03, 3.5868e-06, 2.8626e-07,\n",
              "        5.6900e-07, 4.2636e-12, 2.1469e-03, 7.1463e-09, 3.0097e-11, 2.0846e-08,\n",
              "        1.0506e-06, 9.6086e-08])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = -prob[torch.arange(32),Y].log().mean()\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJzIwMSZ2Zk_",
        "outputId": "101fc7da-8d7f-4b7a-a802-7b8d36f10277"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15.8978)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYS9LADhztiQ",
        "outputId": "0a0658d7-f337-4772-8cd7-aae11f307fe2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(123)\n",
        "C = torch.randn((28,2),generator=g)\n",
        "W1 = torch.randn((6,100),generator=g)\n",
        "b1 = torch.randn(100,generator=g)\n",
        "W2 = torch.randn((100,28),generator=g)\n",
        "b2 = torch.randn(28,generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]\n"
      ],
      "metadata": {
        "id": "I1PhvEKY2ZiL"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.nelement() for p in parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIZYwv5nyrOH",
        "outputId": "ad16d174-e848-4f55-d8fc-7628954ef123"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3584"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[X]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "counts = logits.exp()\n",
        "prob = counts/counts.sum(1,keepdims=True)\n",
        "loss = -prob[torch.arange(32),Y].log().mean()\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsSJwrRBy0OJ",
        "outputId": "7031569c-ded1-4a38-a26e-507780ddc25a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.7648)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.cross_entropy(logits,Y)\n",
        "'''\n",
        "we are basically doing classification, for that there is already this efficient method is there to calculate loss, when we are doing it\n",
        "manually, we are creating counts,prob etc as new memory so we are saving the memory and also the backward pass can be much more efficient\n",
        "not only that cross_entropy is well behaved, i.e if our logits have an extreme big number exponentiating them will produce inf and getting\n",
        "the probability we get nan, wit the manual calculation, what pytorch will do is scaling of the number to eliminate the huge differences.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otCTGo1Sy0Lu",
        "outputId": "7797449b-ebf3-41dc-8e48-e5485a90ea71"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.7648)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "\n",
        "block_size = 3 # context length : how many characters do we take to predict the next one ?\n",
        "X,Y = [],[]\n",
        "for w in words:\n",
        "  # print(w)\n",
        "  context = [0] * block_size\n",
        "  for ch in list(w)+['<E>']:\n",
        "    ix = stoi[ch]\n",
        "    X.append(context)\n",
        "    Y.append(ix)\n",
        "    # print(''.join(itos[i] for i in context), '--->',itos[ix])\n",
        "    context = context[1:] + [ix]\n",
        "    # print(context)\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)"
      ],
      "metadata": {
        "id": "yQtWp6Wn_KSP"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad=True"
      ],
      "metadata": {
        "id": "q3jCa9Pr6j-a"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (100):\n",
        "  # forward pass\n",
        "  emb = C[X]\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Y)\n",
        "  print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  for p in parameters:\n",
        "    p.data -= 0.01 * p.grad\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EXi_HIfFy0JD",
        "outputId": "d5c3faa7-da44-4803-af7f-e0b152d6b22b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   4.071544170379639\n",
            "loss:   4.067666530609131\n",
            "loss:   4.06380558013916\n",
            "loss:   4.059960842132568\n",
            "loss:   4.056132793426514\n",
            "loss:   4.052320957183838\n",
            "loss:   4.048524856567383\n",
            "loss:   4.044745445251465\n",
            "loss:   4.040980815887451\n",
            "loss:   4.037232875823975\n",
            "loss:   4.033499717712402\n",
            "loss:   4.029782295227051\n",
            "loss:   4.026081085205078\n",
            "loss:   4.022394180297852\n",
            "loss:   4.018723011016846\n",
            "loss:   4.015066623687744\n",
            "loss:   4.011425495147705\n",
            "loss:   4.00779914855957\n",
            "loss:   4.00418758392334\n",
            "loss:   4.000591278076172\n",
            "loss:   3.9970085620880127\n",
            "loss:   3.993441104888916\n",
            "loss:   3.9898879528045654\n",
            "loss:   3.986349105834961\n",
            "loss:   3.9828245639801025\n",
            "loss:   3.979313850402832\n",
            "loss:   3.975817918777466\n",
            "loss:   3.97233510017395\n",
            "loss:   3.968867063522339\n",
            "loss:   3.9654128551483154\n",
            "loss:   3.9619717597961426\n",
            "loss:   3.9585447311401367\n",
            "loss:   3.9551312923431396\n",
            "loss:   3.951730966567993\n",
            "loss:   3.9483444690704346\n",
            "loss:   3.9449715614318848\n",
            "loss:   3.9416117668151855\n",
            "loss:   3.938265085220337\n",
            "loss:   3.934931755065918\n",
            "loss:   3.9316112995147705\n",
            "loss:   3.9283039569854736\n",
            "loss:   3.9250097274780273\n",
            "loss:   3.9217276573181152\n",
            "loss:   3.918459177017212\n",
            "loss:   3.915203332901001\n",
            "loss:   3.9119603633880615\n",
            "loss:   3.9087295532226562\n",
            "loss:   3.9055116176605225\n",
            "loss:   3.902306079864502\n",
            "loss:   3.899113416671753\n",
            "loss:   3.895932674407959\n",
            "loss:   3.89276385307312\n",
            "loss:   3.8896079063415527\n",
            "loss:   3.8864641189575195\n",
            "loss:   3.8833324909210205\n",
            "loss:   3.8802132606506348\n",
            "loss:   3.877105951309204\n",
            "loss:   3.8740103244781494\n",
            "loss:   3.87092661857605\n",
            "loss:   3.8678550720214844\n",
            "loss:   3.864795207977295\n",
            "loss:   3.8617472648620605\n",
            "loss:   3.858710765838623\n",
            "loss:   3.8556859493255615\n",
            "loss:   3.852673292160034\n",
            "loss:   3.8496716022491455\n",
            "loss:   3.846682071685791\n",
            "loss:   3.8437037467956543\n",
            "loss:   3.8407368659973145\n",
            "loss:   3.8377811908721924\n",
            "loss:   3.8348371982574463\n",
            "loss:   3.831904411315918\n",
            "loss:   3.8289828300476074\n",
            "loss:   3.8260726928710938\n",
            "loss:   3.823173761367798\n",
            "loss:   3.8202855587005615\n",
            "loss:   3.817408800125122\n",
            "loss:   3.8145430088043213\n",
            "loss:   3.8116886615753174\n",
            "loss:   3.808845043182373\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2035997098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As we can see, the loss is won't become a perfect zero, because of the generalization, i.e if we the 5 examples we are using, the\n",
        "sometimes after ... -> e , or ... -> o and so on.\n",
        "'''"
      ],
      "metadata": {
        "id": "BgPhl37uy0Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "And we can also observe that it is taking time for each iteration, it's because we are doing the forward pass and backward pass for the\n",
        "whole data set, what we should do is take a mini batch by randomly selecting from the dataset and use it for the forward and backward pass\n",
        "'''"
      ],
      "metadata": {
        "id": "gF_y7N9ty0EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (10000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,X.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[X[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Y[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  for p in parameters:\n",
        "    p.data += -0.01 * p.grad\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH7BZKORy0Bl",
        "outputId": "41d6cba7-0b82-4948-af24-0505728cf5bd"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5867156982421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets evaluate the loss for all of the dataset\n",
        "emb = C[X]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Y)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SyCD_I8yz--",
        "outputId": "f13dc08c-a7ba-4de1-d88d-615d440bca89"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.427185535430908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now how to decide a good learning rate, what can we do is, we can find the approximate both extremes that is the lower bound and the\n",
        "upper bound by some trail and error for example like 0.01 and 1, if we lower the lower-bound the optimization is very slow and small and\n",
        "if we increase the upper bound we are not all converging, after finding the two, we are sure that our desired learning rate is between\n",
        "0.01 and 1, so we can create some numbers using linspace and try each one of them.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jso9HTxCyz8p",
        "outputId": "9ec7ac2b-c45b-44d4-c33e-a8c0a838ead7"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = torch.linspace(0.01,1,1000)\n",
        "# lr\n",
        "lre = torch.linspace(-3,0,1000)\n",
        "lrs = 10**lre # we are writing here in the form of exponent 10^-1 = 0.01 and 10^0 is 1 (same as above) so that we can the numbers\n",
        "# exponentially\n"
      ],
      "metadata": {
        "id": "GugfheMayz6I"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lri = []\n",
        "lossi= []\n",
        "for i in range (1000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,X.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[X[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Y[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = lrs[i]\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  lri.append(lre[i])\n",
        "  lossi.append(loss.item())\n",
        "\n",
        "# print(loss.item())"
      ],
      "metadata": {
        "id": "645wD3A1L6V0"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lri,lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "fussgvdNL6SY",
        "outputId": "54d90880-cc9c-4f34-d6ec-0d47c6012bbb"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6dda8d2f30>]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "So based on the above plot, we can observe what is the optimal learning rate. Note the x-axis plot is lre that is the exponentail form\n",
        "so we have to do 10^lre to get our learning rate.\n",
        "'''"
      ],
      "metadata": {
        "id": "v0qF31EnL6QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (100000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,X.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[X[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Y[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.2 # 10^(-0.7)\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "# print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LLOinL5IL6N9",
        "outputId": "40424cf6-d24a-4485-81a8-40d464fef43d"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss:   2.1119608879089355\n",
            "loss:   2.5001072883605957\n",
            "loss:   2.3356940746307373\n",
            "loss:   2.1704764366149902\n",
            "loss:   2.2658987045288086\n",
            "loss:   2.5854287147521973\n",
            "loss:   1.9384231567382812\n",
            "loss:   2.319474458694458\n",
            "loss:   2.5582797527313232\n",
            "loss:   2.1926229000091553\n",
            "loss:   2.6309690475463867\n",
            "loss:   2.2509043216705322\n",
            "loss:   2.2050464153289795\n",
            "loss:   2.4611761569976807\n",
            "loss:   2.3483200073242188\n",
            "loss:   2.4236011505126953\n",
            "loss:   2.851881742477417\n",
            "loss:   2.462811231613159\n",
            "loss:   2.161567449569702\n",
            "loss:   2.6726646423339844\n",
            "loss:   2.2415225505828857\n",
            "loss:   2.1116724014282227\n",
            "loss:   2.544194221496582\n",
            "loss:   2.1690104007720947\n",
            "loss:   2.6213841438293457\n",
            "loss:   2.4647231101989746\n",
            "loss:   1.9446074962615967\n",
            "loss:   2.131898880004883\n",
            "loss:   2.160304546356201\n",
            "loss:   2.020883321762085\n",
            "loss:   2.1574347019195557\n",
            "loss:   2.305614709854126\n",
            "loss:   2.887406826019287\n",
            "loss:   2.370913505554199\n",
            "loss:   2.3499550819396973\n",
            "loss:   1.9367274045944214\n",
            "loss:   2.3763484954833984\n",
            "loss:   2.2238399982452393\n",
            "loss:   2.1080164909362793\n",
            "loss:   2.216296672821045\n",
            "loss:   2.35231351852417\n",
            "loss:   2.3644680976867676\n",
            "loss:   2.3216705322265625\n",
            "loss:   2.572901487350464\n",
            "loss:   2.399179697036743\n",
            "loss:   2.2785439491271973\n",
            "loss:   2.248277187347412\n",
            "loss:   2.344970226287842\n",
            "loss:   2.321599006652832\n",
            "loss:   1.9522253274917603\n",
            "loss:   2.238407611846924\n",
            "loss:   2.620770215988159\n",
            "loss:   2.6680498123168945\n",
            "loss:   2.314626693725586\n",
            "loss:   2.2114832401275635\n",
            "loss:   2.2645065784454346\n",
            "loss:   2.203937530517578\n",
            "loss:   2.456014394760132\n",
            "loss:   2.5208568572998047\n",
            "loss:   2.524024724960327\n",
            "loss:   2.3134710788726807\n",
            "loss:   2.4837348461151123\n",
            "loss:   2.5030953884124756\n",
            "loss:   2.124870777130127\n",
            "loss:   2.3898096084594727\n",
            "loss:   2.201195240020752\n",
            "loss:   2.3546605110168457\n",
            "loss:   2.7154500484466553\n",
            "loss:   2.149200677871704\n",
            "loss:   2.444340944290161\n",
            "loss:   2.4792182445526123\n",
            "loss:   2.046980142593384\n",
            "loss:   2.129009246826172\n",
            "loss:   2.4927923679351807\n",
            "loss:   1.9587428569793701\n",
            "loss:   2.2441582679748535\n",
            "loss:   2.4764978885650635\n",
            "loss:   2.3636491298675537\n",
            "loss:   2.3844823837280273\n",
            "loss:   2.2401084899902344\n",
            "loss:   2.4094338417053223\n",
            "loss:   2.3295304775238037\n",
            "loss:   2.501713991165161\n",
            "loss:   2.4551262855529785\n",
            "loss:   2.8042123317718506\n",
            "loss:   2.537588119506836\n",
            "loss:   2.218549966812134\n",
            "loss:   2.730264186859131\n",
            "loss:   2.3313121795654297\n",
            "loss:   1.8446248769760132\n",
            "loss:   2.1327295303344727\n",
            "loss:   2.234017848968506\n",
            "loss:   2.360654592514038\n",
            "loss:   2.1012802124023438\n",
            "loss:   2.024502754211426\n",
            "loss:   2.079331874847412\n",
            "loss:   2.415632724761963\n",
            "loss:   2.9856069087982178\n",
            "loss:   2.303990364074707\n",
            "loss:   2.4558846950531006\n",
            "loss:   2.076188325881958\n",
            "loss:   2.413814067840576\n",
            "loss:   2.5783183574676514\n",
            "loss:   2.260040760040283\n",
            "loss:   2.4027085304260254\n",
            "loss:   2.275831460952759\n",
            "loss:   2.068244218826294\n",
            "loss:   2.659162998199463\n",
            "loss:   2.713803768157959\n",
            "loss:   2.297468900680542\n",
            "loss:   2.6886911392211914\n",
            "loss:   2.2302467823028564\n",
            "loss:   2.1400091648101807\n",
            "loss:   2.056429862976074\n",
            "loss:   2.5178658962249756\n",
            "loss:   2.2795603275299072\n",
            "loss:   2.429884433746338\n",
            "loss:   2.4181337356567383\n",
            "loss:   2.1296560764312744\n",
            "loss:   2.8438100814819336\n",
            "loss:   2.4662585258483887\n",
            "loss:   2.084975242614746\n",
            "loss:   2.772782564163208\n",
            "loss:   2.004561185836792\n",
            "loss:   2.496678113937378\n",
            "loss:   2.248338222503662\n",
            "loss:   2.4725561141967773\n",
            "loss:   2.8149938583374023\n",
            "loss:   2.3317654132843018\n",
            "loss:   2.605175256729126\n",
            "loss:   2.229213237762451\n",
            "loss:   2.659738063812256\n",
            "loss:   2.268988847732544\n",
            "loss:   2.4498116970062256\n",
            "loss:   2.229735851287842\n",
            "loss:   2.3239312171936035\n",
            "loss:   2.2853636741638184\n",
            "loss:   2.024956226348877\n",
            "loss:   2.1255807876586914\n",
            "loss:   2.491368293762207\n",
            "loss:   2.1858720779418945\n",
            "loss:   2.778205633163452\n",
            "loss:   2.197939395904541\n",
            "loss:   2.562504768371582\n",
            "loss:   2.298405408859253\n",
            "loss:   1.833341360092163\n",
            "loss:   2.0559661388397217\n",
            "loss:   2.285231113433838\n",
            "loss:   2.5528340339660645\n",
            "loss:   2.1670358180999756\n",
            "loss:   1.9708836078643799\n",
            "loss:   2.3086142539978027\n",
            "loss:   2.287721872329712\n",
            "loss:   1.9957584142684937\n",
            "loss:   2.077924966812134\n",
            "loss:   2.4840147495269775\n",
            "loss:   2.699604034423828\n",
            "loss:   2.5555827617645264\n",
            "loss:   2.036283016204834\n",
            "loss:   2.3416051864624023\n",
            "loss:   2.304537296295166\n",
            "loss:   2.712785243988037\n",
            "loss:   2.46032977104187\n",
            "loss:   2.376883029937744\n",
            "loss:   2.249210834503174\n",
            "loss:   2.6046454906463623\n",
            "loss:   2.4483187198638916\n",
            "loss:   2.0418713092803955\n",
            "loss:   2.5033226013183594\n",
            "loss:   2.422764539718628\n",
            "loss:   2.237116575241089\n",
            "loss:   2.7618000507354736\n",
            "loss:   2.5184807777404785\n",
            "loss:   2.075605869293213\n",
            "loss:   2.1563193798065186\n",
            "loss:   2.818178415298462\n",
            "loss:   1.9246582984924316\n",
            "loss:   2.3271305561065674\n",
            "loss:   2.4456276893615723\n",
            "loss:   2.207036256790161\n",
            "loss:   2.0626795291900635\n",
            "loss:   2.0037717819213867\n",
            "loss:   2.5661540031433105\n",
            "loss:   2.237509250640869\n",
            "loss:   2.6543004512786865\n",
            "loss:   2.5440845489501953\n",
            "loss:   2.140066146850586\n",
            "loss:   2.4867968559265137\n",
            "loss:   2.269663095474243\n",
            "loss:   2.698044776916504\n",
            "loss:   1.886204481124878\n",
            "loss:   2.2517764568328857\n",
            "loss:   2.567553997039795\n",
            "loss:   2.422755479812622\n",
            "loss:   2.310633659362793\n",
            "loss:   2.113967180252075\n",
            "loss:   2.0644078254699707\n",
            "loss:   2.4420828819274902\n",
            "loss:   2.636601209640503\n",
            "loss:   2.3399300575256348\n",
            "loss:   2.148735523223877\n",
            "loss:   2.1448605060577393\n",
            "loss:   2.598827838897705\n",
            "loss:   2.5861992835998535\n",
            "loss:   2.3997700214385986\n",
            "loss:   2.1515278816223145\n",
            "loss:   2.0069797039031982\n",
            "loss:   2.544797420501709\n",
            "loss:   2.3261942863464355\n",
            "loss:   2.6532115936279297\n",
            "loss:   2.84049654006958\n",
            "loss:   2.7415997982025146\n",
            "loss:   2.052764892578125\n",
            "loss:   2.5211501121520996\n",
            "loss:   2.06434965133667\n",
            "loss:   2.693774938583374\n",
            "loss:   2.363745927810669\n",
            "loss:   2.0972306728363037\n",
            "loss:   2.346942186355591\n",
            "loss:   2.145232677459717\n",
            "loss:   2.2544846534729004\n",
            "loss:   2.709700345993042\n",
            "loss:   2.142730474472046\n",
            "loss:   2.4192073345184326\n",
            "loss:   2.2175235748291016\n",
            "loss:   2.7568886280059814\n",
            "loss:   2.14432954788208\n",
            "loss:   2.341034173965454\n",
            "loss:   1.9000816345214844\n",
            "loss:   2.517730712890625\n",
            "loss:   1.941037893295288\n",
            "loss:   2.1668107509613037\n",
            "loss:   2.129037380218506\n",
            "loss:   2.1493988037109375\n",
            "loss:   2.4914205074310303\n",
            "loss:   2.3147592544555664\n",
            "loss:   2.3018360137939453\n",
            "loss:   2.454354763031006\n",
            "loss:   2.091765880584717\n",
            "loss:   2.124511957168579\n",
            "loss:   2.349686622619629\n",
            "loss:   2.194085121154785\n",
            "loss:   2.1765925884246826\n",
            "loss:   2.3193390369415283\n",
            "loss:   2.2469658851623535\n",
            "loss:   1.9890739917755127\n",
            "loss:   2.4622881412506104\n",
            "loss:   2.8689539432525635\n",
            "loss:   2.544748306274414\n",
            "loss:   2.4271860122680664\n",
            "loss:   2.3489372730255127\n",
            "loss:   2.1246402263641357\n",
            "loss:   2.3937621116638184\n",
            "loss:   2.1395397186279297\n",
            "loss:   2.509310245513916\n",
            "loss:   2.206374406814575\n",
            "loss:   2.670435905456543\n",
            "loss:   1.9552537202835083\n",
            "loss:   2.3728489875793457\n",
            "loss:   2.4420969486236572\n",
            "loss:   2.328782081604004\n",
            "loss:   2.325427770614624\n",
            "loss:   2.1318345069885254\n",
            "loss:   2.0023131370544434\n",
            "loss:   2.7293014526367188\n",
            "loss:   2.1341843605041504\n",
            "loss:   2.317750930786133\n",
            "loss:   2.4902853965759277\n",
            "loss:   2.35854434967041\n",
            "loss:   2.873734474182129\n",
            "loss:   2.4491639137268066\n",
            "loss:   2.1367642879486084\n",
            "loss:   2.2804038524627686\n",
            "loss:   2.1471431255340576\n",
            "loss:   2.2990119457244873\n",
            "loss:   2.332413911819458\n",
            "loss:   2.3202786445617676\n",
            "loss:   2.2005834579467773\n",
            "loss:   2.584059476852417\n",
            "loss:   2.5380711555480957\n",
            "loss:   2.448247194290161\n",
            "loss:   2.618680715560913\n",
            "loss:   2.6307220458984375\n",
            "loss:   1.9914298057556152\n",
            "loss:   2.183798313140869\n",
            "loss:   2.3731367588043213\n",
            "loss:   2.318647861480713\n",
            "loss:   2.1810312271118164\n",
            "loss:   2.4150633811950684\n",
            "loss:   2.499072551727295\n",
            "loss:   2.080789089202881\n",
            "loss:   2.46809458732605\n",
            "loss:   2.19873309135437\n",
            "loss:   1.8130156993865967\n",
            "loss:   2.1434130668640137\n",
            "loss:   2.8135039806365967\n",
            "loss:   2.9538466930389404\n",
            "loss:   2.179396390914917\n",
            "loss:   2.5875954627990723\n",
            "loss:   1.910179853439331\n",
            "loss:   2.058151960372925\n",
            "loss:   2.139427661895752\n",
            "loss:   2.872286796569824\n",
            "loss:   2.3397552967071533\n",
            "loss:   2.3957712650299072\n",
            "loss:   2.3316428661346436\n",
            "loss:   2.553102731704712\n",
            "loss:   2.042259454727173\n",
            "loss:   2.2717065811157227\n",
            "loss:   2.5987613201141357\n",
            "loss:   1.8918036222457886\n",
            "loss:   2.184749126434326\n",
            "loss:   2.1477081775665283\n",
            "loss:   2.1404876708984375\n",
            "loss:   2.2349679470062256\n",
            "loss:   2.6296558380126953\n",
            "loss:   2.254530668258667\n",
            "loss:   2.5484015941619873\n",
            "loss:   2.511767625808716\n",
            "loss:   2.362464427947998\n",
            "loss:   2.513065814971924\n",
            "loss:   2.244605541229248\n",
            "loss:   2.3283424377441406\n",
            "loss:   2.0087530612945557\n",
            "loss:   2.221264362335205\n",
            "loss:   2.2709691524505615\n",
            "loss:   2.634624481201172\n",
            "loss:   2.483704090118408\n",
            "loss:   2.023501396179199\n",
            "loss:   2.362431287765503\n",
            "loss:   2.261721134185791\n",
            "loss:   2.4876575469970703\n",
            "loss:   2.4282939434051514\n",
            "loss:   2.307417392730713\n",
            "loss:   2.430332660675049\n",
            "loss:   2.379667043685913\n",
            "loss:   2.4651389122009277\n",
            "loss:   2.1839754581451416\n",
            "loss:   2.2774808406829834\n",
            "loss:   2.6907031536102295\n",
            "loss:   2.299755811691284\n",
            "loss:   2.5025582313537598\n",
            "loss:   2.4999871253967285\n",
            "loss:   2.312673568725586\n",
            "loss:   2.5471208095550537\n",
            "loss:   2.4561283588409424\n",
            "loss:   2.9019205570220947\n",
            "loss:   1.9860893487930298\n",
            "loss:   2.3027119636535645\n",
            "loss:   2.4946274757385254\n",
            "loss:   2.4463839530944824\n",
            "loss:   2.6526389122009277\n",
            "loss:   2.6375882625579834\n",
            "loss:   2.244694709777832\n",
            "loss:   2.1201300621032715\n",
            "loss:   2.4118051528930664\n",
            "loss:   2.663980484008789\n",
            "loss:   2.6500449180603027\n",
            "loss:   2.308380603790283\n",
            "loss:   2.49654221534729\n",
            "loss:   2.5676941871643066\n",
            "loss:   2.2672150135040283\n",
            "loss:   1.888700246810913\n",
            "loss:   2.7977449893951416\n",
            "loss:   2.040112018585205\n",
            "loss:   2.283409833908081\n",
            "loss:   2.3792788982391357\n",
            "loss:   2.4715304374694824\n",
            "loss:   2.015812397003174\n",
            "loss:   2.4674785137176514\n",
            "loss:   2.956350803375244\n",
            "loss:   2.1649768352508545\n",
            "loss:   2.5037474632263184\n",
            "loss:   2.588975667953491\n",
            "loss:   2.303971290588379\n",
            "loss:   2.699455738067627\n",
            "loss:   2.3214573860168457\n",
            "loss:   2.38045597076416\n",
            "loss:   1.8580045700073242\n",
            "loss:   2.4427597522735596\n",
            "loss:   2.3813023567199707\n",
            "loss:   2.5948240756988525\n",
            "loss:   2.2744569778442383\n",
            "loss:   2.426572799682617\n",
            "loss:   1.8988120555877686\n",
            "loss:   2.2553024291992188\n",
            "loss:   2.1108744144439697\n",
            "loss:   1.7914671897888184\n",
            "loss:   2.474107027053833\n",
            "loss:   2.7268319129943848\n",
            "loss:   2.1124887466430664\n",
            "loss:   2.162292242050171\n",
            "loss:   2.06463360786438\n",
            "loss:   2.4065158367156982\n",
            "loss:   1.8252218961715698\n",
            "loss:   1.8859235048294067\n",
            "loss:   2.255618095397949\n",
            "loss:   2.302858352661133\n",
            "loss:   2.2035598754882812\n",
            "loss:   2.2045016288757324\n",
            "loss:   2.063716173171997\n",
            "loss:   2.083796262741089\n",
            "loss:   2.4614956378936768\n",
            "loss:   2.311908483505249\n",
            "loss:   2.664109468460083\n",
            "loss:   2.1374714374542236\n",
            "loss:   2.0705769062042236\n",
            "loss:   2.4434261322021484\n",
            "loss:   2.190490961074829\n",
            "loss:   2.1299586296081543\n",
            "loss:   2.272291421890259\n",
            "loss:   2.4103035926818848\n",
            "loss:   2.0473687648773193\n",
            "loss:   2.3494038581848145\n",
            "loss:   2.8685109615325928\n",
            "loss:   2.9263179302215576\n",
            "loss:   2.1438331604003906\n",
            "loss:   2.4079360961914062\n",
            "loss:   2.424983024597168\n",
            "loss:   2.203042984008789\n",
            "loss:   2.590066432952881\n",
            "loss:   2.394030809402466\n",
            "loss:   2.901610851287842\n",
            "loss:   2.4433748722076416\n",
            "loss:   2.461463212966919\n",
            "loss:   2.2673099040985107\n",
            "loss:   1.9735444784164429\n",
            "loss:   2.2497596740722656\n",
            "loss:   2.1322686672210693\n",
            "loss:   2.3558058738708496\n",
            "loss:   2.6189827919006348\n",
            "loss:   2.3296706676483154\n",
            "loss:   2.4667067527770996\n",
            "loss:   2.0320210456848145\n",
            "loss:   2.0737414360046387\n",
            "loss:   2.5812714099884033\n",
            "loss:   2.4303629398345947\n",
            "loss:   2.464174270629883\n",
            "loss:   2.64105224609375\n",
            "loss:   2.1534695625305176\n",
            "loss:   2.465285539627075\n",
            "loss:   2.0039618015289307\n",
            "loss:   2.223203420639038\n",
            "loss:   1.8845410346984863\n",
            "loss:   1.8965603113174438\n",
            "loss:   2.078977584838867\n",
            "loss:   2.3058767318725586\n",
            "loss:   2.5686445236206055\n",
            "loss:   2.374358892440796\n",
            "loss:   2.7198925018310547\n",
            "loss:   2.370115280151367\n",
            "loss:   2.4190876483917236\n",
            "loss:   2.3219244480133057\n",
            "loss:   2.4809744358062744\n",
            "loss:   1.7713857889175415\n",
            "loss:   2.7385122776031494\n",
            "loss:   2.528156042098999\n",
            "loss:   2.553112268447876\n",
            "loss:   2.6784918308258057\n",
            "loss:   2.401796340942383\n",
            "loss:   2.3372445106506348\n",
            "loss:   2.5821986198425293\n",
            "loss:   2.36201810836792\n",
            "loss:   2.1260929107666016\n",
            "loss:   2.186666488647461\n",
            "loss:   2.4428224563598633\n",
            "loss:   2.6005520820617676\n",
            "loss:   2.4618937969207764\n",
            "loss:   2.244091510772705\n",
            "loss:   2.211780548095703\n",
            "loss:   2.2936220169067383\n",
            "loss:   2.333789587020874\n",
            "loss:   2.111755847930908\n",
            "loss:   1.9514293670654297\n",
            "loss:   1.9691561460494995\n",
            "loss:   3.022397518157959\n",
            "loss:   2.543006658554077\n",
            "loss:   2.6647651195526123\n",
            "loss:   2.316145896911621\n",
            "loss:   2.2320103645324707\n",
            "loss:   2.485058069229126\n",
            "loss:   1.9386132955551147\n",
            "loss:   2.619349956512451\n",
            "loss:   2.532167911529541\n",
            "loss:   2.10736346244812\n",
            "loss:   2.022753953933716\n",
            "loss:   2.1474947929382324\n",
            "loss:   2.203511953353882\n",
            "loss:   2.342332124710083\n",
            "loss:   2.3214762210845947\n",
            "loss:   2.368044137954712\n",
            "loss:   2.1663973331451416\n",
            "loss:   2.2696645259857178\n",
            "loss:   1.9511487483978271\n",
            "loss:   2.5585033893585205\n",
            "loss:   2.2088115215301514\n",
            "loss:   2.58469820022583\n",
            "loss:   2.442925214767456\n",
            "loss:   2.2493081092834473\n",
            "loss:   2.435392141342163\n",
            "loss:   2.707463026046753\n",
            "loss:   2.6212716102600098\n",
            "loss:   1.9112247228622437\n",
            "loss:   2.4107518196105957\n",
            "loss:   2.242696523666382\n",
            "loss:   2.1449618339538574\n",
            "loss:   2.440499782562256\n",
            "loss:   1.9906047582626343\n",
            "loss:   2.220710515975952\n",
            "loss:   2.4245314598083496\n",
            "loss:   2.676464080810547\n",
            "loss:   1.9786322116851807\n",
            "loss:   2.6604955196380615\n",
            "loss:   2.410940170288086\n",
            "loss:   2.3779184818267822\n",
            "loss:   2.892944574356079\n",
            "loss:   2.8493294715881348\n",
            "loss:   2.1945269107818604\n",
            "loss:   1.9997047185897827\n",
            "loss:   2.0683774948120117\n",
            "loss:   2.465270519256592\n",
            "loss:   2.1850874423980713\n",
            "loss:   2.0716466903686523\n",
            "loss:   1.9121124744415283\n",
            "loss:   2.4763948917388916\n",
            "loss:   2.3221349716186523\n",
            "loss:   2.159651041030884\n",
            "loss:   2.440410852432251\n",
            "loss:   2.3815088272094727\n",
            "loss:   2.0960724353790283\n",
            "loss:   2.0920543670654297\n",
            "loss:   2.281144618988037\n",
            "loss:   2.3126168251037598\n",
            "loss:   2.4189538955688477\n",
            "loss:   2.812870979309082\n",
            "loss:   2.481290102005005\n",
            "loss:   2.7247016429901123\n",
            "loss:   2.125863552093506\n",
            "loss:   2.4758527278900146\n",
            "loss:   1.920393705368042\n",
            "loss:   2.2533321380615234\n",
            "loss:   2.4833500385284424\n",
            "loss:   1.9649862051010132\n",
            "loss:   2.6061997413635254\n",
            "loss:   2.666654109954834\n",
            "loss:   2.6039438247680664\n",
            "loss:   2.26285982131958\n",
            "loss:   2.34765887260437\n",
            "loss:   2.3847241401672363\n",
            "loss:   2.2294583320617676\n",
            "loss:   1.8147673606872559\n",
            "loss:   2.080256700515747\n",
            "loss:   2.505624532699585\n",
            "loss:   2.489307403564453\n",
            "loss:   2.1291990280151367\n",
            "loss:   2.326174736022949\n",
            "loss:   2.052548408508301\n",
            "loss:   2.2368900775909424\n",
            "loss:   2.0533154010772705\n",
            "loss:   2.2873668670654297\n",
            "loss:   1.984237790107727\n",
            "loss:   2.27593994140625\n",
            "loss:   1.9875330924987793\n",
            "loss:   2.135775089263916\n",
            "loss:   2.151684284210205\n",
            "loss:   2.1825711727142334\n",
            "loss:   2.2797694206237793\n",
            "loss:   2.2359838485717773\n",
            "loss:   2.310993194580078\n",
            "loss:   2.246223211288452\n",
            "loss:   2.161406993865967\n",
            "loss:   2.1927342414855957\n",
            "loss:   2.6178300380706787\n",
            "loss:   2.071448564529419\n",
            "loss:   2.403977394104004\n",
            "loss:   2.105829954147339\n",
            "loss:   2.693227529525757\n",
            "loss:   2.6424124240875244\n",
            "loss:   2.459322690963745\n",
            "loss:   2.0558407306671143\n",
            "loss:   1.8763372898101807\n",
            "loss:   2.206895351409912\n",
            "loss:   2.0357272624969482\n",
            "loss:   2.24050235748291\n",
            "loss:   2.7074973583221436\n",
            "loss:   1.7639859914779663\n",
            "loss:   2.2670915126800537\n",
            "loss:   2.4549777507781982\n",
            "loss:   2.5199484825134277\n",
            "loss:   2.328752279281616\n",
            "loss:   2.5120291709899902\n",
            "loss:   2.3259623050689697\n",
            "loss:   2.3811373710632324\n",
            "loss:   2.3713219165802\n",
            "loss:   2.6097865104675293\n",
            "loss:   2.3313591480255127\n",
            "loss:   1.9133235216140747\n",
            "loss:   2.254160165786743\n",
            "loss:   2.351123809814453\n",
            "loss:   2.2072346210479736\n",
            "loss:   1.7465014457702637\n",
            "loss:   2.0442230701446533\n",
            "loss:   2.3031530380249023\n",
            "loss:   2.2782890796661377\n",
            "loss:   2.264824390411377\n",
            "loss:   2.156562089920044\n",
            "loss:   2.5411932468414307\n",
            "loss:   2.1457583904266357\n",
            "loss:   2.218294143676758\n",
            "loss:   2.5745410919189453\n",
            "loss:   2.53244948387146\n",
            "loss:   2.484743118286133\n",
            "loss:   2.5519347190856934\n",
            "loss:   2.4537670612335205\n",
            "loss:   2.248641014099121\n",
            "loss:   2.401355028152466\n",
            "loss:   2.255833387374878\n",
            "loss:   2.484041213989258\n",
            "loss:   2.0604093074798584\n",
            "loss:   2.4743778705596924\n",
            "loss:   2.2105846405029297\n",
            "loss:   2.072180986404419\n",
            "loss:   2.3302667140960693\n",
            "loss:   2.280461549758911\n",
            "loss:   2.42875599861145\n",
            "loss:   2.4303102493286133\n",
            "loss:   2.3610870838165283\n",
            "loss:   2.2945876121520996\n",
            "loss:   2.5778496265411377\n",
            "loss:   2.1910595893859863\n",
            "loss:   2.3110811710357666\n",
            "loss:   2.24206805229187\n",
            "loss:   2.3320295810699463\n",
            "loss:   2.5392496585845947\n",
            "loss:   2.1168019771575928\n",
            "loss:   3.170966625213623\n",
            "loss:   2.4342892169952393\n",
            "loss:   2.238844156265259\n",
            "loss:   2.5477964878082275\n",
            "loss:   2.7205698490142822\n",
            "loss:   2.4447689056396484\n",
            "loss:   2.213557481765747\n",
            "loss:   2.6383564472198486\n",
            "loss:   2.153823137283325\n",
            "loss:   2.5069456100463867\n",
            "loss:   2.5876338481903076\n",
            "loss:   2.0781755447387695\n",
            "loss:   2.483557939529419\n",
            "loss:   2.1942403316497803\n",
            "loss:   2.4122366905212402\n",
            "loss:   2.4108211994171143\n",
            "loss:   2.215947389602661\n",
            "loss:   2.474138021469116\n",
            "loss:   2.265010356903076\n",
            "loss:   2.4039218425750732\n",
            "loss:   2.4359285831451416\n",
            "loss:   2.0879087448120117\n",
            "loss:   2.3804659843444824\n",
            "loss:   2.2963225841522217\n",
            "loss:   2.481602668762207\n",
            "loss:   2.8665342330932617\n",
            "loss:   2.400928020477295\n",
            "loss:   2.3865811824798584\n",
            "loss:   2.235318660736084\n",
            "loss:   2.261075973510742\n",
            "loss:   2.540935754776001\n",
            "loss:   2.369431972503662\n",
            "loss:   2.054976463317871\n",
            "loss:   2.2627782821655273\n",
            "loss:   2.0134310722351074\n",
            "loss:   2.53399395942688\n",
            "loss:   2.2277908325195312\n",
            "loss:   2.2480568885803223\n",
            "loss:   2.2293145656585693\n",
            "loss:   2.3714966773986816\n",
            "loss:   1.8632152080535889\n",
            "loss:   2.2294259071350098\n",
            "loss:   2.1261632442474365\n",
            "loss:   1.7683554887771606\n",
            "loss:   2.252725839614868\n",
            "loss:   2.4723997116088867\n",
            "loss:   2.1199116706848145\n",
            "loss:   2.472707748413086\n",
            "loss:   2.3430187702178955\n",
            "loss:   2.4643726348876953\n",
            "loss:   2.1323258876800537\n",
            "loss:   2.118671417236328\n",
            "loss:   2.029670000076294\n",
            "loss:   2.68586802482605\n",
            "loss:   2.2831287384033203\n",
            "loss:   2.355698347091675\n",
            "loss:   2.0299813747406006\n",
            "loss:   2.269595146179199\n",
            "loss:   2.4475820064544678\n",
            "loss:   1.7272852659225464\n",
            "loss:   2.1273415088653564\n",
            "loss:   2.1421663761138916\n",
            "loss:   2.299947738647461\n",
            "loss:   2.1667048931121826\n",
            "loss:   2.1295273303985596\n",
            "loss:   2.353557825088501\n",
            "loss:   2.6598401069641113\n",
            "loss:   1.8769173622131348\n",
            "loss:   2.6806607246398926\n",
            "loss:   2.127342462539673\n",
            "loss:   2.002330780029297\n",
            "loss:   2.4337663650512695\n",
            "loss:   2.0712485313415527\n",
            "loss:   2.3583126068115234\n",
            "loss:   2.300774574279785\n",
            "loss:   2.198791265487671\n",
            "loss:   2.497530221939087\n",
            "loss:   2.1983559131622314\n",
            "loss:   2.1749653816223145\n",
            "loss:   2.170717716217041\n",
            "loss:   2.5682127475738525\n",
            "loss:   2.499934434890747\n",
            "loss:   2.1139755249023438\n",
            "loss:   2.804591417312622\n",
            "loss:   2.1372454166412354\n",
            "loss:   2.7024049758911133\n",
            "loss:   2.6896495819091797\n",
            "loss:   2.0572903156280518\n",
            "loss:   2.267587900161743\n",
            "loss:   2.531674861907959\n",
            "loss:   2.1192259788513184\n",
            "loss:   1.8469520807266235\n",
            "loss:   2.4780988693237305\n",
            "loss:   2.176349639892578\n",
            "loss:   2.4062302112579346\n",
            "loss:   2.0638961791992188\n",
            "loss:   2.333442211151123\n",
            "loss:   2.2833940982818604\n",
            "loss:   2.0121266841888428\n",
            "loss:   2.2731974124908447\n",
            "loss:   2.226447105407715\n",
            "loss:   2.580805540084839\n",
            "loss:   2.630810260772705\n",
            "loss:   2.677400827407837\n",
            "loss:   1.9112460613250732\n",
            "loss:   2.4580252170562744\n",
            "loss:   2.2312097549438477\n",
            "loss:   2.190251111984253\n",
            "loss:   2.3807191848754883\n",
            "loss:   2.42470383644104\n",
            "loss:   2.1846835613250732\n",
            "loss:   2.19852352142334\n",
            "loss:   2.4615299701690674\n",
            "loss:   2.5433428287506104\n",
            "loss:   2.2435553073883057\n",
            "loss:   1.988365888595581\n",
            "loss:   2.0108532905578613\n",
            "loss:   2.2062971591949463\n",
            "loss:   2.385852098464966\n",
            "loss:   2.548631429672241\n",
            "loss:   2.378335475921631\n",
            "loss:   2.3275179862976074\n",
            "loss:   2.2316300868988037\n",
            "loss:   2.1328365802764893\n",
            "loss:   2.1585609912872314\n",
            "loss:   2.4447083473205566\n",
            "loss:   2.2908174991607666\n",
            "loss:   2.51669979095459\n",
            "loss:   2.4408130645751953\n",
            "loss:   2.2423079013824463\n",
            "loss:   2.232910394668579\n",
            "loss:   2.327280044555664\n",
            "loss:   2.4863572120666504\n",
            "loss:   2.2458720207214355\n",
            "loss:   1.8419287204742432\n",
            "loss:   2.11938738822937\n",
            "loss:   1.9606513977050781\n",
            "loss:   2.6799817085266113\n",
            "loss:   2.1877217292785645\n",
            "loss:   2.5792670249938965\n",
            "loss:   2.2506155967712402\n",
            "loss:   2.6986019611358643\n",
            "loss:   2.4545180797576904\n",
            "loss:   2.1578376293182373\n",
            "loss:   2.637260913848877\n",
            "loss:   2.151475191116333\n",
            "loss:   1.7654520273208618\n",
            "loss:   2.1164865493774414\n",
            "loss:   2.215853452682495\n",
            "loss:   2.8323497772216797\n",
            "loss:   1.8682979345321655\n",
            "loss:   2.043766736984253\n",
            "loss:   2.0414485931396484\n",
            "loss:   2.5875089168548584\n",
            "loss:   2.076491117477417\n",
            "loss:   2.1085205078125\n",
            "loss:   2.5630834102630615\n",
            "loss:   2.330578565597534\n",
            "loss:   1.9477251768112183\n",
            "loss:   2.2104461193084717\n",
            "loss:   1.8617998361587524\n",
            "loss:   1.8170465230941772\n",
            "loss:   2.1254515647888184\n",
            "loss:   2.1030068397521973\n",
            "loss:   2.1955478191375732\n",
            "loss:   2.6074459552764893\n",
            "loss:   2.4235901832580566\n",
            "loss:   2.4458372592926025\n",
            "loss:   2.251863479614258\n",
            "loss:   2.5172007083892822\n",
            "loss:   2.3579328060150146\n",
            "loss:   2.0801210403442383\n",
            "loss:   2.4611778259277344\n",
            "loss:   2.075486183166504\n",
            "loss:   2.490078926086426\n",
            "loss:   2.092557191848755\n",
            "loss:   2.6643738746643066\n",
            "loss:   2.3948957920074463\n",
            "loss:   2.530884265899658\n",
            "loss:   2.7268412113189697\n",
            "loss:   2.6336686611175537\n",
            "loss:   2.3511223793029785\n",
            "loss:   2.1912918090820312\n",
            "loss:   2.261683464050293\n",
            "loss:   2.0662035942077637\n",
            "loss:   2.489011287689209\n",
            "loss:   2.7105712890625\n",
            "loss:   2.190758228302002\n",
            "loss:   2.733795166015625\n",
            "loss:   2.599313497543335\n",
            "loss:   2.4849307537078857\n",
            "loss:   2.4359853267669678\n",
            "loss:   2.7938895225524902\n",
            "loss:   2.2807397842407227\n",
            "loss:   2.568687915802002\n",
            "loss:   2.244577169418335\n",
            "loss:   2.2679874897003174\n",
            "loss:   2.3992230892181396\n",
            "loss:   2.4552078247070312\n",
            "loss:   2.2311956882476807\n",
            "loss:   2.381380796432495\n",
            "loss:   2.2595396041870117\n",
            "loss:   2.368394613265991\n",
            "loss:   2.5509674549102783\n",
            "loss:   2.488015651702881\n",
            "loss:   2.1341676712036133\n",
            "loss:   2.2739155292510986\n",
            "loss:   2.0769004821777344\n",
            "loss:   2.56699538230896\n",
            "loss:   2.4695422649383545\n",
            "loss:   1.7355977296829224\n",
            "loss:   2.0535919666290283\n",
            "loss:   2.0203475952148438\n",
            "loss:   2.399702548980713\n",
            "loss:   2.547398805618286\n",
            "loss:   2.2251696586608887\n",
            "loss:   2.1535253524780273\n",
            "loss:   2.357933759689331\n",
            "loss:   2.329806327819824\n",
            "loss:   2.334348678588867\n",
            "loss:   2.476266622543335\n",
            "loss:   2.3411548137664795\n",
            "loss:   2.283146381378174\n",
            "loss:   2.107551336288452\n",
            "loss:   2.6657073497772217\n",
            "loss:   2.4693264961242676\n",
            "loss:   2.4131343364715576\n",
            "loss:   2.1782302856445312\n",
            "loss:   2.6949684619903564\n",
            "loss:   2.071791410446167\n",
            "loss:   2.442234992980957\n",
            "loss:   2.198218822479248\n",
            "loss:   2.565953493118286\n",
            "loss:   2.2227914333343506\n",
            "loss:   2.3180012702941895\n",
            "loss:   2.2474207878112793\n",
            "loss:   2.413844585418701\n",
            "loss:   2.556267499923706\n",
            "loss:   2.2794435024261475\n",
            "loss:   2.042220115661621\n",
            "loss:   2.1911284923553467\n",
            "loss:   2.468566417694092\n",
            "loss:   2.044285297393799\n",
            "loss:   2.119555950164795\n",
            "loss:   2.6959259510040283\n",
            "loss:   2.502912759780884\n",
            "loss:   2.3884315490722656\n",
            "loss:   2.4184763431549072\n",
            "loss:   2.484171152114868\n",
            "loss:   2.1684651374816895\n",
            "loss:   2.7952113151550293\n",
            "loss:   2.334571361541748\n",
            "loss:   2.0273306369781494\n",
            "loss:   2.70265793800354\n",
            "loss:   2.521482467651367\n",
            "loss:   2.26572322845459\n",
            "loss:   2.3888044357299805\n",
            "loss:   3.1388792991638184\n",
            "loss:   2.0519840717315674\n",
            "loss:   2.5181727409362793\n",
            "loss:   2.476300001144409\n",
            "loss:   2.3976030349731445\n",
            "loss:   2.407452344894409\n",
            "loss:   2.064966917037964\n",
            "loss:   2.3541204929351807\n",
            "loss:   2.270458221435547\n",
            "loss:   2.3419313430786133\n",
            "loss:   2.374279737472534\n",
            "loss:   2.1942238807678223\n",
            "loss:   2.039562940597534\n",
            "loss:   2.400489330291748\n",
            "loss:   2.676328182220459\n",
            "loss:   2.382139205932617\n",
            "loss:   2.22615385055542\n",
            "loss:   2.693868398666382\n",
            "loss:   2.1677796840667725\n",
            "loss:   2.42578125\n",
            "loss:   2.052295684814453\n",
            "loss:   1.9555987119674683\n",
            "loss:   2.1666362285614014\n",
            "loss:   2.006314516067505\n",
            "loss:   1.9413203001022339\n",
            "loss:   2.291011095046997\n",
            "loss:   2.437415599822998\n",
            "loss:   2.513357400894165\n",
            "loss:   2.3016397953033447\n",
            "loss:   2.747427463531494\n",
            "loss:   2.223116397857666\n",
            "loss:   2.629833221435547\n",
            "loss:   2.1794393062591553\n",
            "loss:   2.532925844192505\n",
            "loss:   2.1117682456970215\n",
            "loss:   2.5049149990081787\n",
            "loss:   2.772785186767578\n",
            "loss:   2.5008790493011475\n",
            "loss:   2.346015691757202\n",
            "loss:   2.3248178958892822\n",
            "loss:   2.570089101791382\n",
            "loss:   2.232386350631714\n",
            "loss:   2.035977363586426\n",
            "loss:   2.213391065597534\n",
            "loss:   2.328725576400757\n",
            "loss:   2.1280269622802734\n",
            "loss:   1.91261887550354\n",
            "loss:   2.4559969902038574\n",
            "loss:   2.1406071186065674\n",
            "loss:   2.3753719329833984\n",
            "loss:   2.6784701347351074\n",
            "loss:   2.3879289627075195\n",
            "loss:   2.1451499462127686\n",
            "loss:   2.259387493133545\n",
            "loss:   1.9122661352157593\n",
            "loss:   2.4721906185150146\n",
            "loss:   2.1883039474487305\n",
            "loss:   1.9594858884811401\n",
            "loss:   2.255769729614258\n",
            "loss:   2.2172963619232178\n",
            "loss:   2.0376875400543213\n",
            "loss:   2.3995251655578613\n",
            "loss:   1.9957431554794312\n",
            "loss:   2.336397409439087\n",
            "loss:   2.26025128364563\n",
            "loss:   2.1746392250061035\n",
            "loss:   2.1537070274353027\n",
            "loss:   2.222721815109253\n",
            "loss:   2.5858922004699707\n",
            "loss:   2.358846664428711\n",
            "loss:   2.500551700592041\n",
            "loss:   2.372772693634033\n",
            "loss:   2.413607358932495\n",
            "loss:   2.3468174934387207\n",
            "loss:   2.3106565475463867\n",
            "loss:   2.265997886657715\n",
            "loss:   2.4192428588867188\n",
            "loss:   2.103625535964966\n",
            "loss:   1.8702579736709595\n",
            "loss:   2.460334062576294\n",
            "loss:   2.203861713409424\n",
            "loss:   2.2728917598724365\n",
            "loss:   2.310640573501587\n",
            "loss:   2.669147491455078\n",
            "loss:   2.1460013389587402\n",
            "loss:   2.3667898178100586\n",
            "loss:   2.3917837142944336\n",
            "loss:   2.4752635955810547\n",
            "loss:   2.2884421348571777\n",
            "loss:   2.2669732570648193\n",
            "loss:   2.442634344100952\n",
            "loss:   2.329604387283325\n",
            "loss:   2.036109685897827\n",
            "loss:   2.2625503540039062\n",
            "loss:   2.2564210891723633\n",
            "loss:   2.3931045532226562\n",
            "loss:   2.2555642127990723\n",
            "loss:   2.3115334510803223\n",
            "loss:   2.391451120376587\n",
            "loss:   2.141263008117676\n",
            "loss:   2.756720781326294\n",
            "loss:   2.3718628883361816\n",
            "loss:   2.503356456756592\n",
            "loss:   2.5127968788146973\n",
            "loss:   2.3389103412628174\n",
            "loss:   2.6592659950256348\n",
            "loss:   2.3381664752960205\n",
            "loss:   2.179715394973755\n",
            "loss:   2.2790119647979736\n",
            "loss:   2.625495433807373\n",
            "loss:   2.384880542755127\n",
            "loss:   2.3728134632110596\n",
            "loss:   1.731239676475525\n",
            "loss:   2.3030741214752197\n",
            "loss:   2.318593978881836\n",
            "loss:   2.273987054824829\n",
            "loss:   2.337756633758545\n",
            "loss:   2.4781057834625244\n",
            "loss:   1.9619474411010742\n",
            "loss:   2.5564091205596924\n",
            "loss:   2.299093008041382\n",
            "loss:   2.2133071422576904\n",
            "loss:   2.144146680831909\n",
            "loss:   2.0154709815979004\n",
            "loss:   2.3147823810577393\n",
            "loss:   2.6140925884246826\n",
            "loss:   1.8117456436157227\n",
            "loss:   2.502568006515503\n",
            "loss:   1.857553482055664\n",
            "loss:   2.4738669395446777\n",
            "loss:   2.0756661891937256\n",
            "loss:   2.283629894256592\n",
            "loss:   2.2627885341644287\n",
            "loss:   2.5194451808929443\n",
            "loss:   2.273848056793213\n",
            "loss:   2.0589938163757324\n",
            "loss:   2.035311698913574\n",
            "loss:   2.2359459400177\n",
            "loss:   2.652379274368286\n",
            "loss:   2.2940540313720703\n",
            "loss:   2.357388734817505\n",
            "loss:   2.017853021621704\n",
            "loss:   2.6039481163024902\n",
            "loss:   2.5399303436279297\n",
            "loss:   2.2974696159362793\n",
            "loss:   2.431711435317993\n",
            "loss:   2.507683277130127\n",
            "loss:   2.3930106163024902\n",
            "loss:   2.24595046043396\n",
            "loss:   2.3358922004699707\n",
            "loss:   2.076258659362793\n",
            "loss:   2.363319158554077\n",
            "loss:   2.2388205528259277\n",
            "loss:   2.272855043411255\n",
            "loss:   2.251256227493286\n",
            "loss:   2.5175275802612305\n",
            "loss:   2.2890822887420654\n",
            "loss:   2.3110718727111816\n",
            "loss:   2.1269586086273193\n",
            "loss:   2.261911392211914\n",
            "loss:   2.289698600769043\n",
            "loss:   2.0653464794158936\n",
            "loss:   2.0133790969848633\n",
            "loss:   2.582592725753784\n",
            "loss:   2.3626224994659424\n",
            "loss:   2.3942923545837402\n",
            "loss:   2.6675209999084473\n",
            "loss:   2.3921589851379395\n",
            "loss:   2.368504762649536\n",
            "loss:   2.308239459991455\n",
            "loss:   2.029726505279541\n",
            "loss:   2.553480386734009\n",
            "loss:   2.2669520378112793\n",
            "loss:   2.204118013381958\n",
            "loss:   2.2409656047821045\n",
            "loss:   2.153095006942749\n",
            "loss:   1.9147663116455078\n",
            "loss:   2.4367146492004395\n",
            "loss:   2.375382661819458\n",
            "loss:   2.4438624382019043\n",
            "loss:   2.3262171745300293\n",
            "loss:   2.3495025634765625\n",
            "loss:   2.662339925765991\n",
            "loss:   2.097872257232666\n",
            "loss:   2.1000266075134277\n",
            "loss:   2.2450196743011475\n",
            "loss:   2.4121086597442627\n",
            "loss:   2.244028091430664\n",
            "loss:   2.504103422164917\n",
            "loss:   2.2288119792938232\n",
            "loss:   2.147433042526245\n",
            "loss:   2.2140285968780518\n",
            "loss:   2.4988253116607666\n",
            "loss:   2.3613431453704834\n",
            "loss:   2.3970963954925537\n",
            "loss:   2.105628728866577\n",
            "loss:   2.3880887031555176\n",
            "loss:   2.4784862995147705\n",
            "loss:   2.179781913757324\n",
            "loss:   2.1099562644958496\n",
            "loss:   2.245879650115967\n",
            "loss:   2.3226282596588135\n",
            "loss:   2.4830844402313232\n",
            "loss:   2.5656282901763916\n",
            "loss:   2.4207968711853027\n",
            "loss:   2.6062850952148438\n",
            "loss:   2.2502384185791016\n",
            "loss:   2.6822965145111084\n",
            "loss:   2.219496726989746\n",
            "loss:   2.246403932571411\n",
            "loss:   2.845510482788086\n",
            "loss:   2.398120641708374\n",
            "loss:   2.358037233352661\n",
            "loss:   2.2894201278686523\n",
            "loss:   2.0282673835754395\n",
            "loss:   2.2252700328826904\n",
            "loss:   2.3850250244140625\n",
            "loss:   2.527771234512329\n",
            "loss:   2.087853193283081\n",
            "loss:   2.243809938430786\n",
            "loss:   1.9239501953125\n",
            "loss:   2.1670961380004883\n",
            "loss:   1.971145510673523\n",
            "loss:   2.381798505783081\n",
            "loss:   2.570037841796875\n",
            "loss:   2.2793960571289062\n",
            "loss:   2.3427116870880127\n",
            "loss:   2.216425895690918\n",
            "loss:   2.204612970352173\n",
            "loss:   2.6796469688415527\n",
            "loss:   2.205308675765991\n",
            "loss:   1.8385891914367676\n",
            "loss:   2.323589563369751\n",
            "loss:   1.9773716926574707\n",
            "loss:   2.438957452774048\n",
            "loss:   2.5576465129852295\n",
            "loss:   2.3837671279907227\n",
            "loss:   2.320605754852295\n",
            "loss:   1.7047107219696045\n",
            "loss:   2.34554386138916\n",
            "loss:   2.107218027114868\n",
            "loss:   2.337089776992798\n",
            "loss:   2.362658977508545\n",
            "loss:   2.06242299079895\n",
            "loss:   2.2033281326293945\n",
            "loss:   2.412888765335083\n",
            "loss:   2.5844833850860596\n",
            "loss:   2.544651508331299\n",
            "loss:   2.499419689178467\n",
            "loss:   2.27071213722229\n",
            "loss:   2.2217230796813965\n",
            "loss:   1.6484010219573975\n",
            "loss:   1.9246832132339478\n",
            "loss:   2.347532033920288\n",
            "loss:   2.2537999153137207\n",
            "loss:   2.441718816757202\n",
            "loss:   1.9653527736663818\n",
            "loss:   2.5107007026672363\n",
            "loss:   2.0468649864196777\n",
            "loss:   2.533947706222534\n",
            "loss:   2.2686102390289307\n",
            "loss:   2.2039639949798584\n",
            "loss:   2.396649122238159\n",
            "loss:   2.310868263244629\n",
            "loss:   2.3009040355682373\n",
            "loss:   2.038423776626587\n",
            "loss:   2.6717019081115723\n",
            "loss:   2.332733631134033\n",
            "loss:   2.677130699157715\n",
            "loss:   1.8681714534759521\n",
            "loss:   2.022935152053833\n",
            "loss:   2.7968673706054688\n",
            "loss:   2.825827121734619\n",
            "loss:   2.27502179145813\n",
            "loss:   2.483290195465088\n",
            "loss:   2.390915870666504\n",
            "loss:   2.2338271141052246\n",
            "loss:   2.3701605796813965\n",
            "loss:   2.4042558670043945\n",
            "loss:   2.0885603427886963\n",
            "loss:   2.636845827102661\n",
            "loss:   2.4385600090026855\n",
            "loss:   2.4585866928100586\n",
            "loss:   2.467963933944702\n",
            "loss:   2.333622455596924\n",
            "loss:   2.484553813934326\n",
            "loss:   2.323899269104004\n",
            "loss:   2.195631265640259\n",
            "loss:   2.2129697799682617\n",
            "loss:   2.228983163833618\n",
            "loss:   2.3904032707214355\n",
            "loss:   2.077713966369629\n",
            "loss:   2.7161972522735596\n",
            "loss:   2.346139430999756\n",
            "loss:   1.9701460599899292\n",
            "loss:   2.045069694519043\n",
            "loss:   2.373655319213867\n",
            "loss:   2.1078381538391113\n",
            "loss:   2.383310317993164\n",
            "loss:   2.4312939643859863\n",
            "loss:   1.8639211654663086\n",
            "loss:   2.2479629516601562\n",
            "loss:   2.293022632598877\n",
            "loss:   2.4889144897460938\n",
            "loss:   1.9658037424087524\n",
            "loss:   2.5601260662078857\n",
            "loss:   2.279747247695923\n",
            "loss:   2.196471691131592\n",
            "loss:   2.1245338916778564\n",
            "loss:   1.809423565864563\n",
            "loss:   2.0158510208129883\n",
            "loss:   2.4344124794006348\n",
            "loss:   1.9927058219909668\n",
            "loss:   2.4311063289642334\n",
            "loss:   2.4159884452819824\n",
            "loss:   2.5786972045898438\n",
            "loss:   2.361652135848999\n",
            "loss:   2.7732739448547363\n",
            "loss:   2.4009592533111572\n",
            "loss:   1.8701848983764648\n",
            "loss:   2.383251905441284\n",
            "loss:   2.294877767562866\n",
            "loss:   2.527087926864624\n",
            "loss:   2.2498040199279785\n",
            "loss:   2.3958258628845215\n",
            "loss:   2.32604718208313\n",
            "loss:   2.035813331604004\n",
            "loss:   2.7456088066101074\n",
            "loss:   2.413043737411499\n",
            "loss:   2.1998631954193115\n",
            "loss:   2.287717580795288\n",
            "loss:   2.11777400970459\n",
            "loss:   2.2936553955078125\n",
            "loss:   2.3071980476379395\n",
            "loss:   2.502486228942871\n",
            "loss:   2.481931447982788\n",
            "loss:   2.1909353733062744\n",
            "loss:   2.2567498683929443\n",
            "loss:   2.2453560829162598\n",
            "loss:   2.647535562515259\n",
            "loss:   2.4572572708129883\n",
            "loss:   2.254120349884033\n",
            "loss:   2.391197681427002\n",
            "loss:   2.481717824935913\n",
            "loss:   2.3428168296813965\n",
            "loss:   2.3312220573425293\n",
            "loss:   2.371464967727661\n",
            "loss:   2.1207528114318848\n",
            "loss:   2.4334158897399902\n",
            "loss:   2.325798988342285\n",
            "loss:   2.536600351333618\n",
            "loss:   2.305459976196289\n",
            "loss:   2.2526962757110596\n",
            "loss:   2.575317144393921\n",
            "loss:   2.5249602794647217\n",
            "loss:   2.5768697261810303\n",
            "loss:   2.532573938369751\n",
            "loss:   1.9565026760101318\n",
            "loss:   2.2358975410461426\n",
            "loss:   2.1836800575256348\n",
            "loss:   2.3055808544158936\n",
            "loss:   2.4433367252349854\n",
            "loss:   1.8536142110824585\n",
            "loss:   2.695230484008789\n",
            "loss:   2.6097450256347656\n",
            "loss:   2.6807785034179688\n",
            "loss:   2.1766855716705322\n",
            "loss:   2.3688740730285645\n",
            "loss:   2.6497080326080322\n",
            "loss:   2.303945779800415\n",
            "loss:   1.841369867324829\n",
            "loss:   2.2259366512298584\n",
            "loss:   2.052246570587158\n",
            "loss:   2.1830971240997314\n",
            "loss:   2.0657312870025635\n",
            "loss:   2.1700942516326904\n",
            "loss:   2.5198609828948975\n",
            "loss:   2.126518726348877\n",
            "loss:   2.10396671295166\n",
            "loss:   2.517216205596924\n",
            "loss:   2.187685012817383\n",
            "loss:   2.524864673614502\n",
            "loss:   2.534653663635254\n",
            "loss:   2.205096960067749\n",
            "loss:   2.54166841506958\n",
            "loss:   1.821890115737915\n",
            "loss:   2.740593671798706\n",
            "loss:   2.529921293258667\n",
            "loss:   2.252723217010498\n",
            "loss:   2.778486490249634\n",
            "loss:   1.973101258277893\n",
            "loss:   2.2545883655548096\n",
            "loss:   2.2970833778381348\n",
            "loss:   2.345198392868042\n",
            "loss:   2.522747278213501\n",
            "loss:   2.2642459869384766\n",
            "loss:   2.6517984867095947\n",
            "loss:   2.4858527183532715\n",
            "loss:   2.140307664871216\n",
            "loss:   2.4631848335266113\n",
            "loss:   2.429414749145508\n",
            "loss:   2.4130115509033203\n",
            "loss:   2.5817079544067383\n",
            "loss:   2.271066904067993\n",
            "loss:   2.23929500579834\n",
            "loss:   2.2599809169769287\n",
            "loss:   2.250864267349243\n",
            "loss:   1.9096508026123047\n",
            "loss:   2.66965389251709\n",
            "loss:   1.8692678213119507\n",
            "loss:   2.2038066387176514\n",
            "loss:   2.166980266571045\n",
            "loss:   2.5721771717071533\n",
            "loss:   2.368743419647217\n",
            "loss:   2.728900194168091\n",
            "loss:   2.5808660984039307\n",
            "loss:   2.0032074451446533\n",
            "loss:   2.315201759338379\n",
            "loss:   2.4642927646636963\n",
            "loss:   2.131361484527588\n",
            "loss:   2.6259615421295166\n",
            "loss:   2.2779343128204346\n",
            "loss:   2.1379306316375732\n",
            "loss:   2.2017602920532227\n",
            "loss:   2.6157824993133545\n",
            "loss:   1.9517155885696411\n",
            "loss:   2.4026780128479004\n",
            "loss:   2.8638510704040527\n",
            "loss:   2.659576654434204\n",
            "loss:   2.4813218116760254\n",
            "loss:   1.8922451734542847\n",
            "loss:   2.451181173324585\n",
            "loss:   2.488585948944092\n",
            "loss:   2.4392759799957275\n",
            "loss:   1.988753080368042\n",
            "loss:   2.787381172180176\n",
            "loss:   2.3584465980529785\n",
            "loss:   2.347644329071045\n",
            "loss:   2.119476318359375\n",
            "loss:   2.551272392272949\n",
            "loss:   2.0409624576568604\n",
            "loss:   2.249227523803711\n",
            "loss:   2.3807735443115234\n",
            "loss:   2.7297754287719727\n",
            "loss:   2.4390769004821777\n",
            "loss:   2.270848035812378\n",
            "loss:   2.0143253803253174\n",
            "loss:   2.3643267154693604\n",
            "loss:   2.4775784015655518\n",
            "loss:   2.2334938049316406\n",
            "loss:   2.4690821170806885\n",
            "loss:   2.4100711345672607\n",
            "loss:   2.1147968769073486\n",
            "loss:   2.6151411533355713\n",
            "loss:   2.580017328262329\n",
            "loss:   2.420987367630005\n",
            "loss:   2.401768207550049\n",
            "loss:   2.525914192199707\n",
            "loss:   2.7731423377990723\n",
            "loss:   2.1751770973205566\n",
            "loss:   2.4528908729553223\n",
            "loss:   2.6194956302642822\n",
            "loss:   2.078190803527832\n",
            "loss:   2.638782262802124\n",
            "loss:   2.3135650157928467\n",
            "loss:   2.3229219913482666\n",
            "loss:   2.2102203369140625\n",
            "loss:   2.29555082321167\n",
            "loss:   2.47468900680542\n",
            "loss:   2.670165777206421\n",
            "loss:   2.3073606491088867\n",
            "loss:   2.2908596992492676\n",
            "loss:   2.302565336227417\n",
            "loss:   2.0643789768218994\n",
            "loss:   2.3843376636505127\n",
            "loss:   2.3397891521453857\n",
            "loss:   2.4527480602264404\n",
            "loss:   2.462393283843994\n",
            "loss:   2.1127583980560303\n",
            "loss:   2.3587379455566406\n",
            "loss:   2.346792697906494\n",
            "loss:   2.4040427207946777\n",
            "loss:   2.2058589458465576\n",
            "loss:   2.2246363162994385\n",
            "loss:   2.0963659286499023\n",
            "loss:   2.3034956455230713\n",
            "loss:   2.347964286804199\n",
            "loss:   2.3526432514190674\n",
            "loss:   2.2714486122131348\n",
            "loss:   2.4599311351776123\n",
            "loss:   2.2726423740386963\n",
            "loss:   2.187580108642578\n",
            "loss:   2.360894203186035\n",
            "loss:   2.3377597332000732\n",
            "loss:   2.343518018722534\n",
            "loss:   2.3806886672973633\n",
            "loss:   2.0787558555603027\n",
            "loss:   2.502145290374756\n",
            "loss:   2.368443489074707\n",
            "loss:   2.5407350063323975\n",
            "loss:   1.8287252187728882\n",
            "loss:   2.31465220451355\n",
            "loss:   1.8845539093017578\n",
            "loss:   2.2315170764923096\n",
            "loss:   2.230977773666382\n",
            "loss:   1.9863594770431519\n",
            "loss:   2.252764940261841\n",
            "loss:   2.279043674468994\n",
            "loss:   2.342362642288208\n",
            "loss:   2.7850253582000732\n",
            "loss:   2.279118299484253\n",
            "loss:   2.3622305393218994\n",
            "loss:   2.1880946159362793\n",
            "loss:   2.44592022895813\n",
            "loss:   2.4813995361328125\n",
            "loss:   2.3236336708068848\n",
            "loss:   2.2850286960601807\n",
            "loss:   2.2916862964630127\n",
            "loss:   2.165574312210083\n",
            "loss:   2.306617259979248\n",
            "loss:   2.0451626777648926\n",
            "loss:   2.221281051635742\n",
            "loss:   2.0749123096466064\n",
            "loss:   2.654867649078369\n",
            "loss:   2.205796957015991\n",
            "loss:   2.4027538299560547\n",
            "loss:   2.1080098152160645\n",
            "loss:   2.0873310565948486\n",
            "loss:   2.4916961193084717\n",
            "loss:   1.6914992332458496\n",
            "loss:   2.249872922897339\n",
            "loss:   2.883713483810425\n",
            "loss:   2.570113182067871\n",
            "loss:   1.9546310901641846\n",
            "loss:   2.993607759475708\n",
            "loss:   2.3372437953948975\n",
            "loss:   2.120093822479248\n",
            "loss:   2.2993574142456055\n",
            "loss:   2.4740302562713623\n",
            "loss:   2.4096555709838867\n",
            "loss:   2.119391441345215\n",
            "loss:   2.4638776779174805\n",
            "loss:   2.6181230545043945\n",
            "loss:   2.1760919094085693\n",
            "loss:   2.5042977333068848\n",
            "loss:   2.053983688354492\n",
            "loss:   2.520143508911133\n",
            "loss:   2.017369270324707\n",
            "loss:   1.8902393579483032\n",
            "loss:   2.5116724967956543\n",
            "loss:   2.5442447662353516\n",
            "loss:   2.675952911376953\n",
            "loss:   2.169538974761963\n",
            "loss:   2.555976629257202\n",
            "loss:   2.615020275115967\n",
            "loss:   2.322011709213257\n",
            "loss:   2.205170154571533\n",
            "loss:   2.155205488204956\n",
            "loss:   2.2078747749328613\n",
            "loss:   2.196424722671509\n",
            "loss:   2.318100690841675\n",
            "loss:   2.499569892883301\n",
            "loss:   2.512800693511963\n",
            "loss:   2.6025989055633545\n",
            "loss:   1.9514724016189575\n",
            "loss:   2.048121690750122\n",
            "loss:   2.346808910369873\n",
            "loss:   2.577410936355591\n",
            "loss:   2.274247407913208\n",
            "loss:   2.3625082969665527\n",
            "loss:   2.022995948791504\n",
            "loss:   2.231729507446289\n",
            "loss:   2.5314624309539795\n",
            "loss:   2.1329054832458496\n",
            "loss:   2.6085727214813232\n",
            "loss:   2.7373929023742676\n",
            "loss:   1.9846351146697998\n",
            "loss:   2.3133485317230225\n",
            "loss:   2.0588619709014893\n",
            "loss:   2.880505084991455\n",
            "loss:   2.230722188949585\n",
            "loss:   2.192763328552246\n",
            "loss:   2.273526906967163\n",
            "loss:   2.05735182762146\n",
            "loss:   2.5256149768829346\n",
            "loss:   2.294055938720703\n",
            "loss:   2.3030855655670166\n",
            "loss:   2.1848883628845215\n",
            "loss:   2.4891064167022705\n",
            "loss:   2.2268528938293457\n",
            "loss:   2.203383207321167\n",
            "loss:   2.245313882827759\n",
            "loss:   2.1723361015319824\n",
            "loss:   2.3597981929779053\n",
            "loss:   2.4994022846221924\n",
            "loss:   2.613887310028076\n",
            "loss:   2.263918161392212\n",
            "loss:   2.29890513420105\n",
            "loss:   2.180962085723877\n",
            "loss:   2.131265640258789\n",
            "loss:   1.8819975852966309\n",
            "loss:   2.463622570037842\n",
            "loss:   2.3141472339630127\n",
            "loss:   1.882401943206787\n",
            "loss:   2.0543453693389893\n",
            "loss:   2.5721490383148193\n",
            "loss:   1.9596880674362183\n",
            "loss:   2.2429802417755127\n",
            "loss:   1.852182149887085\n",
            "loss:   2.1442575454711914\n",
            "loss:   2.0532257556915283\n",
            "loss:   1.8265056610107422\n",
            "loss:   2.208333969116211\n",
            "loss:   2.2258870601654053\n",
            "loss:   2.195300579071045\n",
            "loss:   2.1047327518463135\n",
            "loss:   1.9907857179641724\n",
            "loss:   2.116939067840576\n",
            "loss:   2.346907615661621\n",
            "loss:   2.3951690196990967\n",
            "loss:   2.8224844932556152\n",
            "loss:   2.5747692584991455\n",
            "loss:   2.200476884841919\n",
            "loss:   2.4605579376220703\n",
            "loss:   2.261582612991333\n",
            "loss:   2.186414957046509\n",
            "loss:   2.6355621814727783\n",
            "loss:   2.443781852722168\n",
            "loss:   2.295884132385254\n",
            "loss:   2.252415657043457\n",
            "loss:   2.2834761142730713\n",
            "loss:   2.331865072250366\n",
            "loss:   2.1485435962677\n",
            "loss:   2.3192992210388184\n",
            "loss:   1.9152705669403076\n",
            "loss:   2.4935741424560547\n",
            "loss:   2.200634717941284\n",
            "loss:   2.3298261165618896\n",
            "loss:   2.4002153873443604\n",
            "loss:   2.6957626342773438\n",
            "loss:   2.5091185569763184\n",
            "loss:   2.7287607192993164\n",
            "loss:   2.176135778427124\n",
            "loss:   2.083073616027832\n",
            "loss:   2.2172508239746094\n",
            "loss:   2.2391045093536377\n",
            "loss:   2.070329427719116\n",
            "loss:   2.1923258304595947\n",
            "loss:   2.8246707916259766\n",
            "loss:   2.0856242179870605\n",
            "loss:   2.4883675575256348\n",
            "loss:   2.239121198654175\n",
            "loss:   2.5403664112091064\n",
            "loss:   2.201814889907837\n",
            "loss:   2.557765245437622\n",
            "loss:   2.346214532852173\n",
            "loss:   2.0140702724456787\n",
            "loss:   2.1370208263397217\n",
            "loss:   2.007023334503174\n",
            "loss:   2.5858044624328613\n",
            "loss:   2.4581661224365234\n",
            "loss:   2.062189817428589\n",
            "loss:   2.2824766635894775\n",
            "loss:   1.8830207586288452\n",
            "loss:   1.9937412738800049\n",
            "loss:   2.1931512355804443\n",
            "loss:   2.365579128265381\n",
            "loss:   2.5174293518066406\n",
            "loss:   1.9924734830856323\n",
            "loss:   2.4534401893615723\n",
            "loss:   2.6311001777648926\n",
            "loss:   2.03069806098938\n",
            "loss:   2.431173801422119\n",
            "loss:   2.329012393951416\n",
            "loss:   1.8366360664367676\n",
            "loss:   2.2419745922088623\n",
            "loss:   2.5465378761291504\n",
            "loss:   2.105336904525757\n",
            "loss:   2.5995230674743652\n",
            "loss:   2.292205333709717\n",
            "loss:   2.1384336948394775\n",
            "loss:   2.591662883758545\n",
            "loss:   2.311365842819214\n",
            "loss:   2.3749942779541016\n",
            "loss:   2.237706422805786\n",
            "loss:   2.322336196899414\n",
            "loss:   2.377472162246704\n",
            "loss:   2.7743263244628906\n",
            "loss:   2.545840263366699\n",
            "loss:   2.3958096504211426\n",
            "loss:   2.1817755699157715\n",
            "loss:   2.168436050415039\n",
            "loss:   1.8896809816360474\n",
            "loss:   2.108903169631958\n",
            "loss:   2.4248881340026855\n",
            "loss:   2.6616570949554443\n",
            "loss:   2.436591625213623\n",
            "loss:   2.5425033569335938\n",
            "loss:   2.1587371826171875\n",
            "loss:   2.263643980026245\n",
            "loss:   2.642026901245117\n",
            "loss:   1.9610769748687744\n",
            "loss:   2.2923033237457275\n",
            "loss:   2.496917486190796\n",
            "loss:   2.664600372314453\n",
            "loss:   2.0401034355163574\n",
            "loss:   2.3604753017425537\n",
            "loss:   2.1919877529144287\n",
            "loss:   2.4768619537353516\n",
            "loss:   2.19584321975708\n",
            "loss:   2.5135345458984375\n",
            "loss:   2.157015085220337\n",
            "loss:   2.24163818359375\n",
            "loss:   1.9599584341049194\n",
            "loss:   2.1753039360046387\n",
            "loss:   2.4507739543914795\n",
            "loss:   2.1773364543914795\n",
            "loss:   2.532001256942749\n",
            "loss:   2.746530055999756\n",
            "loss:   2.215228319168091\n",
            "loss:   2.2315099239349365\n",
            "loss:   2.2592310905456543\n",
            "loss:   2.336101770401001\n",
            "loss:   2.4303019046783447\n",
            "loss:   2.5194313526153564\n",
            "loss:   2.4343819618225098\n",
            "loss:   2.6964612007141113\n",
            "loss:   2.4113149642944336\n",
            "loss:   2.492567300796509\n",
            "loss:   2.6377499103546143\n",
            "loss:   2.368072986602783\n",
            "loss:   2.199474573135376\n",
            "loss:   2.0154364109039307\n",
            "loss:   2.162792444229126\n",
            "loss:   2.105733871459961\n",
            "loss:   2.3034725189208984\n",
            "loss:   2.7325334548950195\n",
            "loss:   2.1707265377044678\n",
            "loss:   2.5135176181793213\n",
            "loss:   2.628504753112793\n",
            "loss:   2.5749473571777344\n",
            "loss:   2.2899317741394043\n",
            "loss:   1.9697035551071167\n",
            "loss:   2.155923843383789\n",
            "loss:   2.103045701980591\n",
            "loss:   2.152406692504883\n",
            "loss:   2.5200512409210205\n",
            "loss:   2.250102996826172\n",
            "loss:   2.4002091884613037\n",
            "loss:   2.359022378921509\n",
            "loss:   2.114262104034424\n",
            "loss:   2.168001413345337\n",
            "loss:   2.602153778076172\n",
            "loss:   2.3550095558166504\n",
            "loss:   2.0176162719726562\n",
            "loss:   2.3094520568847656\n",
            "loss:   2.3136539459228516\n",
            "loss:   2.437203884124756\n",
            "loss:   2.759930372238159\n",
            "loss:   2.2361671924591064\n",
            "loss:   2.806753396987915\n",
            "loss:   2.654313087463379\n",
            "loss:   2.2554285526275635\n",
            "loss:   2.1897332668304443\n",
            "loss:   2.743415594100952\n",
            "loss:   2.7433924674987793\n",
            "loss:   2.183983325958252\n",
            "loss:   2.310678005218506\n",
            "loss:   2.273134708404541\n",
            "loss:   2.2694156169891357\n",
            "loss:   2.6081807613372803\n",
            "loss:   2.0492889881134033\n",
            "loss:   2.2435302734375\n",
            "loss:   2.321441888809204\n",
            "loss:   2.7543439865112305\n",
            "loss:   2.195124864578247\n",
            "loss:   2.3159680366516113\n",
            "loss:   2.2089223861694336\n",
            "loss:   2.262122869491577\n",
            "loss:   1.8318274021148682\n",
            "loss:   2.2077317237854004\n",
            "loss:   2.39864444732666\n",
            "loss:   2.2677533626556396\n",
            "loss:   2.8173441886901855\n",
            "loss:   2.1313624382019043\n",
            "loss:   2.20475697517395\n",
            "loss:   2.4011049270629883\n",
            "loss:   2.5481553077697754\n",
            "loss:   2.2335822582244873\n",
            "loss:   2.7950940132141113\n",
            "loss:   2.740729570388794\n",
            "loss:   1.941348671913147\n",
            "loss:   2.1450979709625244\n",
            "loss:   1.8873547315597534\n",
            "loss:   1.9209132194519043\n",
            "loss:   1.96852445602417\n",
            "loss:   2.1698670387268066\n",
            "loss:   2.6942760944366455\n",
            "loss:   2.251997470855713\n",
            "loss:   2.103212594985962\n",
            "loss:   2.622223138809204\n",
            "loss:   2.5003433227539062\n",
            "loss:   2.412797212600708\n",
            "loss:   2.3402292728424072\n",
            "loss:   2.2231764793395996\n",
            "loss:   2.2153851985931396\n",
            "loss:   2.4070889949798584\n",
            "loss:   2.2693212032318115\n",
            "loss:   2.281628370285034\n",
            "loss:   2.5375936031341553\n",
            "loss:   2.5644187927246094\n",
            "loss:   1.9591553211212158\n",
            "loss:   2.469672203063965\n",
            "loss:   2.300152063369751\n",
            "loss:   2.338202476501465\n",
            "loss:   2.45070481300354\n",
            "loss:   2.4986650943756104\n",
            "loss:   2.7033581733703613\n",
            "loss:   2.0730531215667725\n",
            "loss:   2.3621041774749756\n",
            "loss:   2.0279159545898438\n",
            "loss:   2.460663318634033\n",
            "loss:   2.074747085571289\n",
            "loss:   2.16313099861145\n",
            "loss:   2.754359722137451\n",
            "loss:   2.5526206493377686\n",
            "loss:   2.324857473373413\n",
            "loss:   2.57362961769104\n",
            "loss:   2.5415701866149902\n",
            "loss:   2.1728365421295166\n",
            "loss:   2.8046860694885254\n",
            "loss:   2.0534627437591553\n",
            "loss:   1.939207911491394\n",
            "loss:   2.328227996826172\n",
            "loss:   2.3332650661468506\n",
            "loss:   2.108006000518799\n",
            "loss:   1.715847134590149\n",
            "loss:   2.214292526245117\n",
            "loss:   2.267740249633789\n",
            "loss:   2.414445638656616\n",
            "loss:   2.525904417037964\n",
            "loss:   2.270493507385254\n",
            "loss:   1.9198474884033203\n",
            "loss:   1.5815489292144775\n",
            "loss:   2.3753671646118164\n",
            "loss:   2.4034745693206787\n",
            "loss:   2.1967215538024902\n",
            "loss:   2.1617493629455566\n",
            "loss:   2.5085365772247314\n",
            "loss:   2.104842185974121\n",
            "loss:   2.667283773422241\n",
            "loss:   2.3304243087768555\n",
            "loss:   2.3333547115325928\n",
            "loss:   2.232646942138672\n",
            "loss:   1.8956636190414429\n",
            "loss:   2.336153268814087\n",
            "loss:   2.3420896530151367\n",
            "loss:   2.0625061988830566\n",
            "loss:   2.4138967990875244\n",
            "loss:   2.266737461090088\n",
            "loss:   2.564210891723633\n",
            "loss:   2.5988078117370605\n",
            "loss:   2.4880619049072266\n",
            "loss:   2.151273250579834\n",
            "loss:   2.5650851726531982\n",
            "loss:   2.3822286128997803\n",
            "loss:   2.5288009643554688\n",
            "loss:   2.452108860015869\n",
            "loss:   2.2446322441101074\n",
            "loss:   2.445719003677368\n",
            "loss:   2.0721230506896973\n",
            "loss:   2.0027101039886475\n",
            "loss:   2.3970694541931152\n",
            "loss:   2.2526493072509766\n",
            "loss:   2.1794278621673584\n",
            "loss:   2.422605514526367\n",
            "loss:   2.444493055343628\n",
            "loss:   2.2757318019866943\n",
            "loss:   2.0907671451568604\n",
            "loss:   2.312452554702759\n",
            "loss:   2.630819082260132\n",
            "loss:   2.1718008518218994\n",
            "loss:   2.525632381439209\n",
            "loss:   2.6748340129852295\n",
            "loss:   2.160000801086426\n",
            "loss:   1.9445000886917114\n",
            "loss:   2.4369101524353027\n",
            "loss:   2.2249674797058105\n",
            "loss:   2.724123477935791\n",
            "loss:   2.5109310150146484\n",
            "loss:   2.178901195526123\n",
            "loss:   2.754406213760376\n",
            "loss:   2.6104066371917725\n",
            "loss:   2.3342721462249756\n",
            "loss:   2.632235527038574\n",
            "loss:   1.9157317876815796\n",
            "loss:   2.3993124961853027\n",
            "loss:   2.767941474914551\n",
            "loss:   2.158379077911377\n",
            "loss:   2.3446547985076904\n",
            "loss:   2.3635003566741943\n",
            "loss:   2.3421244621276855\n",
            "loss:   2.422725200653076\n",
            "loss:   2.5743958950042725\n",
            "loss:   2.307492733001709\n",
            "loss:   2.267334222793579\n",
            "loss:   2.4807441234588623\n",
            "loss:   2.439328193664551\n",
            "loss:   2.787374496459961\n",
            "loss:   2.3542234897613525\n",
            "loss:   2.3424556255340576\n",
            "loss:   1.8811736106872559\n",
            "loss:   2.262932538986206\n",
            "loss:   2.5795681476593018\n",
            "loss:   2.4574813842773438\n",
            "loss:   2.487811803817749\n",
            "loss:   2.3389856815338135\n",
            "loss:   2.2344419956207275\n",
            "loss:   2.374988317489624\n",
            "loss:   2.2068369388580322\n",
            "loss:   2.7558205127716064\n",
            "loss:   2.75754451751709\n",
            "loss:   2.1713814735412598\n",
            "loss:   2.0151004791259766\n",
            "loss:   2.2591729164123535\n",
            "loss:   2.2877864837646484\n",
            "loss:   2.241987466812134\n",
            "loss:   2.260467529296875\n",
            "loss:   2.162095546722412\n",
            "loss:   2.419766426086426\n",
            "loss:   2.0483977794647217\n",
            "loss:   2.180500030517578\n",
            "loss:   2.5273804664611816\n",
            "loss:   2.3165225982666016\n",
            "loss:   2.9246397018432617\n",
            "loss:   2.148409605026245\n",
            "loss:   2.036496639251709\n",
            "loss:   2.2011518478393555\n",
            "loss:   2.4271421432495117\n",
            "loss:   2.223618268966675\n",
            "loss:   2.2052268981933594\n",
            "loss:   2.4935715198516846\n",
            "loss:   2.0685458183288574\n",
            "loss:   2.1372523307800293\n",
            "loss:   2.358081340789795\n",
            "loss:   2.095604181289673\n",
            "loss:   2.053687334060669\n",
            "loss:   2.4947054386138916\n",
            "loss:   2.604724407196045\n",
            "loss:   2.436664342880249\n",
            "loss:   2.1257855892181396\n",
            "loss:   2.114509344100952\n",
            "loss:   1.944385290145874\n",
            "loss:   2.456242799758911\n",
            "loss:   2.355196237564087\n",
            "loss:   2.3935468196868896\n",
            "loss:   2.6414401531219482\n",
            "loss:   2.5425665378570557\n",
            "loss:   2.603987455368042\n",
            "loss:   2.3274922370910645\n",
            "loss:   2.1988611221313477\n",
            "loss:   2.5108683109283447\n",
            "loss:   2.276332139968872\n",
            "loss:   2.051048517227173\n",
            "loss:   2.8311893939971924\n",
            "loss:   2.253803253173828\n",
            "loss:   2.1959967613220215\n",
            "loss:   2.2889938354492188\n",
            "loss:   2.4196624755859375\n",
            "loss:   2.3874447345733643\n",
            "loss:   2.606267213821411\n",
            "loss:   2.5640668869018555\n",
            "loss:   2.0791141986846924\n",
            "loss:   2.2073757648468018\n",
            "loss:   2.2008514404296875\n",
            "loss:   2.1407299041748047\n",
            "loss:   2.6058526039123535\n",
            "loss:   2.1513967514038086\n",
            "loss:   2.346893787384033\n",
            "loss:   2.2521512508392334\n",
            "loss:   2.1156678199768066\n",
            "loss:   1.8431249856948853\n",
            "loss:   2.3666141033172607\n",
            "loss:   2.3355982303619385\n",
            "loss:   2.3951432704925537\n",
            "loss:   2.1983110904693604\n",
            "loss:   2.58101224899292\n",
            "loss:   2.1945924758911133\n",
            "loss:   2.399275064468384\n",
            "loss:   2.307133674621582\n",
            "loss:   2.3521854877471924\n",
            "loss:   2.5045840740203857\n",
            "loss:   2.407788038253784\n",
            "loss:   2.0940823554992676\n",
            "loss:   2.2713840007781982\n",
            "loss:   1.8836512565612793\n",
            "loss:   2.0350632667541504\n",
            "loss:   2.2404916286468506\n",
            "loss:   2.4793331623077393\n",
            "loss:   2.358344793319702\n",
            "loss:   2.165759801864624\n",
            "loss:   2.274178981781006\n",
            "loss:   1.9927574396133423\n",
            "loss:   2.6920361518859863\n",
            "loss:   2.7417397499084473\n",
            "loss:   2.6860475540161133\n",
            "loss:   2.1936378479003906\n",
            "loss:   2.226404905319214\n",
            "loss:   2.3619368076324463\n",
            "loss:   2.5511348247528076\n",
            "loss:   2.3305883407592773\n",
            "loss:   2.119292736053467\n",
            "loss:   2.874669313430786\n",
            "loss:   2.3591861724853516\n",
            "loss:   2.4205706119537354\n",
            "loss:   2.3720245361328125\n",
            "loss:   2.371194362640381\n",
            "loss:   1.7812069654464722\n",
            "loss:   2.3901333808898926\n",
            "loss:   2.2823400497436523\n",
            "loss:   2.3645379543304443\n",
            "loss:   2.2372536659240723\n",
            "loss:   2.3875505924224854\n",
            "loss:   2.2064692974090576\n",
            "loss:   2.2762703895568848\n",
            "loss:   2.2445762157440186\n",
            "loss:   2.438347339630127\n",
            "loss:   2.4524030685424805\n",
            "loss:   2.344456434249878\n",
            "loss:   2.778881311416626\n",
            "loss:   2.509946823120117\n",
            "loss:   2.591492176055908\n",
            "loss:   2.3841068744659424\n",
            "loss:   2.205207347869873\n",
            "loss:   2.136303186416626\n",
            "loss:   2.2476305961608887\n",
            "loss:   2.344780921936035\n",
            "loss:   2.429062843322754\n",
            "loss:   2.465202569961548\n",
            "loss:   2.839998245239258\n",
            "loss:   2.426509380340576\n",
            "loss:   2.438464641571045\n",
            "loss:   2.3661842346191406\n",
            "loss:   2.243713140487671\n",
            "loss:   2.8522298336029053\n",
            "loss:   2.403019905090332\n",
            "loss:   2.425894021987915\n",
            "loss:   2.3518998622894287\n",
            "loss:   2.337246894836426\n",
            "loss:   2.292728900909424\n",
            "loss:   2.5062150955200195\n",
            "loss:   2.2611329555511475\n",
            "loss:   1.9810571670532227\n",
            "loss:   2.4974827766418457\n",
            "loss:   2.6075210571289062\n",
            "loss:   2.8033978939056396\n",
            "loss:   2.658972978591919\n",
            "loss:   2.5367329120635986\n",
            "loss:   2.8200981616973877\n",
            "loss:   1.9517625570297241\n",
            "loss:   2.5327470302581787\n",
            "loss:   2.7356646060943604\n",
            "loss:   2.1686899662017822\n",
            "loss:   2.1581122875213623\n",
            "loss:   2.2235684394836426\n",
            "loss:   2.3504178524017334\n",
            "loss:   2.4259657859802246\n",
            "loss:   2.4701828956604004\n",
            "loss:   2.0751733779907227\n",
            "loss:   2.363931894302368\n",
            "loss:   2.413501501083374\n",
            "loss:   2.657003879547119\n",
            "loss:   1.9929280281066895\n",
            "loss:   2.3719217777252197\n",
            "loss:   2.5312893390655518\n",
            "loss:   2.0987491607666016\n",
            "loss:   2.436408281326294\n",
            "loss:   2.1481382846832275\n",
            "loss:   2.0930898189544678\n",
            "loss:   2.6882641315460205\n",
            "loss:   2.226292610168457\n",
            "loss:   2.339984655380249\n",
            "loss:   2.5625767707824707\n",
            "loss:   2.5542473793029785\n",
            "loss:   2.2300145626068115\n",
            "loss:   2.3746321201324463\n",
            "loss:   2.3568365573883057\n",
            "loss:   2.231165647506714\n",
            "loss:   2.6117498874664307\n",
            "loss:   2.2484405040740967\n",
            "loss:   2.358687162399292\n",
            "loss:   1.9089295864105225\n",
            "loss:   2.4491922855377197\n",
            "loss:   2.420341730117798\n",
            "loss:   2.3873608112335205\n",
            "loss:   2.5752830505371094\n",
            "loss:   2.433058023452759\n",
            "loss:   2.170734167098999\n",
            "loss:   2.6844091415405273\n",
            "loss:   2.289579391479492\n",
            "loss:   2.34314227104187\n",
            "loss:   2.1722042560577393\n",
            "loss:   1.9016488790512085\n",
            "loss:   2.53965425491333\n",
            "loss:   1.9877824783325195\n",
            "loss:   2.420201539993286\n",
            "loss:   2.5249104499816895\n",
            "loss:   2.1601827144622803\n",
            "loss:   1.9183285236358643\n",
            "loss:   2.485504388809204\n",
            "loss:   2.432767391204834\n",
            "loss:   2.558854103088379\n",
            "loss:   2.2701869010925293\n",
            "loss:   2.824443817138672\n",
            "loss:   2.233828067779541\n",
            "loss:   2.2883307933807373\n",
            "loss:   2.512707471847534\n",
            "loss:   2.6614091396331787\n",
            "loss:   2.440098524093628\n",
            "loss:   2.1794607639312744\n",
            "loss:   2.3444042205810547\n",
            "loss:   2.6225340366363525\n",
            "loss:   2.1889853477478027\n",
            "loss:   2.3647453784942627\n",
            "loss:   2.3322911262512207\n",
            "loss:   2.1398706436157227\n",
            "loss:   2.4899728298187256\n",
            "loss:   2.3273770809173584\n",
            "loss:   2.251178503036499\n",
            "loss:   2.0739874839782715\n",
            "loss:   2.0069761276245117\n",
            "loss:   2.128908634185791\n",
            "loss:   2.1318843364715576\n",
            "loss:   2.1849095821380615\n",
            "loss:   2.135662317276001\n",
            "loss:   2.4318432807922363\n",
            "loss:   2.439082145690918\n",
            "loss:   2.490128517150879\n",
            "loss:   2.230545997619629\n",
            "loss:   2.425250768661499\n",
            "loss:   2.24296236038208\n",
            "loss:   2.5661141872406006\n",
            "loss:   2.3425354957580566\n",
            "loss:   2.6040091514587402\n",
            "loss:   2.2257726192474365\n",
            "loss:   1.9897114038467407\n",
            "loss:   2.189159393310547\n",
            "loss:   2.3518991470336914\n",
            "loss:   2.272974967956543\n",
            "loss:   2.0687787532806396\n",
            "loss:   2.4925119876861572\n",
            "loss:   2.2623672485351562\n",
            "loss:   2.3596932888031006\n",
            "loss:   2.302992105484009\n",
            "loss:   2.3561248779296875\n",
            "loss:   2.090449333190918\n",
            "loss:   2.3938968181610107\n",
            "loss:   2.6450343132019043\n",
            "loss:   1.9929842948913574\n",
            "loss:   2.556133985519409\n",
            "loss:   1.7474459409713745\n",
            "loss:   2.2716517448425293\n",
            "loss:   2.348748207092285\n",
            "loss:   2.215602159500122\n",
            "loss:   2.0899057388305664\n",
            "loss:   2.58712100982666\n",
            "loss:   2.203827381134033\n",
            "loss:   2.5554418563842773\n",
            "loss:   2.7187211513519287\n",
            "loss:   2.1312859058380127\n",
            "loss:   2.116797924041748\n",
            "loss:   2.7151055335998535\n",
            "loss:   2.3276586532592773\n",
            "loss:   2.217024326324463\n",
            "loss:   2.2432992458343506\n",
            "loss:   2.5548174381256104\n",
            "loss:   2.008399724960327\n",
            "loss:   2.4314138889312744\n",
            "loss:   2.7070624828338623\n",
            "loss:   2.076542377471924\n",
            "loss:   1.7588638067245483\n",
            "loss:   2.15861439704895\n",
            "loss:   2.620741128921509\n",
            "loss:   2.4108996391296387\n",
            "loss:   2.1871836185455322\n",
            "loss:   2.5126450061798096\n",
            "loss:   2.0401933193206787\n",
            "loss:   2.0776376724243164\n",
            "loss:   2.2917113304138184\n",
            "loss:   2.3991551399230957\n",
            "loss:   2.179884910583496\n",
            "loss:   2.2358462810516357\n",
            "loss:   2.539764404296875\n",
            "loss:   2.5263254642486572\n",
            "loss:   2.221324920654297\n",
            "loss:   2.7048752307891846\n",
            "loss:   2.4949374198913574\n",
            "loss:   2.3934667110443115\n",
            "loss:   2.507211208343506\n",
            "loss:   2.1719295978546143\n",
            "loss:   2.529111623764038\n",
            "loss:   2.046848773956299\n",
            "loss:   1.8683035373687744\n",
            "loss:   2.2112069129943848\n",
            "loss:   2.5260672569274902\n",
            "loss:   2.4772562980651855\n",
            "loss:   2.3484771251678467\n",
            "loss:   2.384977102279663\n",
            "loss:   2.135913133621216\n",
            "loss:   2.422070026397705\n",
            "loss:   2.4106690883636475\n",
            "loss:   2.32249116897583\n",
            "loss:   2.286158323287964\n",
            "loss:   2.3191115856170654\n",
            "loss:   2.163325071334839\n",
            "loss:   2.1372885704040527\n",
            "loss:   2.7972991466522217\n",
            "loss:   2.3803040981292725\n",
            "loss:   2.1542303562164307\n",
            "loss:   2.39961314201355\n",
            "loss:   2.765775680541992\n",
            "loss:   2.2162556648254395\n",
            "loss:   2.4957311153411865\n",
            "loss:   2.3880865573883057\n",
            "loss:   1.9878208637237549\n",
            "loss:   2.4831037521362305\n",
            "loss:   2.5303897857666016\n",
            "loss:   2.3901612758636475\n",
            "loss:   2.3963639736175537\n",
            "loss:   2.2797858715057373\n",
            "loss:   2.4900145530700684\n",
            "loss:   2.355308771133423\n",
            "loss:   2.4746475219726562\n",
            "loss:   2.2820122241973877\n",
            "loss:   2.6314361095428467\n",
            "loss:   2.34090518951416\n",
            "loss:   2.920741081237793\n",
            "loss:   2.072774887084961\n",
            "loss:   2.3565993309020996\n",
            "loss:   2.5086209774017334\n",
            "loss:   2.3456695079803467\n",
            "loss:   2.746323347091675\n",
            "loss:   2.535977363586426\n",
            "loss:   2.3791422843933105\n",
            "loss:   2.442396640777588\n",
            "loss:   2.035140037536621\n",
            "loss:   2.1728155612945557\n",
            "loss:   2.095787763595581\n",
            "loss:   2.4572150707244873\n",
            "loss:   2.248788595199585\n",
            "loss:   2.640928268432617\n",
            "loss:   2.401228189468384\n",
            "loss:   2.360670566558838\n",
            "loss:   2.2831294536590576\n",
            "loss:   2.410059690475464\n",
            "loss:   2.678865671157837\n",
            "loss:   2.3293697834014893\n",
            "loss:   2.2895026206970215\n",
            "loss:   2.251328945159912\n",
            "loss:   2.078382968902588\n",
            "loss:   2.3000364303588867\n",
            "loss:   2.5704007148742676\n",
            "loss:   2.222024917602539\n",
            "loss:   2.3913991451263428\n",
            "loss:   2.222355604171753\n",
            "loss:   2.7683188915252686\n",
            "loss:   2.372772216796875\n",
            "loss:   2.3436968326568604\n",
            "loss:   2.7143466472625732\n",
            "loss:   2.3585808277130127\n",
            "loss:   2.4867782592773438\n",
            "loss:   2.685015916824341\n",
            "loss:   2.4662864208221436\n",
            "loss:   2.3022162914276123\n",
            "loss:   1.930456280708313\n",
            "loss:   2.4596548080444336\n",
            "loss:   2.111351728439331\n",
            "loss:   2.590860366821289\n",
            "loss:   2.38183331489563\n",
            "loss:   2.30415415763855\n",
            "loss:   2.1919469833374023\n",
            "loss:   2.2931225299835205\n",
            "loss:   2.348863124847412\n",
            "loss:   2.01710844039917\n",
            "loss:   2.1413304805755615\n",
            "loss:   2.5755631923675537\n",
            "loss:   2.223665952682495\n",
            "loss:   2.5945656299591064\n",
            "loss:   2.4072318077087402\n",
            "loss:   2.1534547805786133\n",
            "loss:   2.416468381881714\n",
            "loss:   2.45023775100708\n",
            "loss:   2.1303493976593018\n",
            "loss:   3.1468136310577393\n",
            "loss:   2.537277936935425\n",
            "loss:   2.17744517326355\n",
            "loss:   2.2858965396881104\n",
            "loss:   2.593003034591675\n",
            "loss:   2.292494773864746\n",
            "loss:   2.729400157928467\n",
            "loss:   2.376767635345459\n",
            "loss:   2.3454806804656982\n",
            "loss:   2.3284072875976562\n",
            "loss:   2.2262744903564453\n",
            "loss:   2.5110201835632324\n",
            "loss:   2.570570468902588\n",
            "loss:   2.3438971042633057\n",
            "loss:   2.772233009338379\n",
            "loss:   2.4080114364624023\n",
            "loss:   2.1602747440338135\n",
            "loss:   2.541339874267578\n",
            "loss:   2.2240641117095947\n",
            "loss:   2.298325538635254\n",
            "loss:   2.279872179031372\n",
            "loss:   2.1422433853149414\n",
            "loss:   2.370412588119507\n",
            "loss:   2.010014533996582\n",
            "loss:   2.3051247596740723\n",
            "loss:   2.3345258235931396\n",
            "loss:   2.4050071239471436\n",
            "loss:   2.1511449813842773\n",
            "loss:   2.0704078674316406\n",
            "loss:   2.38627290725708\n",
            "loss:   2.6325149536132812\n",
            "loss:   2.3969759941101074\n",
            "loss:   2.3823885917663574\n",
            "loss:   2.2449803352355957\n",
            "loss:   2.655700206756592\n",
            "loss:   2.494941234588623\n",
            "loss:   1.9822659492492676\n",
            "loss:   2.3957221508026123\n",
            "loss:   2.475396156311035\n",
            "loss:   2.54697322845459\n",
            "loss:   2.4094481468200684\n",
            "loss:   2.5853307247161865\n",
            "loss:   2.4291813373565674\n",
            "loss:   2.5155794620513916\n",
            "loss:   2.2601711750030518\n",
            "loss:   2.291398048400879\n",
            "loss:   2.281261682510376\n",
            "loss:   1.9881598949432373\n",
            "loss:   2.0716192722320557\n",
            "loss:   2.1232872009277344\n",
            "loss:   2.1810193061828613\n",
            "loss:   2.385727882385254\n",
            "loss:   2.3503127098083496\n",
            "loss:   2.6631195545196533\n",
            "loss:   2.5713584423065186\n",
            "loss:   2.7911438941955566\n",
            "loss:   2.2377641201019287\n",
            "loss:   1.9390711784362793\n",
            "loss:   2.0734827518463135\n",
            "loss:   2.556403398513794\n",
            "loss:   2.5495944023132324\n",
            "loss:   2.255188465118408\n",
            "loss:   1.976072907447815\n",
            "loss:   2.1485161781311035\n",
            "loss:   2.6792984008789062\n",
            "loss:   2.2968273162841797\n",
            "loss:   2.480881690979004\n",
            "loss:   2.0667054653167725\n",
            "loss:   2.636775255203247\n",
            "loss:   2.0641376972198486\n",
            "loss:   2.3786261081695557\n",
            "loss:   2.4006474018096924\n",
            "loss:   2.729306936264038\n",
            "loss:   2.3174476623535156\n",
            "loss:   2.532862663269043\n",
            "loss:   2.20576810836792\n",
            "loss:   2.031538724899292\n",
            "loss:   2.107985496520996\n",
            "loss:   2.1375951766967773\n",
            "loss:   2.0759994983673096\n",
            "loss:   2.2785496711730957\n",
            "loss:   2.329714298248291\n",
            "loss:   2.7397384643554688\n",
            "loss:   2.3332130908966064\n",
            "loss:   2.311681032180786\n",
            "loss:   2.1239452362060547\n",
            "loss:   2.5254998207092285\n",
            "loss:   2.526824951171875\n",
            "loss:   1.9559959173202515\n",
            "loss:   2.33036732673645\n",
            "loss:   2.3041396141052246\n",
            "loss:   2.683342933654785\n",
            "loss:   2.2326889038085938\n",
            "loss:   1.9702032804489136\n",
            "loss:   2.569614887237549\n",
            "loss:   2.479771852493286\n",
            "loss:   1.9627922773361206\n",
            "loss:   2.518747568130493\n",
            "loss:   2.4673900604248047\n",
            "loss:   2.311464309692383\n",
            "loss:   2.148648977279663\n",
            "loss:   1.7126206159591675\n",
            "loss:   2.5056254863739014\n",
            "loss:   2.5428965091705322\n",
            "loss:   2.2955150604248047\n",
            "loss:   2.6620190143585205\n",
            "loss:   2.447925567626953\n",
            "loss:   2.7487192153930664\n",
            "loss:   2.4017226696014404\n",
            "loss:   2.0849838256835938\n",
            "loss:   1.7347337007522583\n",
            "loss:   2.0052685737609863\n",
            "loss:   2.3114535808563232\n",
            "loss:   2.2621681690216064\n",
            "loss:   2.257359504699707\n",
            "loss:   2.035133123397827\n",
            "loss:   2.5133049488067627\n",
            "loss:   2.6011645793914795\n",
            "loss:   2.227024555206299\n",
            "loss:   2.433418035507202\n",
            "loss:   2.8408632278442383\n",
            "loss:   2.1882214546203613\n",
            "loss:   2.3505594730377197\n",
            "loss:   2.397533893585205\n",
            "loss:   2.4915895462036133\n",
            "loss:   2.0793094635009766\n",
            "loss:   2.2779886722564697\n",
            "loss:   2.469858407974243\n",
            "loss:   2.036526918411255\n",
            "loss:   2.349355697631836\n",
            "loss:   2.4978322982788086\n",
            "loss:   2.3899142742156982\n",
            "loss:   2.6278927326202393\n",
            "loss:   2.215583086013794\n",
            "loss:   2.101252555847168\n",
            "loss:   2.3111746311187744\n",
            "loss:   2.497568130493164\n",
            "loss:   2.3338983058929443\n",
            "loss:   2.097031354904175\n",
            "loss:   2.0663177967071533\n",
            "loss:   2.6219987869262695\n",
            "loss:   2.2850451469421387\n",
            "loss:   2.114901065826416\n",
            "loss:   2.023430585861206\n",
            "loss:   1.9237791299819946\n",
            "loss:   2.799340009689331\n",
            "loss:   2.4596970081329346\n",
            "loss:   1.9608900547027588\n",
            "loss:   2.5522067546844482\n",
            "loss:   2.3973686695098877\n",
            "loss:   2.354961395263672\n",
            "loss:   2.3492486476898193\n",
            "loss:   2.3420798778533936\n",
            "loss:   2.424154043197632\n",
            "loss:   2.018697738647461\n",
            "loss:   2.458908796310425\n",
            "loss:   2.3380630016326904\n",
            "loss:   2.076061248779297\n",
            "loss:   2.2398667335510254\n",
            "loss:   2.3444015979766846\n",
            "loss:   2.138422727584839\n",
            "loss:   1.9548386335372925\n",
            "loss:   2.273904323577881\n",
            "loss:   2.2958621978759766\n",
            "loss:   2.17519211769104\n",
            "loss:   2.2791693210601807\n",
            "loss:   2.210603713989258\n",
            "loss:   2.189018487930298\n",
            "loss:   2.194641590118408\n",
            "loss:   2.0392355918884277\n",
            "loss:   2.2553963661193848\n",
            "loss:   2.367356061935425\n",
            "loss:   2.0990800857543945\n",
            "loss:   2.363112211227417\n",
            "loss:   2.1399178504943848\n",
            "loss:   2.0003838539123535\n",
            "loss:   1.7957276105880737\n",
            "loss:   2.152528762817383\n",
            "loss:   2.0070526599884033\n",
            "loss:   2.645240306854248\n",
            "loss:   2.1976583003997803\n",
            "loss:   2.092963933944702\n",
            "loss:   2.3977138996124268\n",
            "loss:   2.2201271057128906\n",
            "loss:   2.2752926349639893\n",
            "loss:   2.2997875213623047\n",
            "loss:   2.0550026893615723\n",
            "loss:   2.2004575729370117\n",
            "loss:   2.468977689743042\n",
            "loss:   2.611983299255371\n",
            "loss:   2.2509491443634033\n",
            "loss:   2.7167537212371826\n",
            "loss:   2.574739933013916\n",
            "loss:   2.322899580001831\n",
            "loss:   2.383479595184326\n",
            "loss:   2.19602632522583\n",
            "loss:   2.2517340183258057\n",
            "loss:   2.6396260261535645\n",
            "loss:   2.3529696464538574\n",
            "loss:   2.5173892974853516\n",
            "loss:   2.318159818649292\n",
            "loss:   2.1235077381134033\n",
            "loss:   2.25016188621521\n",
            "loss:   2.5367562770843506\n",
            "loss:   2.32560658454895\n",
            "loss:   2.36442232131958\n",
            "loss:   2.2068307399749756\n",
            "loss:   2.2116055488586426\n",
            "loss:   2.3549623489379883\n",
            "loss:   1.8475168943405151\n",
            "loss:   1.911445140838623\n",
            "loss:   2.4904348850250244\n",
            "loss:   2.649641990661621\n",
            "loss:   2.319113254547119\n",
            "loss:   2.6015377044677734\n",
            "loss:   2.2566802501678467\n",
            "loss:   2.327378511428833\n",
            "loss:   2.132584810256958\n",
            "loss:   2.3544600009918213\n",
            "loss:   2.448044776916504\n",
            "loss:   2.1960384845733643\n",
            "loss:   2.0799953937530518\n",
            "loss:   2.173645496368408\n",
            "loss:   2.3690707683563232\n",
            "loss:   2.13690447807312\n",
            "loss:   2.144359827041626\n",
            "loss:   1.9503992795944214\n",
            "loss:   2.7823452949523926\n",
            "loss:   2.3374288082122803\n",
            "loss:   2.2068638801574707\n",
            "loss:   2.1536216735839844\n",
            "loss:   2.356261730194092\n",
            "loss:   1.943819284439087\n",
            "loss:   2.1933786869049072\n",
            "loss:   2.325356960296631\n",
            "loss:   2.3930118083953857\n",
            "loss:   1.8713266849517822\n",
            "loss:   2.3596835136413574\n",
            "loss:   1.9587478637695312\n",
            "loss:   2.3898072242736816\n",
            "loss:   2.5302813053131104\n",
            "loss:   2.4931063652038574\n",
            "loss:   2.2778613567352295\n",
            "loss:   2.3054161071777344\n",
            "loss:   2.1639719009399414\n",
            "loss:   2.2024056911468506\n",
            "loss:   2.274721145629883\n",
            "loss:   2.400850296020508\n",
            "loss:   2.495628595352173\n",
            "loss:   2.4389610290527344\n",
            "loss:   2.323550224304199\n",
            "loss:   2.1254568099975586\n",
            "loss:   2.072869062423706\n",
            "loss:   2.8961167335510254\n",
            "loss:   2.5026917457580566\n",
            "loss:   2.1063461303710938\n",
            "loss:   2.4549617767333984\n",
            "loss:   2.5425827503204346\n",
            "loss:   2.323448657989502\n",
            "loss:   1.9797481298446655\n",
            "loss:   2.2305989265441895\n",
            "loss:   2.3060715198516846\n",
            "loss:   2.4119515419006348\n",
            "loss:   2.630584478378296\n",
            "loss:   2.374817371368408\n",
            "loss:   2.0429928302764893\n",
            "loss:   2.1117513179779053\n",
            "loss:   2.1240012645721436\n",
            "loss:   2.353214740753174\n",
            "loss:   2.630617141723633\n",
            "loss:   1.9240249395370483\n",
            "loss:   2.5397043228149414\n",
            "loss:   1.9970035552978516\n",
            "loss:   2.0713677406311035\n",
            "loss:   2.2556021213531494\n",
            "loss:   2.280836820602417\n",
            "loss:   2.2832396030426025\n",
            "loss:   2.341479778289795\n",
            "loss:   2.4678285121917725\n",
            "loss:   2.410834789276123\n",
            "loss:   2.474842071533203\n",
            "loss:   2.203261375427246\n",
            "loss:   2.3575570583343506\n",
            "loss:   2.286379814147949\n",
            "loss:   2.3071186542510986\n",
            "loss:   2.1043741703033447\n",
            "loss:   2.021200180053711\n",
            "loss:   2.232762336730957\n",
            "loss:   2.326500654220581\n",
            "loss:   2.1488513946533203\n",
            "loss:   2.498521566390991\n",
            "loss:   2.3432843685150146\n",
            "loss:   2.4143152236938477\n",
            "loss:   2.413271188735962\n",
            "loss:   2.2608132362365723\n",
            "loss:   2.3924570083618164\n",
            "loss:   1.9052520990371704\n",
            "loss:   2.406599760055542\n",
            "loss:   2.475360870361328\n",
            "loss:   1.9491298198699951\n",
            "loss:   2.786616563796997\n",
            "loss:   2.5116403102874756\n",
            "loss:   2.192910671234131\n",
            "loss:   2.3680930137634277\n",
            "loss:   2.2328221797943115\n",
            "loss:   2.4277217388153076\n",
            "loss:   2.4034669399261475\n",
            "loss:   2.7474324703216553\n",
            "loss:   2.680058717727661\n",
            "loss:   2.3925039768218994\n",
            "loss:   2.473989248275757\n",
            "loss:   2.5165557861328125\n",
            "loss:   2.239248752593994\n",
            "loss:   2.5156211853027344\n",
            "loss:   2.2474074363708496\n",
            "loss:   2.2012977600097656\n",
            "loss:   2.0978844165802\n",
            "loss:   2.0135512351989746\n",
            "loss:   2.091399908065796\n",
            "loss:   2.3281049728393555\n",
            "loss:   2.003840208053589\n",
            "loss:   2.428269147872925\n",
            "loss:   1.9445669651031494\n",
            "loss:   2.6025140285491943\n",
            "loss:   2.025686264038086\n",
            "loss:   2.497250556945801\n",
            "loss:   2.1036384105682373\n",
            "loss:   2.538506507873535\n",
            "loss:   2.3863751888275146\n",
            "loss:   2.3978431224823\n",
            "loss:   2.652402639389038\n",
            "loss:   2.2630996704101562\n",
            "loss:   2.4426167011260986\n",
            "loss:   2.16243052482605\n",
            "loss:   2.2256336212158203\n",
            "loss:   1.8669300079345703\n",
            "loss:   2.2333357334136963\n",
            "loss:   2.2757649421691895\n",
            "loss:   1.8504122495651245\n",
            "loss:   2.076111078262329\n",
            "loss:   2.3389735221862793\n",
            "loss:   2.0750906467437744\n",
            "loss:   2.1260833740234375\n",
            "loss:   1.9961968660354614\n",
            "loss:   2.458699941635132\n",
            "loss:   2.3385472297668457\n",
            "loss:   2.4439315795898438\n",
            "loss:   2.1910884380340576\n",
            "loss:   2.48319149017334\n",
            "loss:   2.818413019180298\n",
            "loss:   2.408172369003296\n",
            "loss:   2.7223117351531982\n",
            "loss:   2.377039670944214\n",
            "loss:   2.6204636096954346\n",
            "loss:   2.150440216064453\n",
            "loss:   1.8884729146957397\n",
            "loss:   2.387415885925293\n",
            "loss:   2.3987174034118652\n",
            "loss:   2.5784800052642822\n",
            "loss:   2.5509536266326904\n",
            "loss:   2.3219504356384277\n",
            "loss:   2.413605213165283\n",
            "loss:   2.309753656387329\n",
            "loss:   2.146428108215332\n",
            "loss:   2.3029839992523193\n",
            "loss:   2.419238328933716\n",
            "loss:   2.370673179626465\n",
            "loss:   2.1090102195739746\n",
            "loss:   1.946509599685669\n",
            "loss:   2.1724061965942383\n",
            "loss:   2.3141095638275146\n",
            "loss:   2.3600401878356934\n",
            "loss:   2.455897808074951\n",
            "loss:   2.4359214305877686\n",
            "loss:   2.275508165359497\n",
            "loss:   2.348884105682373\n",
            "loss:   2.6069624423980713\n",
            "loss:   2.4125442504882812\n",
            "loss:   2.084604501724243\n",
            "loss:   2.138559103012085\n",
            "loss:   1.9146736860275269\n",
            "loss:   2.358158588409424\n",
            "loss:   2.4140431880950928\n",
            "loss:   2.173203706741333\n",
            "loss:   1.654083013534546\n",
            "loss:   2.140709400177002\n",
            "loss:   2.0896265506744385\n",
            "loss:   1.9594773054122925\n",
            "loss:   2.467646360397339\n",
            "loss:   2.4770851135253906\n",
            "loss:   2.583559036254883\n",
            "loss:   2.79697322845459\n",
            "loss:   2.1640512943267822\n",
            "loss:   2.2285234928131104\n",
            "loss:   2.858368158340454\n",
            "loss:   2.247203826904297\n",
            "loss:   1.8452917337417603\n",
            "loss:   2.249851703643799\n",
            "loss:   2.372607469558716\n",
            "loss:   2.3469629287719727\n",
            "loss:   2.5897295475006104\n",
            "loss:   2.6171951293945312\n",
            "loss:   2.225811243057251\n",
            "loss:   2.38673996925354\n",
            "loss:   2.1573166847229004\n",
            "loss:   2.3795552253723145\n",
            "loss:   2.6607308387756348\n",
            "loss:   2.235553026199341\n",
            "loss:   2.3241796493530273\n",
            "loss:   2.4322569370269775\n",
            "loss:   2.128528118133545\n",
            "loss:   2.2199714183807373\n",
            "loss:   2.734952449798584\n",
            "loss:   2.059811592102051\n",
            "loss:   2.177678346633911\n",
            "loss:   2.5898728370666504\n",
            "loss:   2.733473539352417\n",
            "loss:   2.368492603302002\n",
            "loss:   2.3156940937042236\n",
            "loss:   2.6248080730438232\n",
            "loss:   2.128688335418701\n",
            "loss:   2.3026115894317627\n",
            "loss:   2.1707406044006348\n",
            "loss:   2.459892511367798\n",
            "loss:   1.9254534244537354\n",
            "loss:   2.6928200721740723\n",
            "loss:   2.3911874294281006\n",
            "loss:   2.214296817779541\n",
            "loss:   2.7392067909240723\n",
            "loss:   2.5968496799468994\n",
            "loss:   2.114642858505249\n",
            "loss:   2.1892731189727783\n",
            "loss:   2.908740282058716\n",
            "loss:   2.1440811157226562\n",
            "loss:   2.464660882949829\n",
            "loss:   2.180377721786499\n",
            "loss:   2.2177703380584717\n",
            "loss:   2.374941349029541\n",
            "loss:   2.315018892288208\n",
            "loss:   1.7242166996002197\n",
            "loss:   2.594970226287842\n",
            "loss:   2.1522884368896484\n",
            "loss:   2.4671967029571533\n",
            "loss:   2.283888578414917\n",
            "loss:   2.3862547874450684\n",
            "loss:   2.2986435890197754\n",
            "loss:   2.817052125930786\n",
            "loss:   2.4230825901031494\n",
            "loss:   2.263655662536621\n",
            "loss:   2.209197998046875\n",
            "loss:   2.048856496810913\n",
            "loss:   2.5469861030578613\n",
            "loss:   2.532510757446289\n",
            "loss:   1.9271246194839478\n",
            "loss:   2.2528903484344482\n",
            "loss:   2.251077651977539\n",
            "loss:   2.4714503288269043\n",
            "loss:   2.1268327236175537\n",
            "loss:   2.3907017707824707\n",
            "loss:   2.5106213092803955\n",
            "loss:   2.2372045516967773\n",
            "loss:   2.3686420917510986\n",
            "loss:   2.614640474319458\n",
            "loss:   2.643367290496826\n",
            "loss:   2.552799940109253\n",
            "loss:   2.4871106147766113\n",
            "loss:   2.411245107650757\n",
            "loss:   2.5463526248931885\n",
            "loss:   2.380492687225342\n",
            "loss:   2.4495880603790283\n",
            "loss:   2.3797338008880615\n",
            "loss:   2.3712661266326904\n",
            "loss:   2.3015942573547363\n",
            "loss:   2.182894706726074\n",
            "loss:   2.456716537475586\n",
            "loss:   2.486298084259033\n",
            "loss:   2.218621253967285\n",
            "loss:   2.2140023708343506\n",
            "loss:   2.2068822383880615\n",
            "loss:   2.1806864738464355\n",
            "loss:   2.2426984310150146\n",
            "loss:   2.3707664012908936\n",
            "loss:   2.6092183589935303\n",
            "loss:   2.0783469676971436\n",
            "loss:   2.1141157150268555\n",
            "loss:   2.1129798889160156\n",
            "loss:   2.257387638092041\n",
            "loss:   2.2500016689300537\n",
            "loss:   2.211413860321045\n",
            "loss:   2.715731620788574\n",
            "loss:   2.524379253387451\n",
            "loss:   2.2154459953308105\n",
            "loss:   2.24867844581604\n",
            "loss:   2.5436620712280273\n",
            "loss:   2.32766056060791\n",
            "loss:   2.133186101913452\n",
            "loss:   2.1449568271636963\n",
            "loss:   2.1731395721435547\n",
            "loss:   2.519681692123413\n",
            "loss:   2.421656608581543\n",
            "loss:   2.0322165489196777\n",
            "loss:   2.11421799659729\n",
            "loss:   2.4258501529693604\n",
            "loss:   2.357429265975952\n",
            "loss:   2.343794345855713\n",
            "loss:   2.1456844806671143\n",
            "loss:   2.1691689491271973\n",
            "loss:   2.4015634059906006\n",
            "loss:   2.2284061908721924\n",
            "loss:   2.4469268321990967\n",
            "loss:   2.3470630645751953\n",
            "loss:   2.372415542602539\n",
            "loss:   2.277574300765991\n",
            "loss:   2.2265827655792236\n",
            "loss:   2.4300830364227295\n",
            "loss:   2.197922706604004\n",
            "loss:   2.1677134037017822\n",
            "loss:   2.260418653488159\n",
            "loss:   2.1836864948272705\n",
            "loss:   2.053166151046753\n",
            "loss:   2.7030909061431885\n",
            "loss:   1.920098900794983\n",
            "loss:   2.3350064754486084\n",
            "loss:   2.7346351146698\n",
            "loss:   2.4304616451263428\n",
            "loss:   2.22562575340271\n",
            "loss:   2.239675283432007\n",
            "loss:   2.404261827468872\n",
            "loss:   1.9968420267105103\n",
            "loss:   2.229656457901001\n",
            "loss:   2.4688808917999268\n",
            "loss:   2.6389377117156982\n",
            "loss:   2.2822890281677246\n",
            "loss:   2.2603304386138916\n",
            "loss:   2.8204219341278076\n",
            "loss:   2.129045248031616\n",
            "loss:   2.2831053733825684\n",
            "loss:   1.8254780769348145\n",
            "loss:   2.3649675846099854\n",
            "loss:   2.404101610183716\n",
            "loss:   2.3193767070770264\n",
            "loss:   2.250608205795288\n",
            "loss:   2.6377460956573486\n",
            "loss:   2.12610125541687\n",
            "loss:   2.2850780487060547\n",
            "loss:   2.3102784156799316\n",
            "loss:   2.222158432006836\n",
            "loss:   2.2971785068511963\n",
            "loss:   2.4117038249969482\n",
            "loss:   1.9987515211105347\n",
            "loss:   2.3985462188720703\n",
            "loss:   2.261268138885498\n",
            "loss:   2.6923484802246094\n",
            "loss:   2.229408025741577\n",
            "loss:   2.108224391937256\n",
            "loss:   2.059518337249756\n",
            "loss:   2.324666738510132\n",
            "loss:   2.719008684158325\n",
            "loss:   2.2233142852783203\n",
            "loss:   2.6085128784179688\n",
            "loss:   2.4770522117614746\n",
            "loss:   2.8285884857177734\n",
            "loss:   2.379838466644287\n",
            "loss:   2.4980695247650146\n",
            "loss:   2.1248204708099365\n",
            "loss:   2.344022750854492\n",
            "loss:   2.476893901824951\n",
            "loss:   2.7185895442962646\n",
            "loss:   2.1309072971343994\n",
            "loss:   2.2211997509002686\n",
            "loss:   2.161292791366577\n",
            "loss:   2.2919559478759766\n",
            "loss:   2.4108738899230957\n",
            "loss:   2.3314859867095947\n",
            "loss:   2.4720757007598877\n",
            "loss:   1.994077205657959\n",
            "loss:   2.430859088897705\n",
            "loss:   2.2808339595794678\n",
            "loss:   2.2638185024261475\n",
            "loss:   2.794541120529175\n",
            "loss:   2.1597166061401367\n",
            "loss:   2.579777479171753\n",
            "loss:   2.322341203689575\n",
            "loss:   2.2180471420288086\n",
            "loss:   2.363389253616333\n",
            "loss:   2.3740997314453125\n",
            "loss:   2.0105020999908447\n",
            "loss:   1.9594024419784546\n",
            "loss:   2.2260496616363525\n",
            "loss:   2.2362427711486816\n",
            "loss:   2.232362985610962\n",
            "loss:   2.2353968620300293\n",
            "loss:   2.0251171588897705\n",
            "loss:   2.3280980587005615\n",
            "loss:   2.3670573234558105\n",
            "loss:   2.0122182369232178\n",
            "loss:   2.133272886276245\n",
            "loss:   2.3696372509002686\n",
            "loss:   2.0216472148895264\n",
            "loss:   2.4676074981689453\n",
            "loss:   2.182575225830078\n",
            "loss:   2.3493082523345947\n",
            "loss:   2.4155097007751465\n",
            "loss:   2.16699481010437\n",
            "loss:   2.022758960723877\n",
            "loss:   1.891797661781311\n",
            "loss:   2.222444534301758\n",
            "loss:   2.8652918338775635\n",
            "loss:   2.3386483192443848\n",
            "loss:   2.3348629474639893\n",
            "loss:   2.0425424575805664\n",
            "loss:   2.2084882259368896\n",
            "loss:   2.44431734085083\n",
            "loss:   2.112300395965576\n",
            "loss:   2.624438524246216\n",
            "loss:   2.2527847290039062\n",
            "loss:   2.0308022499084473\n",
            "loss:   2.007237672805786\n",
            "loss:   2.852574348449707\n",
            "loss:   2.3030269145965576\n",
            "loss:   2.208285093307495\n",
            "loss:   2.316680431365967\n",
            "loss:   2.40486741065979\n",
            "loss:   2.470686197280884\n",
            "loss:   2.615586519241333\n",
            "loss:   2.774965286254883\n",
            "loss:   1.9697277545928955\n",
            "loss:   1.99996018409729\n",
            "loss:   1.9987884759902954\n",
            "loss:   2.1922848224639893\n",
            "loss:   2.10918927192688\n",
            "loss:   2.436516523361206\n",
            "loss:   2.2913248538970947\n",
            "loss:   2.33463978767395\n",
            "loss:   1.8884236812591553\n",
            "loss:   2.581458330154419\n",
            "loss:   2.432526111602783\n",
            "loss:   2.550365924835205\n",
            "loss:   2.5378241539001465\n",
            "loss:   1.98446524143219\n",
            "loss:   2.5241587162017822\n",
            "loss:   2.096886396408081\n",
            "loss:   2.5573387145996094\n",
            "loss:   2.4839539527893066\n",
            "loss:   2.6010732650756836\n",
            "loss:   2.4314732551574707\n",
            "loss:   2.3276920318603516\n",
            "loss:   2.6323788166046143\n",
            "loss:   2.3450381755828857\n",
            "loss:   2.0843210220336914\n",
            "loss:   2.349453926086426\n",
            "loss:   2.154236316680908\n",
            "loss:   2.0588622093200684\n",
            "loss:   2.4351062774658203\n",
            "loss:   2.0817739963531494\n",
            "loss:   2.252885580062866\n",
            "loss:   2.311105251312256\n",
            "loss:   2.290727376937866\n",
            "loss:   1.7475452423095703\n",
            "loss:   2.5589001178741455\n",
            "loss:   2.637669563293457\n",
            "loss:   2.272338628768921\n",
            "loss:   1.9926313161849976\n",
            "loss:   2.4338083267211914\n",
            "loss:   2.396987199783325\n",
            "loss:   2.720414876937866\n",
            "loss:   2.057906150817871\n",
            "loss:   2.6180408000946045\n",
            "loss:   2.1483876705169678\n",
            "loss:   2.0956103801727295\n",
            "loss:   2.2354989051818848\n",
            "loss:   2.201549768447876\n",
            "loss:   2.3332176208496094\n",
            "loss:   2.0565547943115234\n",
            "loss:   2.2590813636779785\n",
            "loss:   2.6418895721435547\n",
            "loss:   2.0134990215301514\n",
            "loss:   2.327157497406006\n",
            "loss:   2.129234790802002\n",
            "loss:   2.2640156745910645\n",
            "loss:   2.1180102825164795\n",
            "loss:   2.0785531997680664\n",
            "loss:   2.082732915878296\n",
            "loss:   2.522826671600342\n",
            "loss:   2.4952054023742676\n",
            "loss:   2.596388578414917\n",
            "loss:   2.475773572921753\n",
            "loss:   2.197176456451416\n",
            "loss:   2.6515846252441406\n",
            "loss:   2.1140527725219727\n",
            "loss:   2.294926404953003\n",
            "loss:   2.7217159271240234\n",
            "loss:   2.545931100845337\n",
            "loss:   2.1908750534057617\n",
            "loss:   2.019911527633667\n",
            "loss:   2.1330716609954834\n",
            "loss:   2.5208353996276855\n",
            "loss:   2.0899600982666016\n",
            "loss:   2.585373640060425\n",
            "loss:   2.3159267902374268\n",
            "loss:   2.7171425819396973\n",
            "loss:   2.294443368911743\n",
            "loss:   2.08204984664917\n",
            "loss:   2.2685773372650146\n",
            "loss:   2.585815191268921\n",
            "loss:   2.2875585556030273\n",
            "loss:   2.455919027328491\n",
            "loss:   2.1066083908081055\n",
            "loss:   2.3094754219055176\n",
            "loss:   2.4787757396698\n",
            "loss:   2.5627834796905518\n",
            "loss:   2.731229543685913\n",
            "loss:   2.2504055500030518\n",
            "loss:   2.2079598903656006\n",
            "loss:   2.363398551940918\n",
            "loss:   2.1093711853027344\n",
            "loss:   2.0393612384796143\n",
            "loss:   2.082261323928833\n",
            "loss:   2.437302827835083\n",
            "loss:   2.230902671813965\n",
            "loss:   2.05106782913208\n",
            "loss:   2.5445446968078613\n",
            "loss:   2.2126822471618652\n",
            "loss:   2.014883518218994\n",
            "loss:   2.3336219787597656\n",
            "loss:   2.455012559890747\n",
            "loss:   1.984580636024475\n",
            "loss:   2.220508337020874\n",
            "loss:   2.342850923538208\n",
            "loss:   2.0723752975463867\n",
            "loss:   2.4736502170562744\n",
            "loss:   2.2864646911621094\n",
            "loss:   2.5405654907226562\n",
            "loss:   2.660378932952881\n",
            "loss:   2.3124399185180664\n",
            "loss:   2.541440010070801\n",
            "loss:   2.070889711380005\n",
            "loss:   2.644777297973633\n",
            "loss:   2.292117118835449\n",
            "loss:   2.2828025817871094\n",
            "loss:   2.0664687156677246\n",
            "loss:   2.6063201427459717\n",
            "loss:   2.4779255390167236\n",
            "loss:   2.3698034286499023\n",
            "loss:   2.4549777507781982\n",
            "loss:   2.349381446838379\n",
            "loss:   2.576174020767212\n",
            "loss:   2.5770418643951416\n",
            "loss:   1.9417166709899902\n",
            "loss:   2.369347333908081\n",
            "loss:   2.455648422241211\n",
            "loss:   2.3280258178710938\n",
            "loss:   2.0209503173828125\n",
            "loss:   2.2582011222839355\n",
            "loss:   2.0648674964904785\n",
            "loss:   1.9912763833999634\n",
            "loss:   2.3762598037719727\n",
            "loss:   2.5226240158081055\n",
            "loss:   2.179781198501587\n",
            "loss:   2.29307222366333\n",
            "loss:   2.4420883655548096\n",
            "loss:   2.508448600769043\n",
            "loss:   2.096149206161499\n",
            "loss:   1.8102601766586304\n",
            "loss:   2.232600688934326\n",
            "loss:   2.370162010192871\n",
            "loss:   2.259042501449585\n",
            "loss:   2.3115923404693604\n",
            "loss:   2.0768349170684814\n",
            "loss:   2.187870502471924\n",
            "loss:   2.56188702583313\n",
            "loss:   2.2386014461517334\n",
            "loss:   2.2999632358551025\n",
            "loss:   2.440944194793701\n",
            "loss:   2.4030699729919434\n",
            "loss:   2.434865951538086\n",
            "loss:   1.9991661310195923\n",
            "loss:   2.4749624729156494\n",
            "loss:   2.033308982849121\n",
            "loss:   2.1539647579193115\n",
            "loss:   2.3956820964813232\n",
            "loss:   2.237593650817871\n",
            "loss:   2.239406108856201\n",
            "loss:   2.265772581100464\n",
            "loss:   2.2449235916137695\n",
            "loss:   2.228097438812256\n",
            "loss:   2.3673839569091797\n",
            "loss:   2.0645976066589355\n",
            "loss:   2.403123617172241\n",
            "loss:   2.5051815509796143\n",
            "loss:   2.257476806640625\n",
            "loss:   2.1418561935424805\n",
            "loss:   2.4074950218200684\n",
            "loss:   2.1458580493927\n",
            "loss:   2.5513622760772705\n",
            "loss:   2.421567678451538\n",
            "loss:   2.4614126682281494\n",
            "loss:   1.835530400276184\n",
            "loss:   2.4031271934509277\n",
            "loss:   2.7486095428466797\n",
            "loss:   2.452730417251587\n",
            "loss:   2.2372899055480957\n",
            "loss:   2.4441676139831543\n",
            "loss:   2.325488328933716\n",
            "loss:   2.3999204635620117\n",
            "loss:   2.5293967723846436\n",
            "loss:   2.4747982025146484\n",
            "loss:   2.2040836811065674\n",
            "loss:   2.2022831439971924\n",
            "loss:   2.461794853210449\n",
            "loss:   2.249899387359619\n",
            "loss:   2.6688880920410156\n",
            "loss:   2.2621777057647705\n",
            "loss:   2.079890251159668\n",
            "loss:   1.9946869611740112\n",
            "loss:   2.463690757751465\n",
            "loss:   2.5415940284729004\n",
            "loss:   2.2503042221069336\n",
            "loss:   2.3899519443511963\n",
            "loss:   2.357987880706787\n",
            "loss:   1.9118131399154663\n",
            "loss:   2.4260847568511963\n",
            "loss:   2.3729145526885986\n",
            "loss:   2.1763882637023926\n",
            "loss:   2.283660650253296\n",
            "loss:   2.43068790435791\n",
            "loss:   2.547261953353882\n",
            "loss:   2.2058424949645996\n",
            "loss:   2.397989273071289\n",
            "loss:   2.1976680755615234\n",
            "loss:   2.0408377647399902\n",
            "loss:   2.1286356449127197\n",
            "loss:   2.3208203315734863\n",
            "loss:   2.031916618347168\n",
            "loss:   2.7818405628204346\n",
            "loss:   2.1342434883117676\n",
            "loss:   2.370407819747925\n",
            "loss:   2.070256233215332\n",
            "loss:   2.4203600883483887\n",
            "loss:   2.2038509845733643\n",
            "loss:   2.3798787593841553\n",
            "loss:   2.152055263519287\n",
            "loss:   1.9657423496246338\n",
            "loss:   2.1759862899780273\n",
            "loss:   2.411162853240967\n",
            "loss:   2.174798011779785\n",
            "loss:   2.431931734085083\n",
            "loss:   2.4452836513519287\n",
            "loss:   2.503748655319214\n",
            "loss:   2.6138789653778076\n",
            "loss:   2.5673108100891113\n",
            "loss:   2.1787829399108887\n",
            "loss:   2.0724704265594482\n",
            "loss:   2.2684311866760254\n",
            "loss:   1.8944171667099\n",
            "loss:   2.1966686248779297\n",
            "loss:   2.503671169281006\n",
            "loss:   1.8834595680236816\n",
            "loss:   2.0826687812805176\n",
            "loss:   2.3437438011169434\n",
            "loss:   2.7977705001831055\n",
            "loss:   2.082718849182129\n",
            "loss:   2.08770489692688\n",
            "loss:   2.4759438037872314\n",
            "loss:   2.2172322273254395\n",
            "loss:   2.1428399085998535\n",
            "loss:   2.3954975605010986\n",
            "loss:   2.1326372623443604\n",
            "loss:   2.389082431793213\n",
            "loss:   2.182183265686035\n",
            "loss:   2.6312084197998047\n",
            "loss:   2.140035390853882\n",
            "loss:   2.2814996242523193\n",
            "loss:   2.098945379257202\n",
            "loss:   2.041306495666504\n",
            "loss:   2.492978811264038\n",
            "loss:   2.077033042907715\n",
            "loss:   2.605879783630371\n",
            "loss:   2.114678144454956\n",
            "loss:   2.2125895023345947\n",
            "loss:   2.353146553039551\n",
            "loss:   2.167362689971924\n",
            "loss:   2.504944324493408\n",
            "loss:   2.2766273021698\n",
            "loss:   2.6289515495300293\n",
            "loss:   2.1084086894989014\n",
            "loss:   2.4792048931121826\n",
            "loss:   2.3713295459747314\n",
            "loss:   2.3350019454956055\n",
            "loss:   3.0000765323638916\n",
            "loss:   2.144017219543457\n",
            "loss:   2.4901883602142334\n",
            "loss:   2.3062961101531982\n",
            "loss:   2.5999326705932617\n",
            "loss:   2.5387744903564453\n",
            "loss:   2.1126654148101807\n",
            "loss:   2.3679020404815674\n",
            "loss:   2.3741304874420166\n",
            "loss:   2.3280837535858154\n",
            "loss:   2.8413712978363037\n",
            "loss:   2.4580490589141846\n",
            "loss:   2.115532875061035\n",
            "loss:   2.3101582527160645\n",
            "loss:   2.5108413696289062\n",
            "loss:   2.5553951263427734\n",
            "loss:   2.2622792720794678\n",
            "loss:   2.0475621223449707\n",
            "loss:   2.2968056201934814\n",
            "loss:   1.7366135120391846\n",
            "loss:   2.2196128368377686\n",
            "loss:   2.4183969497680664\n",
            "loss:   2.166076183319092\n",
            "loss:   1.9807393550872803\n",
            "loss:   2.245500326156616\n",
            "loss:   2.1098849773406982\n",
            "loss:   2.395956039428711\n",
            "loss:   2.3289859294891357\n",
            "loss:   2.6629257202148438\n",
            "loss:   2.823451042175293\n",
            "loss:   2.524705171585083\n",
            "loss:   2.05198073387146\n",
            "loss:   2.5562520027160645\n",
            "loss:   2.592623233795166\n",
            "loss:   2.5531246662139893\n",
            "loss:   2.2625746726989746\n",
            "loss:   2.483693838119507\n",
            "loss:   2.073699712753296\n",
            "loss:   2.49361252784729\n",
            "loss:   2.692204475402832\n",
            "loss:   2.5256991386413574\n",
            "loss:   2.668656587600708\n",
            "loss:   2.4738805294036865\n",
            "loss:   2.0071959495544434\n",
            "loss:   2.576090097427368\n",
            "loss:   1.9761399030685425\n",
            "loss:   2.135814666748047\n",
            "loss:   1.9986127614974976\n",
            "loss:   2.062314033508301\n",
            "loss:   2.1972382068634033\n",
            "loss:   2.2065086364746094\n",
            "loss:   2.383254289627075\n",
            "loss:   1.977685809135437\n",
            "loss:   1.9322230815887451\n",
            "loss:   2.4145851135253906\n",
            "loss:   2.1203179359436035\n",
            "loss:   2.683583974838257\n",
            "loss:   2.7054271697998047\n",
            "loss:   2.137869358062744\n",
            "loss:   2.5420143604278564\n",
            "loss:   2.061096429824829\n",
            "loss:   2.34891414642334\n",
            "loss:   2.6665573120117188\n",
            "loss:   2.384432077407837\n",
            "loss:   2.413219451904297\n",
            "loss:   2.2582457065582275\n",
            "loss:   2.261449098587036\n",
            "loss:   2.6207408905029297\n",
            "loss:   2.1058244705200195\n",
            "loss:   2.219142198562622\n",
            "loss:   2.15864634513855\n",
            "loss:   2.108332633972168\n",
            "loss:   2.4658265113830566\n",
            "loss:   2.50395131111145\n",
            "loss:   2.4647393226623535\n",
            "loss:   2.2101035118103027\n",
            "loss:   2.213599681854248\n",
            "loss:   1.9619609117507935\n",
            "loss:   2.310378313064575\n",
            "loss:   2.2008309364318848\n",
            "loss:   2.3348050117492676\n",
            "loss:   2.4238440990448\n",
            "loss:   2.397529125213623\n",
            "loss:   2.475109577178955\n",
            "loss:   2.4333014488220215\n",
            "loss:   2.616436243057251\n",
            "loss:   2.373042583465576\n",
            "loss:   2.0052671432495117\n",
            "loss:   2.1911725997924805\n",
            "loss:   2.1399588584899902\n",
            "loss:   2.281280279159546\n",
            "loss:   2.4237680435180664\n",
            "loss:   2.3871395587921143\n",
            "loss:   2.385963201522827\n",
            "loss:   2.185265302658081\n",
            "loss:   2.5735039710998535\n",
            "loss:   2.6822831630706787\n",
            "loss:   2.450711727142334\n",
            "loss:   2.542651891708374\n",
            "loss:   2.691012382507324\n",
            "loss:   2.4731409549713135\n",
            "loss:   2.2846150398254395\n",
            "loss:   2.5364596843719482\n",
            "loss:   2.5330233573913574\n",
            "loss:   2.0431764125823975\n",
            "loss:   2.497046709060669\n",
            "loss:   2.0872387886047363\n",
            "loss:   2.1570887565612793\n",
            "loss:   2.263523578643799\n",
            "loss:   2.137289047241211\n",
            "loss:   2.2542083263397217\n",
            "loss:   2.443087577819824\n",
            "loss:   2.6536552906036377\n",
            "loss:   2.463346481323242\n",
            "loss:   2.0493850708007812\n",
            "loss:   2.127028226852417\n",
            "loss:   2.2193427085876465\n",
            "loss:   2.2643823623657227\n",
            "loss:   1.8918858766555786\n",
            "loss:   1.8602007627487183\n",
            "loss:   2.551841974258423\n",
            "loss:   2.3383054733276367\n",
            "loss:   1.6202632188796997\n",
            "loss:   2.3951199054718018\n",
            "loss:   2.336103677749634\n",
            "loss:   2.198286533355713\n",
            "loss:   1.9259699583053589\n",
            "loss:   2.3307924270629883\n",
            "loss:   2.37783145904541\n",
            "loss:   2.4137816429138184\n",
            "loss:   2.0310356616973877\n",
            "loss:   2.400123357772827\n",
            "loss:   2.312779664993286\n",
            "loss:   2.640322685241699\n",
            "loss:   2.1152830123901367\n",
            "loss:   2.60748028755188\n",
            "loss:   1.9759856462478638\n",
            "loss:   2.1366727352142334\n",
            "loss:   2.3888912200927734\n",
            "loss:   2.7358603477478027\n",
            "loss:   2.538634777069092\n",
            "loss:   2.4544403553009033\n",
            "loss:   2.1642584800720215\n",
            "loss:   2.114445924758911\n",
            "loss:   2.421010732650757\n",
            "loss:   1.9036945104599\n",
            "loss:   2.28558611869812\n",
            "loss:   2.2326769828796387\n",
            "loss:   2.1448771953582764\n",
            "loss:   1.8835413455963135\n",
            "loss:   2.673644542694092\n",
            "loss:   2.031446695327759\n",
            "loss:   2.421966552734375\n",
            "loss:   2.097167491912842\n",
            "loss:   2.6172118186950684\n",
            "loss:   2.6493279933929443\n",
            "loss:   2.2056386470794678\n",
            "loss:   2.0154647827148438\n",
            "loss:   2.382526159286499\n",
            "loss:   2.231145143508911\n",
            "loss:   2.343452215194702\n",
            "loss:   2.6515238285064697\n",
            "loss:   2.1245980262756348\n",
            "loss:   2.0201072692871094\n",
            "loss:   2.3931357860565186\n",
            "loss:   2.472590208053589\n",
            "loss:   2.556516170501709\n",
            "loss:   2.4473328590393066\n",
            "loss:   2.101099729537964\n",
            "loss:   2.4399173259735107\n",
            "loss:   2.419301986694336\n",
            "loss:   2.35907244682312\n",
            "loss:   2.4665277004241943\n",
            "loss:   2.1365714073181152\n",
            "loss:   2.1456875801086426\n",
            "loss:   2.3886642456054688\n",
            "loss:   2.465812921524048\n",
            "loss:   2.3438992500305176\n",
            "loss:   2.4101455211639404\n",
            "loss:   2.3306989669799805\n",
            "loss:   2.599168300628662\n",
            "loss:   2.3075690269470215\n",
            "loss:   2.6030666828155518\n",
            "loss:   2.4765167236328125\n",
            "loss:   2.5275092124938965\n",
            "loss:   2.5147552490234375\n",
            "loss:   2.5271759033203125\n",
            "loss:   2.4634029865264893\n",
            "loss:   2.147773504257202\n",
            "loss:   2.410665273666382\n",
            "loss:   2.0731453895568848\n",
            "loss:   2.0709052085876465\n",
            "loss:   2.0994269847869873\n",
            "loss:   2.1240274906158447\n",
            "loss:   2.3608131408691406\n",
            "loss:   2.371732711791992\n",
            "loss:   2.212606191635132\n",
            "loss:   2.1767940521240234\n",
            "loss:   1.996824026107788\n",
            "loss:   2.836526870727539\n",
            "loss:   2.1398839950561523\n",
            "loss:   2.4992241859436035\n",
            "loss:   2.6559672355651855\n",
            "loss:   2.142362356185913\n",
            "loss:   1.995668888092041\n",
            "loss:   2.4795897006988525\n",
            "loss:   2.3127851486206055\n",
            "loss:   1.8800541162490845\n",
            "loss:   2.348159074783325\n",
            "loss:   1.8695563077926636\n",
            "loss:   2.426140785217285\n",
            "loss:   2.0420923233032227\n",
            "loss:   2.5899577140808105\n",
            "loss:   2.106928825378418\n",
            "loss:   2.3413350582122803\n",
            "loss:   2.4153716564178467\n",
            "loss:   2.1799263954162598\n",
            "loss:   2.599261522293091\n",
            "loss:   2.5423085689544678\n",
            "loss:   2.4081008434295654\n",
            "loss:   2.6620726585388184\n",
            "loss:   2.0663256645202637\n",
            "loss:   2.4069912433624268\n",
            "loss:   2.3263325691223145\n",
            "loss:   2.207685708999634\n",
            "loss:   2.1981186866760254\n",
            "loss:   2.1042826175689697\n",
            "loss:   2.0909476280212402\n",
            "loss:   2.446805000305176\n",
            "loss:   2.348677158355713\n",
            "loss:   2.1531360149383545\n",
            "loss:   2.160939931869507\n",
            "loss:   2.8925821781158447\n",
            "loss:   2.1662538051605225\n",
            "loss:   2.529059886932373\n",
            "loss:   1.9892184734344482\n",
            "loss:   2.50117564201355\n",
            "loss:   2.0510194301605225\n",
            "loss:   2.8940770626068115\n",
            "loss:   2.5087509155273438\n",
            "loss:   2.1617941856384277\n",
            "loss:   2.226888418197632\n",
            "loss:   2.5435314178466797\n",
            "loss:   2.185615301132202\n",
            "loss:   2.6632235050201416\n",
            "loss:   2.4901626110076904\n",
            "loss:   2.4917454719543457\n",
            "loss:   2.504539966583252\n",
            "loss:   2.6109142303466797\n",
            "loss:   2.3026764392852783\n",
            "loss:   2.413729667663574\n",
            "loss:   2.2966091632843018\n",
            "loss:   2.0969316959381104\n",
            "loss:   1.997061014175415\n",
            "loss:   2.5632946491241455\n",
            "loss:   2.187847852706909\n",
            "loss:   2.68587064743042\n",
            "loss:   2.2268285751342773\n",
            "loss:   2.397939682006836\n",
            "loss:   2.104407787322998\n",
            "loss:   2.452054977416992\n",
            "loss:   2.680180788040161\n",
            "loss:   2.500131368637085\n",
            "loss:   2.6745667457580566\n",
            "loss:   2.38279128074646\n",
            "loss:   2.402642011642456\n",
            "loss:   2.5069215297698975\n",
            "loss:   2.1535534858703613\n",
            "loss:   2.4130992889404297\n",
            "loss:   2.2705698013305664\n",
            "loss:   2.40366530418396\n",
            "loss:   2.0261571407318115\n",
            "loss:   2.059727907180786\n",
            "loss:   2.0517003536224365\n",
            "loss:   2.3972768783569336\n",
            "loss:   2.408143997192383\n",
            "loss:   2.5365617275238037\n",
            "loss:   2.296471357345581\n",
            "loss:   2.009268283843994\n",
            "loss:   2.0090622901916504\n",
            "loss:   2.818113088607788\n",
            "loss:   2.107023000717163\n",
            "loss:   2.253546714782715\n",
            "loss:   2.4702258110046387\n",
            "loss:   2.437725067138672\n",
            "loss:   2.101684808731079\n",
            "loss:   2.2170779705047607\n",
            "loss:   2.711519241333008\n",
            "loss:   2.271442174911499\n",
            "loss:   2.0911951065063477\n",
            "loss:   1.9274978637695312\n",
            "loss:   2.2334635257720947\n",
            "loss:   2.308431625366211\n",
            "loss:   2.298722743988037\n",
            "loss:   2.3596787452697754\n",
            "loss:   2.273451089859009\n",
            "loss:   2.8683230876922607\n",
            "loss:   2.017366647720337\n",
            "loss:   2.624823570251465\n",
            "loss:   2.422361135482788\n",
            "loss:   1.8623356819152832\n",
            "loss:   2.037949800491333\n",
            "loss:   2.058004379272461\n",
            "loss:   2.1094913482666016\n",
            "loss:   2.5101158618927\n",
            "loss:   2.4437201023101807\n",
            "loss:   2.4909331798553467\n",
            "loss:   2.468081474304199\n",
            "loss:   2.7168757915496826\n",
            "loss:   2.136883020401001\n",
            "loss:   2.2802953720092773\n",
            "loss:   2.551037311553955\n",
            "loss:   2.065403938293457\n",
            "loss:   2.2791433334350586\n",
            "loss:   2.021329402923584\n",
            "loss:   2.2068679332733154\n",
            "loss:   2.040926456451416\n",
            "loss:   2.166940212249756\n",
            "loss:   2.351742744445801\n",
            "loss:   2.246208667755127\n",
            "loss:   2.476731300354004\n",
            "loss:   1.9985605478286743\n",
            "loss:   2.362776041030884\n",
            "loss:   1.8696163892745972\n",
            "loss:   2.077162981033325\n",
            "loss:   2.6083080768585205\n",
            "loss:   2.1765525341033936\n",
            "loss:   2.3628134727478027\n",
            "loss:   2.261383056640625\n",
            "loss:   2.632951259613037\n",
            "loss:   2.0536201000213623\n",
            "loss:   2.279134511947632\n",
            "loss:   2.4082279205322266\n",
            "loss:   2.1346633434295654\n",
            "loss:   2.3261332511901855\n",
            "loss:   1.7785993814468384\n",
            "loss:   2.223182201385498\n",
            "loss:   2.636136293411255\n",
            "loss:   2.3304734230041504\n",
            "loss:   2.140007734298706\n",
            "loss:   2.185204267501831\n",
            "loss:   2.3180055618286133\n",
            "loss:   2.2962236404418945\n",
            "loss:   2.408021926879883\n",
            "loss:   2.4270784854888916\n",
            "loss:   2.118156671524048\n",
            "loss:   2.391190528869629\n",
            "loss:   2.077091932296753\n",
            "loss:   1.8255548477172852\n",
            "loss:   2.378727674484253\n",
            "loss:   2.569129705429077\n",
            "loss:   2.517305612564087\n",
            "loss:   2.1557345390319824\n",
            "loss:   2.299360990524292\n",
            "loss:   2.465069055557251\n",
            "loss:   2.3484294414520264\n",
            "loss:   2.0948455333709717\n",
            "loss:   2.7843565940856934\n",
            "loss:   2.1648106575012207\n",
            "loss:   2.1081931591033936\n",
            "loss:   2.101158857345581\n",
            "loss:   2.3942954540252686\n",
            "loss:   2.202699661254883\n",
            "loss:   1.8227248191833496\n",
            "loss:   2.8210980892181396\n",
            "loss:   2.0852038860321045\n",
            "loss:   2.518545389175415\n",
            "loss:   2.139101266860962\n",
            "loss:   1.892556071281433\n",
            "loss:   2.3897645473480225\n",
            "loss:   2.0674431324005127\n",
            "loss:   2.1191952228546143\n",
            "loss:   2.408212423324585\n",
            "loss:   2.657317638397217\n",
            "loss:   2.284705638885498\n",
            "loss:   2.2397541999816895\n",
            "loss:   2.114314079284668\n",
            "loss:   2.0517330169677734\n",
            "loss:   2.5377297401428223\n",
            "loss:   2.332728624343872\n",
            "loss:   2.393519878387451\n",
            "loss:   2.4255645275115967\n",
            "loss:   2.2948803901672363\n",
            "loss:   2.729889154434204\n",
            "loss:   2.5537238121032715\n",
            "loss:   2.190706491470337\n",
            "loss:   2.333014965057373\n",
            "loss:   2.4456570148468018\n",
            "loss:   2.257051467895508\n",
            "loss:   2.8832390308380127\n",
            "loss:   2.665635347366333\n",
            "loss:   1.8699913024902344\n",
            "loss:   2.4338788986206055\n",
            "loss:   2.197669267654419\n",
            "loss:   2.428621768951416\n",
            "loss:   2.7605044841766357\n",
            "loss:   2.2727913856506348\n",
            "loss:   2.37780499458313\n",
            "loss:   2.4513306617736816\n",
            "loss:   2.16281795501709\n",
            "loss:   2.4716122150421143\n",
            "loss:   2.448448657989502\n",
            "loss:   2.4929709434509277\n",
            "loss:   2.1996426582336426\n",
            "loss:   2.5709078311920166\n",
            "loss:   2.420779228210449\n",
            "loss:   2.924592971801758\n",
            "loss:   2.389787197113037\n",
            "loss:   2.4749341011047363\n",
            "loss:   2.773819923400879\n",
            "loss:   2.361891984939575\n",
            "loss:   2.6334145069122314\n",
            "loss:   2.2869880199432373\n",
            "loss:   2.4925568103790283\n",
            "loss:   2.4585065841674805\n",
            "loss:   2.1914708614349365\n",
            "loss:   2.315850019454956\n",
            "loss:   2.5676186084747314\n",
            "loss:   2.242448568344116\n",
            "loss:   2.319864511489868\n",
            "loss:   2.572862148284912\n",
            "loss:   2.236363172531128\n",
            "loss:   2.4676003456115723\n",
            "loss:   2.513763427734375\n",
            "loss:   1.7905595302581787\n",
            "loss:   2.282728433609009\n",
            "loss:   2.066798210144043\n",
            "loss:   2.2415077686309814\n",
            "loss:   1.8980422019958496\n",
            "loss:   2.4283151626586914\n",
            "loss:   2.1191275119781494\n",
            "loss:   2.102623462677002\n",
            "loss:   2.0742011070251465\n",
            "loss:   2.2693281173706055\n",
            "loss:   2.586453914642334\n",
            "loss:   2.0866193771362305\n",
            "loss:   2.5329253673553467\n",
            "loss:   2.4581527709960938\n",
            "loss:   2.252559185028076\n",
            "loss:   2.2841057777404785\n",
            "loss:   2.6617650985717773\n",
            "loss:   2.4632887840270996\n",
            "loss:   2.496516227722168\n",
            "loss:   2.3636093139648438\n",
            "loss:   2.278942108154297\n",
            "loss:   2.0618863105773926\n",
            "loss:   2.4373109340667725\n",
            "loss:   2.208390951156616\n",
            "loss:   2.5041496753692627\n",
            "loss:   2.423457384109497\n",
            "loss:   2.3460044860839844\n",
            "loss:   2.2844958305358887\n",
            "loss:   2.3002967834472656\n",
            "loss:   2.379059314727783\n",
            "loss:   2.155599594116211\n",
            "loss:   2.450843095779419\n",
            "loss:   2.1397511959075928\n",
            "loss:   2.408668041229248\n",
            "loss:   2.332573175430298\n",
            "loss:   2.448992967605591\n",
            "loss:   2.1505796909332275\n",
            "loss:   2.377223253250122\n",
            "loss:   2.1869378089904785\n",
            "loss:   2.0490241050720215\n",
            "loss:   2.175386905670166\n",
            "loss:   2.1332333087921143\n",
            "loss:   2.819753885269165\n",
            "loss:   2.136112689971924\n",
            "loss:   2.442479133605957\n",
            "loss:   2.2097582817077637\n",
            "loss:   2.4448516368865967\n",
            "loss:   2.1989529132843018\n",
            "loss:   1.946879506111145\n",
            "loss:   2.4193739891052246\n",
            "loss:   2.2153890132904053\n",
            "loss:   1.8371624946594238\n",
            "loss:   2.4509687423706055\n",
            "loss:   2.4782590866088867\n",
            "loss:   2.1149983406066895\n",
            "loss:   2.343073606491089\n",
            "loss:   2.17419171333313\n",
            "loss:   2.3636386394500732\n",
            "loss:   2.139782667160034\n",
            "loss:   2.1995809078216553\n",
            "loss:   2.291764974594116\n",
            "loss:   1.9941257238388062\n",
            "loss:   2.1003952026367188\n",
            "loss:   2.452482223510742\n",
            "loss:   2.304244041442871\n",
            "loss:   2.4796526432037354\n",
            "loss:   2.344083309173584\n",
            "loss:   2.119032859802246\n",
            "loss:   2.377720355987549\n",
            "loss:   2.327850341796875\n",
            "loss:   2.1327009201049805\n",
            "loss:   2.5818710327148438\n",
            "loss:   2.3096914291381836\n",
            "loss:   2.5938937664031982\n",
            "loss:   2.772371530532837\n",
            "loss:   2.453171968460083\n",
            "loss:   2.115065336227417\n",
            "loss:   2.1971278190612793\n",
            "loss:   1.9193086624145508\n",
            "loss:   2.2859206199645996\n",
            "loss:   2.2222678661346436\n",
            "loss:   2.4151320457458496\n",
            "loss:   2.5128986835479736\n",
            "loss:   2.5004286766052246\n",
            "loss:   2.9209883213043213\n",
            "loss:   2.256765604019165\n",
            "loss:   2.343136787414551\n",
            "loss:   2.261904239654541\n",
            "loss:   2.3631930351257324\n",
            "loss:   2.860586643218994\n",
            "loss:   2.4265270233154297\n",
            "loss:   2.0846168994903564\n",
            "loss:   2.2155871391296387\n",
            "loss:   2.3592402935028076\n",
            "loss:   2.309556722640991\n",
            "loss:   2.2292327880859375\n",
            "loss:   2.3106861114501953\n",
            "loss:   2.416701078414917\n",
            "loss:   2.625749111175537\n",
            "loss:   1.9867850542068481\n",
            "loss:   1.9742854833602905\n",
            "loss:   1.8582019805908203\n",
            "loss:   2.367684841156006\n",
            "loss:   1.8645875453948975\n",
            "loss:   2.3672385215759277\n",
            "loss:   2.287493944168091\n",
            "loss:   2.298582077026367\n",
            "loss:   2.3778772354125977\n",
            "loss:   2.047423839569092\n",
            "loss:   2.3464608192443848\n",
            "loss:   2.4876370429992676\n",
            "loss:   2.1187663078308105\n",
            "loss:   2.077888011932373\n",
            "loss:   2.656013011932373\n",
            "loss:   2.07608962059021\n",
            "loss:   2.7447633743286133\n",
            "loss:   1.9553744792938232\n",
            "loss:   2.409581422805786\n",
            "loss:   2.384413242340088\n",
            "loss:   2.1802775859832764\n",
            "loss:   2.5864317417144775\n",
            "loss:   2.2193570137023926\n",
            "loss:   2.1087801456451416\n",
            "loss:   2.113229513168335\n",
            "loss:   2.5826544761657715\n",
            "loss:   2.13012957572937\n",
            "loss:   1.980837345123291\n",
            "loss:   2.441739082336426\n",
            "loss:   2.3380637168884277\n",
            "loss:   2.337944984436035\n",
            "loss:   2.640538215637207\n",
            "loss:   2.0090737342834473\n",
            "loss:   2.4193432331085205\n",
            "loss:   2.4454903602600098\n",
            "loss:   2.1870086193084717\n",
            "loss:   2.182145357131958\n",
            "loss:   2.48708176612854\n",
            "loss:   2.500464677810669\n",
            "loss:   2.226630687713623\n",
            "loss:   2.529689073562622\n",
            "loss:   2.0560946464538574\n",
            "loss:   2.1663122177124023\n",
            "loss:   2.474330425262451\n",
            "loss:   2.4035394191741943\n",
            "loss:   2.5727970600128174\n",
            "loss:   2.3940467834472656\n",
            "loss:   2.405686140060425\n",
            "loss:   2.402143955230713\n",
            "loss:   2.5915350914001465\n",
            "loss:   2.712162494659424\n",
            "loss:   2.688133478164673\n",
            "loss:   2.419449806213379\n",
            "loss:   2.585887908935547\n",
            "loss:   2.12276554107666\n",
            "loss:   2.178943395614624\n",
            "loss:   2.3927149772644043\n",
            "loss:   1.9737255573272705\n",
            "loss:   2.514974594116211\n",
            "loss:   1.9530130624771118\n",
            "loss:   2.5165164470672607\n",
            "loss:   2.4045445919036865\n",
            "loss:   2.515359878540039\n",
            "loss:   2.439877510070801\n",
            "loss:   2.310481071472168\n",
            "loss:   2.379903554916382\n",
            "loss:   2.3655526638031006\n",
            "loss:   2.0262200832366943\n",
            "loss:   1.9148085117340088\n",
            "loss:   2.1013126373291016\n",
            "loss:   2.5208306312561035\n",
            "loss:   2.5118515491485596\n",
            "loss:   2.5284950733184814\n",
            "loss:   2.6339166164398193\n",
            "loss:   2.2419068813323975\n",
            "loss:   2.0294418334960938\n",
            "loss:   2.4008219242095947\n",
            "loss:   2.8161418437957764\n",
            "loss:   2.3637025356292725\n",
            "loss:   2.574424982070923\n",
            "loss:   2.4706907272338867\n",
            "loss:   2.5292224884033203\n",
            "loss:   2.5235073566436768\n",
            "loss:   1.8154175281524658\n",
            "loss:   2.6298930644989014\n",
            "loss:   2.706488609313965\n",
            "loss:   2.2863454818725586\n",
            "loss:   2.5215039253234863\n",
            "loss:   2.130154848098755\n",
            "loss:   2.363654136657715\n",
            "loss:   2.3177075386047363\n",
            "loss:   2.405531406402588\n",
            "loss:   2.322669267654419\n",
            "loss:   2.4236464500427246\n",
            "loss:   2.533583402633667\n",
            "loss:   2.3555712699890137\n",
            "loss:   2.4164881706237793\n",
            "loss:   2.310436248779297\n",
            "loss:   2.4555745124816895\n",
            "loss:   2.4356558322906494\n",
            "loss:   2.0579047203063965\n",
            "loss:   2.293039321899414\n",
            "loss:   2.767345428466797\n",
            "loss:   2.476139545440674\n",
            "loss:   1.8179800510406494\n",
            "loss:   2.089975595474243\n",
            "loss:   2.346235752105713\n",
            "loss:   2.5068020820617676\n",
            "loss:   2.743196487426758\n",
            "loss:   2.1538681983947754\n",
            "loss:   2.7222249507904053\n",
            "loss:   2.4238851070404053\n",
            "loss:   2.0372068881988525\n",
            "loss:   2.0527234077453613\n",
            "loss:   2.3639283180236816\n",
            "loss:   2.295138359069824\n",
            "loss:   2.70293927192688\n",
            "loss:   2.354842185974121\n",
            "loss:   2.1147375106811523\n",
            "loss:   2.863449811935425\n",
            "loss:   2.647613763809204\n",
            "loss:   2.340480327606201\n",
            "loss:   2.134418249130249\n",
            "loss:   2.326695442199707\n",
            "loss:   2.3790225982666016\n",
            "loss:   2.0155627727508545\n",
            "loss:   2.325432300567627\n",
            "loss:   2.0527408123016357\n",
            "loss:   2.3603339195251465\n",
            "loss:   2.388030529022217\n",
            "loss:   1.9482007026672363\n",
            "loss:   2.282118082046509\n",
            "loss:   2.1102914810180664\n",
            "loss:   2.072418451309204\n",
            "loss:   2.4089677333831787\n",
            "loss:   1.952097773551941\n",
            "loss:   2.266312599182129\n",
            "loss:   2.3175578117370605\n",
            "loss:   2.611650228500366\n",
            "loss:   2.89920973777771\n",
            "loss:   2.3285114765167236\n",
            "loss:   2.5968494415283203\n",
            "loss:   2.264853000640869\n",
            "loss:   2.149733781814575\n",
            "loss:   1.9119226932525635\n",
            "loss:   2.3890883922576904\n",
            "loss:   2.2598087787628174\n",
            "loss:   1.9917489290237427\n",
            "loss:   2.304572105407715\n",
            "loss:   2.4856908321380615\n",
            "loss:   2.4616618156433105\n",
            "loss:   2.313614845275879\n",
            "loss:   2.2164130210876465\n",
            "loss:   1.8663921356201172\n",
            "loss:   2.1247427463531494\n",
            "loss:   2.718848466873169\n",
            "loss:   2.358539581298828\n",
            "loss:   2.254674196243286\n",
            "loss:   2.4940240383148193\n",
            "loss:   2.5124404430389404\n",
            "loss:   2.6455466747283936\n",
            "loss:   2.5736067295074463\n",
            "loss:   2.2121353149414062\n",
            "loss:   2.1575355529785156\n",
            "loss:   2.1705164909362793\n",
            "loss:   2.266808271408081\n",
            "loss:   2.9580233097076416\n",
            "loss:   1.9734539985656738\n",
            "loss:   1.9825360774993896\n",
            "loss:   2.2002158164978027\n",
            "loss:   2.3934895992279053\n",
            "loss:   2.129260540008545\n",
            "loss:   1.746796727180481\n",
            "loss:   2.185620069503784\n",
            "loss:   2.3433585166931152\n",
            "loss:   2.27561616897583\n",
            "loss:   2.2818899154663086\n",
            "loss:   2.4229061603546143\n",
            "loss:   2.3924639225006104\n",
            "loss:   2.714611053466797\n",
            "loss:   2.7849013805389404\n",
            "loss:   2.2743079662323\n",
            "loss:   2.3727712631225586\n",
            "loss:   2.480708599090576\n",
            "loss:   2.3659071922302246\n",
            "loss:   1.9469852447509766\n",
            "loss:   2.227191209793091\n",
            "loss:   2.1816186904907227\n",
            "loss:   2.519235134124756\n",
            "loss:   2.602005958557129\n",
            "loss:   2.108638286590576\n",
            "loss:   2.260880947113037\n",
            "loss:   2.297548294067383\n",
            "loss:   2.5346782207489014\n",
            "loss:   2.4462099075317383\n",
            "loss:   2.21065092086792\n",
            "loss:   2.212700843811035\n",
            "loss:   2.0194029808044434\n",
            "loss:   2.3714215755462646\n",
            "loss:   2.14695143699646\n",
            "loss:   2.4024600982666016\n",
            "loss:   2.305424690246582\n",
            "loss:   2.178406238555908\n",
            "loss:   2.6424598693847656\n",
            "loss:   2.396732807159424\n",
            "loss:   2.367917776107788\n",
            "loss:   2.092890977859497\n",
            "loss:   1.9422320127487183\n",
            "loss:   2.445047616958618\n",
            "loss:   2.2046468257904053\n",
            "loss:   2.2062931060791016\n",
            "loss:   2.4891371726989746\n",
            "loss:   2.515559673309326\n",
            "loss:   2.292613983154297\n",
            "loss:   2.3308756351470947\n",
            "loss:   2.4689979553222656\n",
            "loss:   2.3430511951446533\n",
            "loss:   2.255525588989258\n",
            "loss:   2.0998599529266357\n",
            "loss:   2.473088502883911\n",
            "loss:   2.565612316131592\n",
            "loss:   2.310873031616211\n",
            "loss:   2.1089234352111816\n",
            "loss:   2.3178350925445557\n",
            "loss:   2.4049713611602783\n",
            "loss:   2.1135573387145996\n",
            "loss:   2.233809471130371\n",
            "loss:   2.4343507289886475\n",
            "loss:   1.7992364168167114\n",
            "loss:   2.585982084274292\n",
            "loss:   2.0762455463409424\n",
            "loss:   2.302825450897217\n",
            "loss:   2.3785128593444824\n",
            "loss:   2.0654313564300537\n",
            "loss:   2.1897380352020264\n",
            "loss:   2.689680814743042\n",
            "loss:   2.243136167526245\n",
            "loss:   2.7645459175109863\n",
            "loss:   2.2392046451568604\n",
            "loss:   2.2403173446655273\n",
            "loss:   2.395174741744995\n",
            "loss:   2.401992082595825\n",
            "loss:   2.4062156677246094\n",
            "loss:   2.379643678665161\n",
            "loss:   1.9754540920257568\n",
            "loss:   2.7191059589385986\n",
            "loss:   2.096120834350586\n",
            "loss:   2.4550817012786865\n",
            "loss:   2.1462934017181396\n",
            "loss:   2.0731682777404785\n",
            "loss:   2.569209098815918\n",
            "loss:   2.830256938934326\n",
            "loss:   2.164912462234497\n",
            "loss:   2.211374521255493\n",
            "loss:   2.1930577754974365\n",
            "loss:   2.4313888549804688\n",
            "loss:   2.527679204940796\n",
            "loss:   2.4462366104125977\n",
            "loss:   2.2747554779052734\n",
            "loss:   2.3447365760803223\n",
            "loss:   2.415153741836548\n",
            "loss:   2.5077714920043945\n",
            "loss:   2.0296902656555176\n",
            "loss:   1.9180306196212769\n",
            "loss:   2.5567193031311035\n",
            "loss:   2.5528173446655273\n",
            "loss:   2.8822574615478516\n",
            "loss:   2.5326194763183594\n",
            "loss:   2.688694715499878\n",
            "loss:   2.688335657119751\n",
            "loss:   2.2188706398010254\n",
            "loss:   2.931985378265381\n",
            "loss:   2.055673360824585\n",
            "loss:   2.324352979660034\n",
            "loss:   2.385099172592163\n",
            "loss:   2.5380208492279053\n",
            "loss:   2.1792025566101074\n",
            "loss:   2.613499164581299\n",
            "loss:   1.948018193244934\n",
            "loss:   2.520340919494629\n",
            "loss:   2.57104754447937\n",
            "loss:   2.1690356731414795\n",
            "loss:   2.4982333183288574\n",
            "loss:   2.5632007122039795\n",
            "loss:   2.6976664066314697\n",
            "loss:   2.3109703063964844\n",
            "loss:   2.3926801681518555\n",
            "loss:   2.4437780380249023\n",
            "loss:   2.1160342693328857\n",
            "loss:   2.0954136848449707\n",
            "loss:   2.3831238746643066\n",
            "loss:   2.2882707118988037\n",
            "loss:   1.8911798000335693\n",
            "loss:   2.3407135009765625\n",
            "loss:   2.3484108448028564\n",
            "loss:   2.1246049404144287\n",
            "loss:   2.3526570796966553\n",
            "loss:   2.429408311843872\n",
            "loss:   2.0313775539398193\n",
            "loss:   2.4315690994262695\n",
            "loss:   2.430640697479248\n",
            "loss:   2.3266186714172363\n",
            "loss:   2.8779757022857666\n",
            "loss:   2.193488359451294\n",
            "loss:   2.1647350788116455\n",
            "loss:   2.3563477993011475\n",
            "loss:   2.1738839149475098\n",
            "loss:   2.3102543354034424\n",
            "loss:   2.3063879013061523\n",
            "loss:   2.443568706512451\n",
            "loss:   1.747259497642517\n",
            "loss:   2.075554847717285\n",
            "loss:   2.301088809967041\n",
            "loss:   2.730637550354004\n",
            "loss:   2.367821455001831\n",
            "loss:   2.194058656692505\n",
            "loss:   2.58201003074646\n",
            "loss:   2.4331347942352295\n",
            "loss:   2.0855910778045654\n",
            "loss:   2.2852048873901367\n",
            "loss:   2.063096761703491\n",
            "loss:   2.6201281547546387\n",
            "loss:   2.241814374923706\n",
            "loss:   2.4467544555664062\n",
            "loss:   2.378525495529175\n",
            "loss:   2.028324842453003\n",
            "loss:   2.142568826675415\n",
            "loss:   2.1358745098114014\n",
            "loss:   2.7973077297210693\n",
            "loss:   2.27643084526062\n",
            "loss:   2.5059149265289307\n",
            "loss:   2.6652348041534424\n",
            "loss:   2.1224045753479004\n",
            "loss:   2.3530805110931396\n",
            "loss:   1.9931162595748901\n",
            "loss:   2.317549705505371\n",
            "loss:   1.7917976379394531\n",
            "loss:   2.4377944469451904\n",
            "loss:   2.383317470550537\n",
            "loss:   2.4199917316436768\n",
            "loss:   2.596329927444458\n",
            "loss:   2.310244560241699\n",
            "loss:   1.9535804986953735\n",
            "loss:   2.7717926502227783\n",
            "loss:   1.8999103307724\n",
            "loss:   2.638453483581543\n",
            "loss:   2.348085641860962\n",
            "loss:   2.1316077709198\n",
            "loss:   2.460386276245117\n",
            "loss:   2.4198033809661865\n",
            "loss:   2.511451244354248\n",
            "loss:   2.168194532394409\n",
            "loss:   2.3538670539855957\n",
            "loss:   2.573955774307251\n",
            "loss:   2.1135377883911133\n",
            "loss:   2.306175708770752\n",
            "loss:   2.3247945308685303\n",
            "loss:   2.141979217529297\n",
            "loss:   2.5625224113464355\n",
            "loss:   2.3836803436279297\n",
            "loss:   2.2268710136413574\n",
            "loss:   2.2037241458892822\n",
            "loss:   2.2588095664978027\n",
            "loss:   2.3543806076049805\n",
            "loss:   2.869948148727417\n",
            "loss:   2.6339404582977295\n",
            "loss:   2.0072641372680664\n",
            "loss:   2.067434787750244\n",
            "loss:   2.0337557792663574\n",
            "loss:   2.2059004306793213\n",
            "loss:   2.697718381881714\n",
            "loss:   2.0459625720977783\n",
            "loss:   1.8244625329971313\n",
            "loss:   2.555685520172119\n",
            "loss:   2.10444974899292\n",
            "loss:   2.8597023487091064\n",
            "loss:   2.5286436080932617\n",
            "loss:   2.1860668659210205\n",
            "loss:   2.5216174125671387\n",
            "loss:   2.182650089263916\n",
            "loss:   1.9280341863632202\n",
            "loss:   2.6274220943450928\n",
            "loss:   2.5738589763641357\n",
            "loss:   2.7702202796936035\n",
            "loss:   2.157874822616577\n",
            "loss:   1.9982112646102905\n",
            "loss:   2.2295258045196533\n",
            "loss:   2.373976707458496\n",
            "loss:   2.2670469284057617\n",
            "loss:   2.3669586181640625\n",
            "loss:   2.468644380569458\n",
            "loss:   1.9985980987548828\n",
            "loss:   2.127376079559326\n",
            "loss:   2.5187582969665527\n",
            "loss:   2.3282477855682373\n",
            "loss:   2.254755735397339\n",
            "loss:   2.61027455329895\n",
            "loss:   2.5867254734039307\n",
            "loss:   2.2050139904022217\n",
            "loss:   2.162313222885132\n",
            "loss:   2.1167495250701904\n",
            "loss:   2.2360172271728516\n",
            "loss:   2.643357038497925\n",
            "loss:   2.1471145153045654\n",
            "loss:   2.348416328430176\n",
            "loss:   2.169797420501709\n",
            "loss:   2.1208972930908203\n",
            "loss:   1.9638569355010986\n",
            "loss:   2.170071840286255\n",
            "loss:   2.5567362308502197\n",
            "loss:   1.9874968528747559\n",
            "loss:   2.5884974002838135\n",
            "loss:   2.445716381072998\n",
            "loss:   2.351792573928833\n",
            "loss:   2.155168056488037\n",
            "loss:   2.5003421306610107\n",
            "loss:   2.2337327003479004\n",
            "loss:   2.16542911529541\n",
            "loss:   2.2406318187713623\n",
            "loss:   2.4958322048187256\n",
            "loss:   2.0907557010650635\n",
            "loss:   2.1979174613952637\n",
            "loss:   2.763343334197998\n",
            "loss:   2.401749849319458\n",
            "loss:   2.3082475662231445\n",
            "loss:   2.7829408645629883\n",
            "loss:   2.5022497177124023\n",
            "loss:   2.3527162075042725\n",
            "loss:   2.2557334899902344\n",
            "loss:   2.271063804626465\n",
            "loss:   2.1117429733276367\n",
            "loss:   2.1490025520324707\n",
            "loss:   2.5886547565460205\n",
            "loss:   2.693772077560425\n",
            "loss:   2.2115230560302734\n",
            "loss:   2.62624454498291\n",
            "loss:   2.4117469787597656\n",
            "loss:   2.3861498832702637\n",
            "loss:   2.123387336730957\n",
            "loss:   2.2409448623657227\n",
            "loss:   2.461576223373413\n",
            "loss:   2.553065538406372\n",
            "loss:   2.7454991340637207\n",
            "loss:   2.123814582824707\n",
            "loss:   2.358304262161255\n",
            "loss:   2.093385696411133\n",
            "loss:   2.190135955810547\n",
            "loss:   2.6783535480499268\n",
            "loss:   2.3081045150756836\n",
            "loss:   2.0722124576568604\n",
            "loss:   2.215420722961426\n",
            "loss:   2.299201488494873\n",
            "loss:   2.2808685302734375\n",
            "loss:   2.4350595474243164\n",
            "loss:   2.3873450756073\n",
            "loss:   2.2454819679260254\n",
            "loss:   2.489157199859619\n",
            "loss:   2.1392822265625\n",
            "loss:   2.535038948059082\n",
            "loss:   2.6138484477996826\n",
            "loss:   1.998891830444336\n",
            "loss:   2.734294891357422\n",
            "loss:   2.020496368408203\n",
            "loss:   1.888991355895996\n",
            "loss:   2.306396245956421\n",
            "loss:   2.293215274810791\n",
            "loss:   2.3483076095581055\n",
            "loss:   2.3410375118255615\n",
            "loss:   2.167184591293335\n",
            "loss:   2.6364595890045166\n",
            "loss:   2.1507482528686523\n",
            "loss:   2.137223720550537\n",
            "loss:   2.0108377933502197\n",
            "loss:   2.1569528579711914\n",
            "loss:   2.231735944747925\n",
            "loss:   1.8898606300354004\n",
            "loss:   2.3326594829559326\n",
            "loss:   2.4312376976013184\n",
            "loss:   2.3122594356536865\n",
            "loss:   2.565903902053833\n",
            "loss:   2.562136173248291\n",
            "loss:   1.9720211029052734\n",
            "loss:   2.2465317249298096\n",
            "loss:   2.700854778289795\n",
            "loss:   2.057673931121826\n",
            "loss:   2.1665115356445312\n",
            "loss:   1.7699171304702759\n",
            "loss:   2.217407703399658\n",
            "loss:   2.5110816955566406\n",
            "loss:   2.2319679260253906\n",
            "loss:   2.496220111846924\n",
            "loss:   2.2916979789733887\n",
            "loss:   2.244736909866333\n",
            "loss:   2.662902593612671\n",
            "loss:   2.438985586166382\n",
            "loss:   2.7475011348724365\n",
            "loss:   2.264885663986206\n",
            "loss:   2.228801965713501\n",
            "loss:   2.350780487060547\n",
            "loss:   2.2570347785949707\n",
            "loss:   2.6450109481811523\n",
            "loss:   2.23801326751709\n",
            "loss:   2.217707395553589\n",
            "loss:   2.4802114963531494\n",
            "loss:   2.274198293685913\n",
            "loss:   2.5032613277435303\n",
            "loss:   2.38909649848938\n",
            "loss:   2.4651131629943848\n",
            "loss:   2.348435163497925\n",
            "loss:   2.2674360275268555\n",
            "loss:   2.259397029876709\n",
            "loss:   2.814544916152954\n",
            "loss:   2.7184877395629883\n",
            "loss:   1.9682776927947998\n",
            "loss:   2.3166918754577637\n",
            "loss:   2.1820361614227295\n",
            "loss:   2.194112539291382\n",
            "loss:   2.6703083515167236\n",
            "loss:   2.13344144821167\n",
            "loss:   2.207645893096924\n",
            "loss:   2.239624261856079\n",
            "loss:   2.2627015113830566\n",
            "loss:   2.277219295501709\n",
            "loss:   2.460580587387085\n",
            "loss:   2.457902669906616\n",
            "loss:   1.9640169143676758\n",
            "loss:   2.527050733566284\n",
            "loss:   2.4062230587005615\n",
            "loss:   2.646603584289551\n",
            "loss:   2.3684463500976562\n",
            "loss:   2.3843581676483154\n",
            "loss:   2.607910394668579\n",
            "loss:   2.417647123336792\n",
            "loss:   2.1174967288970947\n",
            "loss:   2.4312896728515625\n",
            "loss:   2.4570813179016113\n",
            "loss:   2.515669584274292\n",
            "loss:   2.354978084564209\n",
            "loss:   2.2919557094573975\n",
            "loss:   2.169909954071045\n",
            "loss:   2.3756089210510254\n",
            "loss:   1.9015278816223145\n",
            "loss:   2.41729736328125\n",
            "loss:   2.2325375080108643\n",
            "loss:   2.239722967147827\n",
            "loss:   2.4920730590820312\n",
            "loss:   2.356933832168579\n",
            "loss:   2.046144723892212\n",
            "loss:   2.611431837081909\n",
            "loss:   2.235689163208008\n",
            "loss:   2.177755117416382\n",
            "loss:   2.622539758682251\n",
            "loss:   2.002345561981201\n",
            "loss:   1.7948720455169678\n",
            "loss:   2.5045313835144043\n",
            "loss:   2.1652421951293945\n",
            "loss:   2.3644790649414062\n",
            "loss:   2.5692734718322754\n",
            "loss:   2.2475945949554443\n",
            "loss:   2.2105863094329834\n",
            "loss:   2.284968614578247\n",
            "loss:   2.547635555267334\n",
            "loss:   2.228739023208618\n",
            "loss:   2.1109731197357178\n",
            "loss:   2.126561403274536\n",
            "loss:   2.5854313373565674\n",
            "loss:   2.960103750228882\n",
            "loss:   2.2785863876342773\n",
            "loss:   2.0851285457611084\n",
            "loss:   2.5628414154052734\n",
            "loss:   2.3412368297576904\n",
            "loss:   2.3901355266571045\n",
            "loss:   2.4590084552764893\n",
            "loss:   2.647865056991577\n",
            "loss:   2.3343348503112793\n",
            "loss:   2.1012375354766846\n",
            "loss:   2.5394482612609863\n",
            "loss:   2.1294326782226562\n",
            "loss:   1.9182474613189697\n",
            "loss:   2.407794237136841\n",
            "loss:   1.960317850112915\n",
            "loss:   2.0953540802001953\n",
            "loss:   2.1381161212921143\n",
            "loss:   2.453745126724243\n",
            "loss:   2.2132315635681152\n",
            "loss:   1.9908256530761719\n",
            "loss:   2.4977033138275146\n",
            "loss:   2.6600341796875\n",
            "loss:   2.6223127841949463\n",
            "loss:   2.3645811080932617\n",
            "loss:   2.0601136684417725\n",
            "loss:   2.2314159870147705\n",
            "loss:   2.6061980724334717\n",
            "loss:   2.3895533084869385\n",
            "loss:   2.5641050338745117\n",
            "loss:   2.0578205585479736\n",
            "loss:   2.278299331665039\n",
            "loss:   2.3276782035827637\n",
            "loss:   2.1144206523895264\n",
            "loss:   2.4569077491760254\n",
            "loss:   2.56050705909729\n",
            "loss:   2.5292158126831055\n",
            "loss:   2.6065869331359863\n",
            "loss:   2.5596704483032227\n",
            "loss:   2.1611287593841553\n",
            "loss:   1.963399052619934\n",
            "loss:   2.3258004188537598\n",
            "loss:   2.3817286491394043\n",
            "loss:   2.4824185371398926\n",
            "loss:   2.1935296058654785\n",
            "loss:   2.469195604324341\n",
            "loss:   2.512124538421631\n",
            "loss:   2.108240842819214\n",
            "loss:   2.4557836055755615\n",
            "loss:   2.0203304290771484\n",
            "loss:   2.4668478965759277\n",
            "loss:   2.2590954303741455\n",
            "loss:   2.6688270568847656\n",
            "loss:   2.0890464782714844\n",
            "loss:   2.3162670135498047\n",
            "loss:   2.393604278564453\n",
            "loss:   2.3558363914489746\n",
            "loss:   2.205145835876465\n",
            "loss:   2.4868509769439697\n",
            "loss:   2.1459457874298096\n",
            "loss:   2.0783579349517822\n",
            "loss:   2.374166250228882\n",
            "loss:   2.3717522621154785\n",
            "loss:   2.2535464763641357\n",
            "loss:   2.10141921043396\n",
            "loss:   2.498065948486328\n",
            "loss:   2.0099081993103027\n",
            "loss:   2.6112656593322754\n",
            "loss:   2.552281618118286\n",
            "loss:   2.418560266494751\n",
            "loss:   2.5798110961914062\n",
            "loss:   1.6390178203582764\n",
            "loss:   2.2596354484558105\n",
            "loss:   2.3353841304779053\n",
            "loss:   2.3762624263763428\n",
            "loss:   2.1217918395996094\n",
            "loss:   2.2808310985565186\n",
            "loss:   2.2513527870178223\n",
            "loss:   2.131683349609375\n",
            "loss:   2.0880603790283203\n",
            "loss:   2.520179510116577\n",
            "loss:   1.9750980138778687\n",
            "loss:   2.414848566055298\n",
            "loss:   2.02335786819458\n",
            "loss:   1.9469987154006958\n",
            "loss:   2.639777660369873\n",
            "loss:   2.037243366241455\n",
            "loss:   2.051038980484009\n",
            "loss:   2.1828083992004395\n",
            "loss:   2.595787525177002\n",
            "loss:   3.004622459411621\n",
            "loss:   2.37443470954895\n",
            "loss:   2.23317551612854\n",
            "loss:   2.1634163856506348\n",
            "loss:   2.6107046604156494\n",
            "loss:   2.2854220867156982\n",
            "loss:   2.256507396697998\n",
            "loss:   2.32287859916687\n",
            "loss:   2.490171194076538\n",
            "loss:   2.1920108795166016\n",
            "loss:   2.0535919666290283\n",
            "loss:   2.3528847694396973\n",
            "loss:   2.051244020462036\n",
            "loss:   2.091392755508423\n",
            "loss:   2.6194281578063965\n",
            "loss:   2.167720317840576\n",
            "loss:   2.380662679672241\n",
            "loss:   2.343096971511841\n",
            "loss:   2.1924445629119873\n",
            "loss:   2.4137072563171387\n",
            "loss:   2.0679523944854736\n",
            "loss:   2.2186834812164307\n",
            "loss:   2.546630620956421\n",
            "loss:   2.251359224319458\n",
            "loss:   2.119410514831543\n",
            "loss:   2.6419029235839844\n",
            "loss:   2.393444776535034\n",
            "loss:   2.171830415725708\n",
            "loss:   2.6842191219329834\n",
            "loss:   2.241981029510498\n",
            "loss:   2.0858681201934814\n",
            "loss:   2.396392583847046\n",
            "loss:   2.0746262073516846\n",
            "loss:   2.0707294940948486\n",
            "loss:   2.4518280029296875\n",
            "loss:   2.1602025032043457\n",
            "loss:   2.3875951766967773\n",
            "loss:   2.136206865310669\n",
            "loss:   2.2830421924591064\n",
            "loss:   2.317204236984253\n",
            "loss:   2.270066499710083\n",
            "loss:   2.133708953857422\n",
            "loss:   2.782742738723755\n",
            "loss:   2.376315116882324\n",
            "loss:   2.4375932216644287\n",
            "loss:   2.4662201404571533\n",
            "loss:   2.440509796142578\n",
            "loss:   2.2868242263793945\n",
            "loss:   2.3796050548553467\n",
            "loss:   2.4150519371032715\n",
            "loss:   1.8926076889038086\n",
            "loss:   2.211029052734375\n",
            "loss:   2.190014362335205\n",
            "loss:   2.1709470748901367\n",
            "loss:   2.045548915863037\n",
            "loss:   2.3662588596343994\n",
            "loss:   2.6372456550598145\n",
            "loss:   2.2258694171905518\n",
            "loss:   2.1636741161346436\n",
            "loss:   2.190185785293579\n",
            "loss:   2.084965705871582\n",
            "loss:   2.5430169105529785\n",
            "loss:   2.7431371212005615\n",
            "loss:   2.142563819885254\n",
            "loss:   2.3601973056793213\n",
            "loss:   2.1450512409210205\n",
            "loss:   2.6024739742279053\n",
            "loss:   2.2228314876556396\n",
            "loss:   2.1580562591552734\n",
            "loss:   2.3666117191314697\n",
            "loss:   2.3814847469329834\n",
            "loss:   2.2178285121917725\n",
            "loss:   2.3078603744506836\n",
            "loss:   2.366424798965454\n",
            "loss:   2.4243788719177246\n",
            "loss:   2.4276371002197266\n",
            "loss:   2.5835859775543213\n",
            "loss:   2.3496971130371094\n",
            "loss:   2.2066733837127686\n",
            "loss:   2.1418910026550293\n",
            "loss:   2.1583783626556396\n",
            "loss:   2.2832231521606445\n",
            "loss:   2.4429848194122314\n",
            "loss:   2.2300972938537598\n",
            "loss:   2.295966863632202\n",
            "loss:   2.5150394439697266\n",
            "loss:   2.5470027923583984\n",
            "loss:   1.7692960500717163\n",
            "loss:   2.3564810752868652\n",
            "loss:   1.9859256744384766\n",
            "loss:   2.408149480819702\n",
            "loss:   2.6930007934570312\n",
            "loss:   2.199467658996582\n",
            "loss:   2.406934976577759\n",
            "loss:   2.27193546295166\n",
            "loss:   2.026930093765259\n",
            "loss:   2.7052974700927734\n",
            "loss:   2.2167420387268066\n",
            "loss:   2.4280166625976562\n",
            "loss:   2.3623366355895996\n",
            "loss:   2.1138854026794434\n",
            "loss:   2.657744884490967\n",
            "loss:   2.3487775325775146\n",
            "loss:   2.5256519317626953\n",
            "loss:   2.453132390975952\n",
            "loss:   2.3922812938690186\n",
            "loss:   2.3856630325317383\n",
            "loss:   2.350531578063965\n",
            "loss:   2.447932243347168\n",
            "loss:   2.1524131298065186\n",
            "loss:   2.2028636932373047\n",
            "loss:   2.0588035583496094\n",
            "loss:   2.2393665313720703\n",
            "loss:   2.2611048221588135\n",
            "loss:   2.421543836593628\n",
            "loss:   2.6694037914276123\n",
            "loss:   2.464358329772949\n",
            "loss:   2.5583128929138184\n",
            "loss:   2.6245951652526855\n",
            "loss:   2.0981085300445557\n",
            "loss:   2.2998461723327637\n",
            "loss:   2.5565075874328613\n",
            "loss:   2.3487966060638428\n",
            "loss:   2.1495676040649414\n",
            "loss:   2.3495476245880127\n",
            "loss:   2.373267650604248\n",
            "loss:   2.4593276977539062\n",
            "loss:   2.187638998031616\n",
            "loss:   2.2111666202545166\n",
            "loss:   2.408613920211792\n",
            "loss:   2.40753173828125\n",
            "loss:   2.5063421726226807\n",
            "loss:   1.906990885734558\n",
            "loss:   2.2736947536468506\n",
            "loss:   2.1746768951416016\n",
            "loss:   2.000007390975952\n",
            "loss:   2.1093311309814453\n",
            "loss:   2.20125150680542\n",
            "loss:   1.9881659746170044\n",
            "loss:   2.1109328269958496\n",
            "loss:   2.0115549564361572\n",
            "loss:   2.688509464263916\n",
            "loss:   2.1928091049194336\n",
            "loss:   2.279653310775757\n",
            "loss:   2.5998799800872803\n",
            "loss:   2.080648422241211\n",
            "loss:   2.228647470474243\n",
            "loss:   1.9567978382110596\n",
            "loss:   2.2652218341827393\n",
            "loss:   2.5749661922454834\n",
            "loss:   2.331159830093384\n",
            "loss:   2.3279662132263184\n",
            "loss:   3.0058939456939697\n",
            "loss:   2.2008354663848877\n",
            "loss:   2.282085657119751\n",
            "loss:   2.4874026775360107\n",
            "loss:   2.4999196529388428\n",
            "loss:   2.1577396392822266\n",
            "loss:   2.1486477851867676\n",
            "loss:   2.3001086711883545\n",
            "loss:   2.473301649093628\n",
            "loss:   2.7512567043304443\n",
            "loss:   2.235583782196045\n",
            "loss:   2.2209455966949463\n",
            "loss:   2.5688467025756836\n",
            "loss:   2.276988983154297\n",
            "loss:   2.029573440551758\n",
            "loss:   2.249648332595825\n",
            "loss:   2.2990097999572754\n",
            "loss:   2.2576470375061035\n",
            "loss:   2.3484303951263428\n",
            "loss:   2.5114400386810303\n",
            "loss:   2.8306925296783447\n",
            "loss:   2.2954037189483643\n",
            "loss:   2.250436305999756\n",
            "loss:   2.1124613285064697\n",
            "loss:   2.2092864513397217\n",
            "loss:   1.8613051176071167\n",
            "loss:   2.420888662338257\n",
            "loss:   2.2710368633270264\n",
            "loss:   2.507387399673462\n",
            "loss:   2.2576279640197754\n",
            "loss:   1.878327488899231\n",
            "loss:   2.5527689456939697\n",
            "loss:   2.535123825073242\n",
            "loss:   2.395355224609375\n",
            "loss:   2.2005856037139893\n",
            "loss:   2.378901720046997\n",
            "loss:   2.7950870990753174\n",
            "loss:   2.443253517150879\n",
            "loss:   2.52915358543396\n",
            "loss:   2.2250242233276367\n",
            "loss:   2.177555561065674\n",
            "loss:   2.042360305786133\n",
            "loss:   1.9310723543167114\n",
            "loss:   3.0071935653686523\n",
            "loss:   2.4355902671813965\n",
            "loss:   2.529313087463379\n",
            "loss:   2.589535713195801\n",
            "loss:   2.323216199874878\n",
            "loss:   2.1959874629974365\n",
            "loss:   2.4047813415527344\n",
            "loss:   2.1892194747924805\n",
            "loss:   2.1589548587799072\n",
            "loss:   2.191176414489746\n",
            "loss:   2.0022122859954834\n",
            "loss:   2.3452882766723633\n",
            "loss:   2.399691104888916\n",
            "loss:   2.7228598594665527\n",
            "loss:   2.3235902786254883\n",
            "loss:   1.9506279230117798\n",
            "loss:   2.305814266204834\n",
            "loss:   2.407623291015625\n",
            "loss:   2.0693588256835938\n",
            "loss:   2.212799549102783\n",
            "loss:   2.2269692420959473\n",
            "loss:   2.523144483566284\n",
            "loss:   2.3349814414978027\n",
            "loss:   2.0198769569396973\n",
            "loss:   2.424839496612549\n",
            "loss:   1.9786096811294556\n",
            "loss:   2.4034879207611084\n",
            "loss:   2.303189754486084\n",
            "loss:   2.474187135696411\n",
            "loss:   2.2137343883514404\n",
            "loss:   2.329277515411377\n",
            "loss:   2.266469955444336\n",
            "loss:   2.5308163166046143\n",
            "loss:   1.9417911767959595\n",
            "loss:   2.0643463134765625\n",
            "loss:   1.97788405418396\n",
            "loss:   2.3498470783233643\n",
            "loss:   2.527207374572754\n",
            "loss:   2.5359580516815186\n",
            "loss:   1.768713355064392\n",
            "loss:   2.250830888748169\n",
            "loss:   2.448516368865967\n",
            "loss:   2.3393781185150146\n",
            "loss:   2.233687400817871\n",
            "loss:   2.6173593997955322\n",
            "loss:   2.168203592300415\n",
            "loss:   2.569420337677002\n",
            "loss:   2.8280298709869385\n",
            "loss:   2.2216787338256836\n",
            "loss:   2.368417263031006\n",
            "loss:   2.6835572719573975\n",
            "loss:   2.258272409439087\n",
            "loss:   2.057159185409546\n",
            "loss:   2.4971845149993896\n",
            "loss:   2.2335121631622314\n",
            "loss:   2.7142438888549805\n",
            "loss:   2.5989997386932373\n",
            "loss:   1.9728105068206787\n",
            "loss:   2.489522933959961\n",
            "loss:   2.513885974884033\n",
            "loss:   2.3592886924743652\n",
            "loss:   2.5576043128967285\n",
            "loss:   2.260007619857788\n",
            "loss:   1.9550827741622925\n",
            "loss:   2.42063307762146\n",
            "loss:   2.2046470642089844\n",
            "loss:   2.3416285514831543\n",
            "loss:   2.2715368270874023\n",
            "loss:   2.608513832092285\n",
            "loss:   2.7183594703674316\n",
            "loss:   2.6346871852874756\n",
            "loss:   2.380607843399048\n",
            "loss:   2.418874502182007\n",
            "loss:   2.4376795291900635\n",
            "loss:   2.6345725059509277\n",
            "loss:   2.7647643089294434\n",
            "loss:   2.253300189971924\n",
            "loss:   2.2762300968170166\n",
            "loss:   2.156921148300171\n",
            "loss:   2.5984818935394287\n",
            "loss:   2.4704692363739014\n",
            "loss:   2.0835225582122803\n",
            "loss:   2.052995443344116\n",
            "loss:   2.2032458782196045\n",
            "loss:   2.4415740966796875\n",
            "loss:   2.128702402114868\n",
            "loss:   2.9778714179992676\n",
            "loss:   2.0899858474731445\n",
            "loss:   2.1429896354675293\n",
            "loss:   2.525895357131958\n",
            "loss:   2.285449981689453\n",
            "loss:   2.1444108486175537\n",
            "loss:   2.4323134422302246\n",
            "loss:   2.092458963394165\n",
            "loss:   2.3768184185028076\n",
            "loss:   2.2899482250213623\n",
            "loss:   2.0775046348571777\n",
            "loss:   2.07560658454895\n",
            "loss:   2.254168748855591\n",
            "loss:   2.492734909057617\n",
            "loss:   2.0706167221069336\n",
            "loss:   2.167283296585083\n",
            "loss:   2.147922992706299\n",
            "loss:   2.6985809803009033\n",
            "loss:   2.112490653991699\n",
            "loss:   2.160972833633423\n",
            "loss:   2.4998526573181152\n",
            "loss:   2.628819227218628\n",
            "loss:   2.0025434494018555\n",
            "loss:   2.4315528869628906\n",
            "loss:   2.597334146499634\n",
            "loss:   1.8852119445800781\n",
            "loss:   2.6028873920440674\n",
            "loss:   2.2811429500579834\n",
            "loss:   2.3272244930267334\n",
            "loss:   2.543144941329956\n",
            "loss:   2.195037603378296\n",
            "loss:   2.098132848739624\n",
            "loss:   2.8967854976654053\n",
            "loss:   2.132497787475586\n",
            "loss:   2.0731024742126465\n",
            "loss:   2.3287744522094727\n",
            "loss:   2.4020280838012695\n",
            "loss:   2.0125832557678223\n",
            "loss:   2.653888702392578\n",
            "loss:   2.241490125656128\n",
            "loss:   2.712167263031006\n",
            "loss:   2.519679069519043\n",
            "loss:   2.058605670928955\n",
            "loss:   2.5123207569122314\n",
            "loss:   2.2173285484313965\n",
            "loss:   2.3172430992126465\n",
            "loss:   1.9849001169204712\n",
            "loss:   2.6676931381225586\n",
            "loss:   2.4840683937072754\n",
            "loss:   1.9914357662200928\n",
            "loss:   2.4105570316314697\n",
            "loss:   2.3308324813842773\n",
            "loss:   2.300935745239258\n",
            "loss:   1.9081088304519653\n",
            "loss:   2.613438606262207\n",
            "loss:   2.019735813140869\n",
            "loss:   2.179943561553955\n",
            "loss:   2.454432725906372\n",
            "loss:   2.6160545349121094\n",
            "loss:   2.067884922027588\n",
            "loss:   2.1680803298950195\n",
            "loss:   2.370382070541382\n",
            "loss:   2.2916927337646484\n",
            "loss:   2.163848876953125\n",
            "loss:   2.402909517288208\n",
            "loss:   2.1360790729522705\n",
            "loss:   2.5118918418884277\n",
            "loss:   2.5894176959991455\n",
            "loss:   1.9313615560531616\n",
            "loss:   2.6559901237487793\n",
            "loss:   2.558831214904785\n",
            "loss:   2.2075328826904297\n",
            "loss:   2.6781461238861084\n",
            "loss:   2.5763309001922607\n",
            "loss:   2.5491042137145996\n",
            "loss:   2.331024169921875\n",
            "loss:   2.3843557834625244\n",
            "loss:   2.3500380516052246\n",
            "loss:   2.301595687866211\n",
            "loss:   2.7870142459869385\n",
            "loss:   2.3647103309631348\n",
            "loss:   2.234585762023926\n",
            "loss:   2.1814048290252686\n",
            "loss:   2.8574674129486084\n",
            "loss:   2.5113017559051514\n",
            "loss:   2.3954241275787354\n",
            "loss:   2.197385311126709\n",
            "loss:   1.9692378044128418\n",
            "loss:   1.9853099584579468\n",
            "loss:   2.245393753051758\n",
            "loss:   2.725517988204956\n",
            "loss:   2.5594027042388916\n",
            "loss:   2.279222011566162\n",
            "loss:   2.520026206970215\n",
            "loss:   2.125455141067505\n",
            "loss:   2.4201531410217285\n",
            "loss:   2.334559917449951\n",
            "loss:   2.3044357299804688\n",
            "loss:   2.3431196212768555\n",
            "loss:   2.603806972503662\n",
            "loss:   2.4917356967926025\n",
            "loss:   2.1539194583892822\n",
            "loss:   2.3106794357299805\n",
            "loss:   2.136080026626587\n",
            "loss:   2.720012903213501\n",
            "loss:   1.972015142440796\n",
            "loss:   2.037992000579834\n",
            "loss:   2.662673234939575\n",
            "loss:   2.257189989089966\n",
            "loss:   2.064471960067749\n",
            "loss:   2.5985147953033447\n",
            "loss:   2.079287528991699\n",
            "loss:   1.8100184202194214\n",
            "loss:   2.081130266189575\n",
            "loss:   2.335980176925659\n",
            "loss:   2.303258180618286\n",
            "loss:   2.394529342651367\n",
            "loss:   2.0571417808532715\n",
            "loss:   2.280186653137207\n",
            "loss:   2.4726099967956543\n",
            "loss:   2.4732210636138916\n",
            "loss:   2.489994764328003\n",
            "loss:   1.9520074129104614\n",
            "loss:   1.9661915302276611\n",
            "loss:   2.205918073654175\n",
            "loss:   1.9402996301651\n",
            "loss:   2.3982913494110107\n",
            "loss:   2.7936458587646484\n",
            "loss:   2.067254066467285\n",
            "loss:   2.3565242290496826\n",
            "loss:   2.476219892501831\n",
            "loss:   1.9174854755401611\n",
            "loss:   2.536276340484619\n",
            "loss:   2.2798049449920654\n",
            "loss:   2.2219719886779785\n",
            "loss:   2.3156285285949707\n",
            "loss:   2.197316884994507\n",
            "loss:   1.8763766288757324\n",
            "loss:   2.2680504322052\n",
            "loss:   2.4504075050354004\n",
            "loss:   2.55818510055542\n",
            "loss:   2.4618983268737793\n",
            "loss:   2.2650859355926514\n",
            "loss:   2.8107128143310547\n",
            "loss:   1.9448295831680298\n",
            "loss:   2.0802693367004395\n",
            "loss:   1.9843560457229614\n",
            "loss:   2.2386844158172607\n",
            "loss:   2.1801717281341553\n",
            "loss:   2.0087711811065674\n",
            "loss:   2.3792896270751953\n",
            "loss:   2.3270795345306396\n",
            "loss:   2.5608537197113037\n",
            "loss:   2.1128811836242676\n",
            "loss:   2.3815784454345703\n",
            "loss:   2.24153470993042\n",
            "loss:   2.219059944152832\n",
            "loss:   2.4706904888153076\n",
            "loss:   2.4290239810943604\n",
            "loss:   2.3737709522247314\n",
            "loss:   2.120205879211426\n",
            "loss:   2.5690431594848633\n",
            "loss:   2.1204938888549805\n",
            "loss:   2.3809754848480225\n",
            "loss:   2.121356725692749\n",
            "loss:   2.180448532104492\n",
            "loss:   2.2710189819335938\n",
            "loss:   2.3652188777923584\n",
            "loss:   2.4045233726501465\n",
            "loss:   2.540508985519409\n",
            "loss:   2.5114407539367676\n",
            "loss:   2.6350998878479004\n",
            "loss:   2.177013635635376\n",
            "loss:   2.7665390968322754\n",
            "loss:   2.4626002311706543\n",
            "loss:   2.34788179397583\n",
            "loss:   2.638667583465576\n",
            "loss:   2.377446413040161\n",
            "loss:   2.318614959716797\n",
            "loss:   2.5713768005371094\n",
            "loss:   2.515401601791382\n",
            "loss:   2.2095539569854736\n",
            "loss:   2.3056716918945312\n",
            "loss:   1.8701436519622803\n",
            "loss:   2.13391375541687\n",
            "loss:   2.248368740081787\n",
            "loss:   2.305927038192749\n",
            "loss:   2.5436549186706543\n",
            "loss:   2.3451013565063477\n",
            "loss:   2.014916181564331\n",
            "loss:   2.3747453689575195\n",
            "loss:   2.338785409927368\n",
            "loss:   1.8378138542175293\n",
            "loss:   2.1755452156066895\n",
            "loss:   2.483346700668335\n",
            "loss:   2.459479808807373\n",
            "loss:   2.234600782394409\n",
            "loss:   2.5439088344573975\n",
            "loss:   2.0715200901031494\n",
            "loss:   2.3935372829437256\n",
            "loss:   2.241957426071167\n",
            "loss:   2.527505874633789\n",
            "loss:   2.3660035133361816\n",
            "loss:   2.5191056728363037\n",
            "loss:   2.243529796600342\n",
            "loss:   2.366135358810425\n",
            "loss:   2.6036157608032227\n",
            "loss:   2.358745813369751\n",
            "loss:   2.6153995990753174\n",
            "loss:   2.1853020191192627\n",
            "loss:   2.208526372909546\n",
            "loss:   2.396068811416626\n",
            "loss:   2.1750857830047607\n",
            "loss:   2.1980268955230713\n",
            "loss:   2.3211028575897217\n",
            "loss:   2.0977563858032227\n",
            "loss:   2.4117329120635986\n",
            "loss:   2.429473400115967\n",
            "loss:   2.402204990386963\n",
            "loss:   2.3179783821105957\n",
            "loss:   2.5968379974365234\n",
            "loss:   2.447496175765991\n",
            "loss:   1.948424220085144\n",
            "loss:   2.307652235031128\n",
            "loss:   2.3977067470550537\n",
            "loss:   2.3607330322265625\n",
            "loss:   2.325922966003418\n",
            "loss:   2.402845859527588\n",
            "loss:   2.530221939086914\n",
            "loss:   2.248889207839966\n",
            "loss:   2.461252450942993\n",
            "loss:   2.312018632888794\n",
            "loss:   2.198636531829834\n",
            "loss:   1.9738987684249878\n",
            "loss:   2.3093535900115967\n",
            "loss:   2.279604911804199\n",
            "loss:   2.703523635864258\n",
            "loss:   2.2400476932525635\n",
            "loss:   2.2281274795532227\n",
            "loss:   2.564150094985962\n",
            "loss:   2.35524582862854\n",
            "loss:   2.2518153190612793\n",
            "loss:   2.1111650466918945\n",
            "loss:   2.0103001594543457\n",
            "loss:   2.0486550331115723\n",
            "loss:   2.345642328262329\n",
            "loss:   2.366593599319458\n",
            "loss:   2.5475664138793945\n",
            "loss:   2.3245363235473633\n",
            "loss:   2.3424692153930664\n",
            "loss:   2.375110149383545\n",
            "loss:   2.346292495727539\n",
            "loss:   2.3019649982452393\n",
            "loss:   2.5048227310180664\n",
            "loss:   2.4286386966705322\n",
            "loss:   2.545626640319824\n",
            "loss:   2.0573182106018066\n",
            "loss:   2.2427737712860107\n",
            "loss:   2.2755579948425293\n",
            "loss:   2.17024564743042\n",
            "loss:   2.306903600692749\n",
            "loss:   2.0132153034210205\n",
            "loss:   2.53670072555542\n",
            "loss:   2.3997886180877686\n",
            "loss:   2.0461301803588867\n",
            "loss:   2.2472498416900635\n",
            "loss:   2.3559815883636475\n",
            "loss:   2.094733238220215\n",
            "loss:   2.716639518737793\n",
            "loss:   2.2944118976593018\n",
            "loss:   2.1968181133270264\n",
            "loss:   2.3484389781951904\n",
            "loss:   2.134799003601074\n",
            "loss:   2.1068828105926514\n",
            "loss:   2.3707470893859863\n",
            "loss:   2.5058631896972656\n",
            "loss:   2.368325710296631\n",
            "loss:   2.5542335510253906\n",
            "loss:   2.252135753631592\n",
            "loss:   2.092435598373413\n",
            "loss:   2.4653236865997314\n",
            "loss:   2.4662821292877197\n",
            "loss:   2.203768014907837\n",
            "loss:   2.1340725421905518\n",
            "loss:   2.8083012104034424\n",
            "loss:   2.806926727294922\n",
            "loss:   2.7469708919525146\n",
            "loss:   2.3812949657440186\n",
            "loss:   2.1969027519226074\n",
            "loss:   2.219597578048706\n",
            "loss:   2.455353260040283\n",
            "loss:   2.5256052017211914\n",
            "loss:   2.399641513824463\n",
            "loss:   2.0491175651550293\n",
            "loss:   2.760864496231079\n",
            "loss:   2.251110553741455\n",
            "loss:   2.2506182193756104\n",
            "loss:   2.354938507080078\n",
            "loss:   2.1360154151916504\n",
            "loss:   2.749182939529419\n",
            "loss:   2.4650213718414307\n",
            "loss:   1.9239270687103271\n",
            "loss:   2.106947898864746\n",
            "loss:   2.8235883712768555\n",
            "loss:   2.3139169216156006\n",
            "loss:   1.7992751598358154\n",
            "loss:   2.32218599319458\n",
            "loss:   2.3595471382141113\n",
            "loss:   2.1794047355651855\n",
            "loss:   2.2821364402770996\n",
            "loss:   2.5684399604797363\n",
            "loss:   2.0657482147216797\n",
            "loss:   2.314657211303711\n",
            "loss:   2.1007564067840576\n",
            "loss:   2.025108575820923\n",
            "loss:   2.598355531692505\n",
            "loss:   2.30792498588562\n",
            "loss:   2.2861971855163574\n",
            "loss:   2.216137647628784\n",
            "loss:   2.516988515853882\n",
            "loss:   2.440047025680542\n",
            "loss:   1.874150276184082\n",
            "loss:   2.6511902809143066\n",
            "loss:   2.654550552368164\n",
            "loss:   2.2064385414123535\n",
            "loss:   2.2096080780029297\n",
            "loss:   1.965617299079895\n",
            "loss:   1.7837841510772705\n",
            "loss:   2.375681161880493\n",
            "loss:   2.313102960586548\n",
            "loss:   2.5853304862976074\n",
            "loss:   2.235396146774292\n",
            "loss:   1.996665596961975\n",
            "loss:   2.5716474056243896\n",
            "loss:   2.2113089561462402\n",
            "loss:   2.1090948581695557\n",
            "loss:   2.4377200603485107\n",
            "loss:   1.9899646043777466\n",
            "loss:   2.5673012733459473\n",
            "loss:   1.9489587545394897\n",
            "loss:   2.1371984481811523\n",
            "loss:   2.47039794921875\n",
            "loss:   2.0200257301330566\n",
            "loss:   2.6533446311950684\n",
            "loss:   2.080449342727661\n",
            "loss:   2.3565759658813477\n",
            "loss:   2.0986688137054443\n",
            "loss:   2.783132791519165\n",
            "loss:   2.1590235233306885\n",
            "loss:   1.9966951608657837\n",
            "loss:   2.2995047569274902\n",
            "loss:   1.81441330909729\n",
            "loss:   2.1138758659362793\n",
            "loss:   2.298624277114868\n",
            "loss:   2.2605130672454834\n",
            "loss:   2.4279680252075195\n",
            "loss:   2.372246742248535\n",
            "loss:   2.5562891960144043\n",
            "loss:   2.1940512657165527\n",
            "loss:   2.12585186958313\n",
            "loss:   2.5670552253723145\n",
            "loss:   2.31367564201355\n",
            "loss:   2.5454189777374268\n",
            "loss:   2.2733702659606934\n",
            "loss:   2.192591667175293\n",
            "loss:   2.2988197803497314\n",
            "loss:   1.8000426292419434\n",
            "loss:   2.1052603721618652\n",
            "loss:   2.4368486404418945\n",
            "loss:   2.2277119159698486\n",
            "loss:   2.468613862991333\n",
            "loss:   2.485605001449585\n",
            "loss:   1.9773560762405396\n",
            "loss:   2.4586946964263916\n",
            "loss:   2.264711380004883\n",
            "loss:   2.685087203979492\n",
            "loss:   2.855722427368164\n",
            "loss:   2.5169742107391357\n",
            "loss:   2.48296856880188\n",
            "loss:   2.0657050609588623\n",
            "loss:   2.3329265117645264\n",
            "loss:   2.368129014968872\n",
            "loss:   2.1287105083465576\n",
            "loss:   2.0120489597320557\n",
            "loss:   2.2674756050109863\n",
            "loss:   2.5814366340637207\n",
            "loss:   2.5583322048187256\n",
            "loss:   2.105125904083252\n",
            "loss:   2.367709159851074\n",
            "loss:   2.4149909019470215\n",
            "loss:   2.6593804359436035\n",
            "loss:   2.611543655395508\n",
            "loss:   2.055785894393921\n",
            "loss:   2.034355401992798\n",
            "loss:   2.3417160511016846\n",
            "loss:   2.255384683609009\n",
            "loss:   2.25394868850708\n",
            "loss:   1.902271032333374\n",
            "loss:   1.9396939277648926\n",
            "loss:   2.2220616340637207\n",
            "loss:   1.9056087732315063\n",
            "loss:   2.5234785079956055\n",
            "loss:   1.8385798931121826\n",
            "loss:   2.484091281890869\n",
            "loss:   2.159651279449463\n",
            "loss:   2.14192271232605\n",
            "loss:   2.4433236122131348\n",
            "loss:   2.6379926204681396\n",
            "loss:   1.9065653085708618\n",
            "loss:   2.279656410217285\n",
            "loss:   1.971184253692627\n",
            "loss:   2.502018690109253\n",
            "loss:   2.644343376159668\n",
            "loss:   2.2583818435668945\n",
            "loss:   2.2310903072357178\n",
            "loss:   2.624825954437256\n",
            "loss:   2.580831527709961\n",
            "loss:   2.372926950454712\n",
            "loss:   2.350640058517456\n",
            "loss:   2.366924285888672\n",
            "loss:   2.5420162677764893\n",
            "loss:   2.0482828617095947\n",
            "loss:   2.280168294906616\n",
            "loss:   2.517125368118286\n",
            "loss:   1.817758321762085\n",
            "loss:   2.57429838180542\n",
            "loss:   2.468369483947754\n",
            "loss:   2.965496301651001\n",
            "loss:   2.532703399658203\n",
            "loss:   1.9595186710357666\n",
            "loss:   1.9966710805892944\n",
            "loss:   2.604604721069336\n",
            "loss:   2.7061638832092285\n",
            "loss:   2.0822298526763916\n",
            "loss:   2.3114044666290283\n",
            "loss:   2.2116942405700684\n",
            "loss:   2.523143768310547\n",
            "loss:   2.3530025482177734\n",
            "loss:   2.450650453567505\n",
            "loss:   2.4237380027770996\n",
            "loss:   3.0149471759796143\n",
            "loss:   2.100364923477173\n",
            "loss:   2.3261609077453613\n",
            "loss:   2.1005401611328125\n",
            "loss:   2.2657246589660645\n",
            "loss:   2.052370309829712\n",
            "loss:   1.9934546947479248\n",
            "loss:   2.2660858631134033\n",
            "loss:   2.329850196838379\n",
            "loss:   2.473525285720825\n",
            "loss:   2.610711097717285\n",
            "loss:   2.110757827758789\n",
            "loss:   2.3058090209960938\n",
            "loss:   2.2014989852905273\n",
            "loss:   2.6034889221191406\n",
            "loss:   2.2663159370422363\n",
            "loss:   2.30454683303833\n",
            "loss:   2.4526431560516357\n",
            "loss:   2.907567262649536\n",
            "loss:   2.507671356201172\n",
            "loss:   2.1775646209716797\n",
            "loss:   2.1929571628570557\n",
            "loss:   2.092735528945923\n",
            "loss:   2.2040796279907227\n",
            "loss:   2.271005153656006\n",
            "loss:   2.1789255142211914\n",
            "loss:   2.5996811389923096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets evaluate the loss for all of the dataset\n",
        "emb = C[X]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Y)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf7yQ25RL6Lm",
        "outputId": "718ccfe4-2b31-4a04-91a4-7b6c70b0507e"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.3534607887268066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After we trained at the last we will do a learning rate decay we decay it by may 10 if it is 0.1 -> 0.01 and run the algo again\n",
        "'''"
      ],
      "metadata": {
        "id": "5hkMOqudyz3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "One most important thing to note here is, we got the loss 2.35 and the bigram model had a loss of 2.45, so it means we are doing better than\n",
        "the bigram model ? It's not true, because we add up more parameters and more neurons to thousands to millions, the model will start\n",
        "overfitting the data so well, basically it memorises the data and we try to sample from it, we will get the exact same words as in the\n",
        "data set, so what should we do is the classical split, training set (80%), validation set (10%), test set(10%).\n",
        "We do the training using the training set, validation set is used to adjust/train the hyper parameters like adding more neurons or changing\n",
        "the architecture etc, what works best, test set is used for evaluation/accuracy\n",
        "'''"
      ],
      "metadata": {
        "id": "Q7epyqGvyz1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # build the dataset\n",
        "def build_dataset(words):\n",
        "  block_size = 3 # context length : how many characters do we take to predict the next one ?\n",
        "  X,Y = [],[]\n",
        "  for w in words:\n",
        "    # print(w)\n",
        "    context = [0] * block_size\n",
        "    for ch in list(w)+['<E>']:\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      # print(''.join(itos[i] for i in context), '--->',itos[ix])\n",
        "      context = context[1:] + [ix]\n",
        "      # print(context)\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape,Y.shape)\n",
        "  return X,Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8 * len(words))  # 80% of the words\n",
        "n2 = int(0.9 * len(words))  # 90% of the words\n",
        "\n",
        "Xtr,Ytr = build_dataset(words[:n1])\n",
        "Xdev,Ydev = build_dataset(words[n1:n2])\n",
        "Xte,Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDhVTY-WPyQ6",
        "outputId": "9200371c-b778-468b-a9aa-3a0f5b1b30b7"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182580, 3]) torch.Size([182580])\n",
            "torch.Size([22767, 3]) torch.Size([22767])\n",
            "torch.Size([22799, 3]) torch.Size([22799])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(123)\n",
        "C = torch.randn((28,2),generator=g)\n",
        "W1 = torch.randn((6,100),generator=g)\n",
        "b1 = torch.randn(100,generator=g)\n",
        "W2 = torch.randn((100,28),generator=g)\n",
        "b2 = torch.randn(28,generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad=True\n"
      ],
      "metadata": {
        "id": "73qam-t_TXd8"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (100000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,Xtr.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Ytr[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.2 # 10^(-0.7)\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nva6-fuHQoV4",
        "outputId": "d43359c9-594d-4720-e06f-8c2574e11892"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.064493417739868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look the loss for the whole training set\n",
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ytr)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTMPV_DfPyLS",
        "outputId": "b810fc7c-f258-4d33-ad2b-a1afa7300296"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.317795515060425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets evaluate the loss for dev set\n",
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ydev)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0LrWaDZPyN4",
        "outputId": "16cfde08-9540-48b4-fd60-6a99c60da35c"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.3248095512390137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As we can see the training loss(on the whole training data) and the dev loss are about same, so we are not overfitting, this model is not\n",
        "powerful enough to just be purely memorizing the data so far we are underfitting, so what it means is our network is very tiny and we expect\n",
        "to make performance improvements by scaling up the size of the neural net\n",
        "'''"
      ],
      "metadata": {
        "id": "qtwRDXXRPyIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(123)\n",
        "C = torch.randn((28,2),generator=g)\n",
        "W1 = torch.randn((6,300),generator=g)\n",
        "b1 = torch.randn(300,generator=g)\n",
        "W2 = torch.randn((300,28),generator=g)\n",
        "b2 = torch.randn(28,generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad=True\n",
        "sum(p.nelement() for p in parameters)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG3fgLzCPyGS",
        "outputId": "e51552c4-ff4f-4d44-b39a-4494ad771ea9"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10584"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lri = []\n",
        "stepi = [] # lets track the steps (i)\n",
        "lossi= []\n",
        "for i in range (100000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,Xtr.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Ytr[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.07\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  # lri.append(lre[i])\n",
        "  stepi.append(i)\n",
        "  lossi.append(loss.item())\n",
        "\n",
        "# print(loss.item())"
      ],
      "metadata": {
        "id": "K2Y9D3tPVz0Z"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(stepi,lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "i5O0ulI1Vzw4",
        "outputId": "b5114dec-2bc6-4762-ddeb-5a2de04e43d8"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6dda3149b0>]"
            ]
          },
          "metadata": {},
          "execution_count": 228
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "In the plot we can see a lot of accumulation at the bottom, this can be because of the our small size of minibatch creating noise.\n",
        "'''"
      ],
      "metadata": {
        "id": "Qq3Mjk4IVzuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look the loss for the whole training set\n",
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ytr)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfMLo2zUVzsK",
        "outputId": "a78ff0b9-7d92-4091-d7a9-4f435a83747a"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.1924679279327393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets evaluate the loss for dev set\n",
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ydev)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mmmjjeLVzp7",
        "outputId": "e8c452b6-c07a-444f-826d-b2e6a6705bca"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.2198567390441895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As we increased the neural net size, but our loss is not that greatly reduced, our C matrix mapped to 2d maybe the bottle neck\n",
        "'''"
      ],
      "metadata": {
        "id": "NChThvbRVzno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
        "for i in range(C.shape[0]):\n",
        "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
        "plt.grid('minor')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "-gNAD0m3VzlU",
        "outputId": "715bd002-adf2-4766-b0f5-b09126e8c885"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets make the embeddings bigger"
      ],
      "metadata": {
        "id": "7KRZqn1AjeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(123)\n",
        "C = torch.randn((28,10),generator=g)\n",
        "W1 = torch.randn((30,200),generator=g) # also decreasing the neurons size too\n",
        "b1 = torch.randn(200,generator=g)\n",
        "W2 = torch.randn((200,28),generator=g)\n",
        "b2 = torch.randn(28,generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "  p.requires_grad=True\n",
        "sum(p.nelement() for p in parameters)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcaRueURjd9y",
        "outputId": "449f8860-fa5f-40dd-a94d-e1ea3a212266"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12108"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lri = []\n",
        "stepi = [] # lets track the steps (i)\n",
        "lossi= []"
      ],
      "metadata": {
        "id": "rpHF-d_MlALe"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range (100000):\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0,Xtr.shape[0],(32,)) # we are creating a tuple of size 32, of number b/w 0 and size of X.\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # updating with ix\n",
        "  h = torch.tanh(emb.view(-1,30) @ W1 + b1)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits,Ytr[ix]) # updating with ix\n",
        "  # print(\"loss:  \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  # lri.append(lre[i])\n",
        "  stepi.append(i)\n",
        "  lossi.append(loss.log10().item()) # log loss for better visualization while plotting\n",
        "\n",
        "# print(loss.item())"
      ],
      "metadata": {
        "id": "0RXnG6aljd7d"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(stepi,lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "pE86LnJSjd4z",
        "outputId": "f42c3df8-9dfc-41ce-8eb4-1aa12a2b3a8d"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c6dd676b5c0>]"
            ]
          },
          "metadata": {},
          "execution_count": 251
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look the loss for the whole training set\n",
        "emb = C[Xtr]\n",
        "h = torch.tanh(emb.view(-1,30) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ytr)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4U5T_U7jd2K",
        "outputId": "1c6b400d-dd9b-426b-cda1-af4a2760f511"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.0717296600341797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets evaluate the loss for dev set\n",
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1,30) @ W1 + b1)\n",
        "logits = h @ W2 + b2\n",
        "loss = F.cross_entropy(logits,Ydev)\n",
        "print(\"loss:  \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeXg4HKSjdzT",
        "outputId": "9e33cf93-80e6-400e-c7b3-b4fd69e2ed64"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:   2.12495756149292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As we can see we were able to reduce the loss little bit after adjusting the C matrix, so it maybe was the bottle neck.\n",
        "'''"
      ],
      "metadata": {
        "id": "OqYtMmmWouA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Typically we would be running lots of experiments and then you are slowly stcrutinizing whichever ones give you the best performance and once\n",
        "you find all the hyper parameters that make you performance good, you take that model and you evaluate the test performance single time and\n",
        "that's the number we report on the papers.\n",
        "'''"
      ],
      "metadata": {
        "id": "tHqe0eeilqxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "There are multiple ways we can go from here,\n",
        "we can continue tuning the optimization,\n",
        "or we can continue changing the parameters or the embedding dimmensionality\n",
        "'''"
      ],
      "metadata": {
        "id": "cPp7ZF6klquQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C[torch.tensor([0,0,0])].view(1,-1).shape, W1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9qOwf3putlu",
        "outputId": "311309e1-9df0-48fe-c83a-edd4f6433ae7"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 30]), torch.Size([30, 200]))"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import generators\n",
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(1236)\n",
        "for i in range(1):\n",
        "  out = []\n",
        "  context = [0] * block_size\n",
        "  while True:\n",
        "    emb = C[torch.tensor(context)]\n",
        "    h = torch.tanh(emb.view(1,-1) @ W1 + b1 )\n",
        "    logits = h @ W2 + b2\n",
        "    probs = F.softmax(logits,dim=1)\n",
        "    ix = torch.multinomial(probs,num_samples=1,generator=g).item()\n",
        "    context = context[1:] + [ix]\n",
        "    print(context)\n",
        "    out.append(ix)\n",
        "    if ix == 27:\n",
        "      break\n",
        "  print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg91kQ8FlqsL",
        "outputId": "a7ca91c8-0062-4cf9-c9c7-1fc8ba97b7e1"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 3]\n",
            "[0, 3, 1]\n",
            "[3, 1, 18]\n",
            "[1, 18, 12]\n",
            "[18, 12, 25]\n",
            "[12, 25, 14]\n",
            "[25, 14, 27]\n",
            "carlyn<E>\n"
          ]
        }
      ]
    }
  ]
}